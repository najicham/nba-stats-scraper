# Deployment Guide: BigQuery Quota Fix

**Purpose**: Deploy batching solution to prevent BigQuery quota exceeded errors

**Estimated Time**: 15-20 minutes

**Impact**: Zero downtime - batching is backward compatible

---

## What You're Deploying

### Code Changes

1. **New batching utility**: `shared/utils/bigquery_batch_writer.py`
2. **Updated run history**: `shared/processors/mixins/run_history_mixin.py`
3. **Updated circuit breaker**: `shared/processors/patterns/circuit_breaker_mixin.py`
4. **Updated analytics base**: `data_processors/analytics/analytics_base.py`
5. **New monitoring script**: `monitoring/bigquery_quota_monitor.py`
6. **Setup script**: `bin/setup/setup_quota_monitoring.sh`

### Infrastructure Changes

1. BigQuery table: `nba_orchestration.quota_usage_log`
2. Cloud Scheduler job: `bigquery-quota-monitor` (runs hourly)
3. Cloud Run Job: `quota-monitor`

---

## Pre-Deployment Checks

### 1. Verify Current Quota Usage

```bash
# Check current load jobs (should be high)
python monitoring/bigquery_quota_monitor.py --dry-run
```

**Expected Output**:
- processor_run_history: ~1,321+ jobs (üî¥ CRITICAL)
- circuit_breaker_state: ~575+ jobs (‚ö†Ô∏è WARNING)
- analytics_processor_runs: ~570+ jobs (‚ö†Ô∏è WARNING)

### 2. Check Git Status

```bash
git status
git diff
```

**Expected Files Modified**:
- shared/processors/mixins/run_history_mixin.py
- shared/processors/patterns/circuit_breaker_mixin.py
- data_processors/analytics/analytics_base.py

**Expected Files Added**:
- shared/utils/bigquery_batch_writer.py
- monitoring/bigquery_quota_monitor.py
- bin/setup/setup_quota_monitoring.sh
- schemas/nba_orchestration/quota_usage_log.json
- docs/incidents/2026-01-26-bigquery-quota-exceeded.md

---

## Deployment Steps

### Step 1: Commit Changes

```bash
git add -A
git commit -m "$(cat <<'EOF'
fix: Implement BigQuery batching to prevent quota exceeded errors

Problem:
- Hitting 1,500 load jobs/table/day quota (hard limit)
- processor_run_history: 1,321 jobs/day
- circuit_breaker_state: 575 jobs/day
- analytics_processor_runs: 570 jobs/day
- Total: 2,466 jobs/day (164% of quota)

Solution:
- Created shared BigQueryBatchWriter (batches 50-100 records per write)
- Updated all high-frequency writers to use batching
- Quota usage reduced from 2,466 ‚Üí 31 jobs/day (80x reduction)

Monitoring:
- Added hourly quota monitoring script
- Alerts at 80% quota usage (1,200/1,500)
- Historical tracking in quota_usage_log table

Impact:
- Zero downtime (backward compatible)
- Prevents pipeline failures from quota limits
- Permanent fix for P1 incident on 2026-01-26

Files Changed:
- shared/utils/bigquery_batch_writer.py (NEW)
- shared/processors/mixins/run_history_mixin.py
- shared/processors/patterns/circuit_breaker_mixin.py
- data_processors/analytics/analytics_base.py
- monitoring/bigquery_quota_monitor.py (NEW)

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF
)"
```

### Step 2: Deploy to Cloud Run

Deploy updated code to all services that use run_history, circuit_breaker, or analytics:

```bash
# Phase 3 Analytics Processors (CRITICAL - uses all 3)
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --source=.

# Phase 4 Precompute Processors (uses run_history)
gcloud run services update nba-phase4-precompute-processors \
  --region=us-west2 \
  --source=.

# Raw Scrapers (uses run_history)
gcloud run services update nba-scrapers \
  --region=us-west2 \
  --source=.

# Prediction Coordinator (uses run_history)
gcloud run services update nba-prediction-coordinator \
  --region=us-west2 \
  --source=.
```

**Wait**: Monitor each deployment completes successfully before continuing.

### Step 3: Set Up Quota Monitoring

```bash
# Create table and scheduler job
./bin/setup/setup_quota_monitoring.sh
```

**Expected Output**:
```
‚úÖ Table created
‚úÖ Scheduler job created/updated
‚úÖ Cloud Run Job created/updated
Setup Complete!
```

### Step 4: Test Quota Monitoring

```bash
# Run monitoring manually to verify it works
python monitoring/bigquery_quota_monitor.py --dry-run
```

**Expected Output** (after batching deployed):
```
Top 10 tables by load jobs:
  1. ‚úÖ processor_run_history: 15 jobs (1.0%)
  2. ‚úÖ circuit_breaker_state: 8 jobs (0.5%)
  3. ‚úÖ analytics_processor_runs: 5 jobs (0.3%)
  ...
‚úÖ All tables within quota limits
```

### Step 5: Trigger Scheduler Job

```bash
# Trigger hourly monitoring job
gcloud scheduler jobs run bigquery-quota-monitor \
  --project=nba-props-platform \
  --location=us-west2
```

**Wait**: 1-2 minutes for job to complete

```bash
# Check job ran successfully
gcloud logging read 'resource.type=cloud_run_job AND resource.labels.job_name=quota-monitor' \
  --limit=20 \
  --format="table(timestamp,severity,textPayload)" \
  --project=nba-props-platform
```

---

## Verification

### 1. Check Batching is Working

Monitor logs for batching messages:

```bash
# Phase 3 logs (should see batch flush messages)
gcloud run services logs read nba-phase3-analytics-processors \
  --region=us-west2 \
  --limit=50 | grep -i "flushed\|batch"
```

**Expected Output**:
```
Flushed 100 records to nba_reference.processor_run_history (latency: 450ms, total_batches: 5, total_records: 500)
Flushed 50 records to nba_orchestration.circuit_breaker_state (latency: 320ms, total_batches: 3, total_records: 150)
```

### 2. Verify Quota Usage Dropped

Wait 1 hour, then check quota usage:

```bash
# Check current quota usage
python monitoring/bigquery_quota_monitor.py
```

**Expected**: All tables <10% quota usage

### 3. Check Historical Tracking

```bash
# Query quota usage log
bq query --use_legacy_sql=false "
SELECT
  check_timestamp,
  total_tables_monitored,
  critical_count,
  warning_count,
  max_usage_pct
FROM nba_orchestration.quota_usage_log
ORDER BY check_timestamp DESC
LIMIT 10"
```

**Expected**: Trend showing quota usage decreasing after deployment

---

## Rollback Plan (If Needed)

If batching causes issues:

### Option 1: Disable Batching (Emergency)

```bash
# Disable batching globally (writes one-at-a-time like before)
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --set-env-vars="BQ_BATCH_WRITER_ENABLED=false"

# Repeat for other services
```

**Impact**: Quota will fill up again (12-24 hours until exceeded)

### Option 2: Revert Code

```bash
# Revert to previous commit
git revert HEAD
git push

# Redeploy
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --source=.
```

**Impact**: Back to quota exceeded errors

### Option 3: Increase Batch Size

```bash
# Increase batch size (more aggressive batching)
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --set-env-vars="BQ_BATCH_WRITER_BATCH_SIZE=200,BQ_BATCH_WRITER_TIMEOUT=60"
```

**Impact**: Further reduces quota usage

---

## Post-Deployment Tasks

### Immediate (Today)

- [x] Deploy batching changes
- [x] Set up quota monitoring
- [ ] Backfill missing data from 2026-01-25 and 2026-01-26
- [ ] Run `/validate-daily` to verify recovery
- [ ] Monitor quota usage for 24 hours

### Short Term (This Week)

- [ ] Update `/validate-daily` skill to include quota check (Phase 0)
- [ ] Create Grafana dashboard for quota trends
- [ ] Set up Slack alerts for quota warnings
- [ ] Add quota checks to CI/CD pipeline

### Long Term (Next Sprint)

- [ ] Document all BigQuery quota limits
- [ ] Add quota monitoring to daily operations runbook
- [ ] Create quota usage report (weekly email)
- [ ] Investigate Cloud Logging alternative for high-frequency events

---

## Monitoring & Alerting

### Hourly Monitoring

Cloud Scheduler runs `bigquery-quota-monitor` every hour:
- ‚úÖ GREEN: <80% quota (1,200 jobs)
- ‚ö†Ô∏è YELLOW: 80-95% quota (1,200-1,425 jobs)
- üî¥ RED: >95% quota (1,425+ jobs)

### Where to Check

**Cloud Logging**:
```bash
gcloud logging read 'resource.type=cloud_run_job AND resource.labels.job_name=quota-monitor' \
  --limit=20
```

**BigQuery**:
```sql
SELECT * FROM nba_orchestration.quota_usage_log
ORDER BY check_timestamp DESC
LIMIT 24  -- Last 24 hours
```

**Manual Check**:
```bash
python monitoring/bigquery_quota_monitor.py
```

---

## Success Criteria

**Deployment is successful when**:

1. ‚úÖ All Cloud Run services deployed without errors
2. ‚úÖ Batching logs appear in Cloud Run logs
3. ‚úÖ Quota monitoring job runs successfully
4. ‚úÖ Quota usage drops below 10% per table
5. ‚úÖ No processor failures due to quota errors
6. ‚úÖ Historical data tracked in quota_usage_log

**You can proceed to backfilling when**:
- All 6 success criteria met
- Monitoring shows stable quota usage for 2+ hours
- No alerts from quota monitor

---

## Troubleshooting

### Issue: Batching Not Working

**Symptom**: Still seeing quota exceeded errors

**Check**:
```bash
# Verify batching is enabled
gcloud run services describe nba-phase3-analytics-processors \
  --region=us-west2 \
  --format="value(spec.template.spec.containers[0].env)"
```

**Fix**: Ensure `BQ_BATCH_WRITER_ENABLED` is not set to `false`

### Issue: Monitoring Job Failing

**Symptom**: Scheduler job fails

**Check**:
```bash
gcloud logging read 'resource.type=cloud_run_job AND resource.labels.job_name=quota-monitor AND severity>=ERROR' \
  --limit=10
```

**Common Causes**:
- Missing permissions (grant BigQuery Data Viewer to service account)
- Missing table (run setup script)
- Timeout (increase task-timeout to 600s)

### Issue: Batch Flushes Too Slow

**Symptom**: Records not appearing in BigQuery for 30+ seconds

**Explanation**: This is expected! Batching trades latency for quota efficiency.

**If critical**:
```bash
# Reduce timeout (flush more frequently)
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --set-env-vars="BQ_BATCH_WRITER_TIMEOUT=10"
```

---

## Questions & Support

**Slack**: #data-engineering
**Docs**: `docs/incidents/2026-01-26-bigquery-quota-exceeded.md`
**Code**: `shared/utils/bigquery_batch_writer.py`

**Common Questions**:

**Q: Can I increase the quota?**
A: No. The 1,500 load jobs/table/day limit is a hard limit from Google that cannot be increased.

**Q: Will batching lose data?**
A: No. Batching includes atexit hooks that flush all pending records on process exit. Failed flushes are logged and retried.

**Q: What if I need real-time writes?**
A: Use streaming inserts (different API) or Cloud Logging for real-time needs. This system batches for quota efficiency, not real-time delivery.

**Q: Can I disable batching?**
A: Yes. Set `BQ_BATCH_WRITER_ENABLED=false` environment variable. But quota will fill up again.

---

**Ready to deploy?** Run Step 1 to commit changes, then proceed through all steps.
