# workflows/late-night-recovery.yaml
# NBA Late Night Recovery Workflow - Enhanced PBP collection + comprehensive retry
# Runs daily at 2 AM PT to collect Enhanced PBP (available 2+ hours after games)
# Also retries all post-game scrapers using Option 1 approach (run everything)
# VERSION: 1.0 - New workflow for recovery strategy

main:
  params: [args]
  steps:
    - init:
        assign:
          - current_timestamp: ${sys.now()}
          - workflow_start: ${sys.now()}
          - execution_id: ${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}

    - log_workflow_start:
        call: sys.log
        args:
          text: "Starting NBA Late Night Recovery Workflow"
          severity: INFO

    # PHASE 1: Enhanced PBP Collection (primary purpose - definitely available by 2AM)
    - enhanced_pbp_collection:
        try:
          steps:
            - call_enhanced_pbp:
                call: run_scraper
                args:
                  scraper_name: "pbp-enhanced-pbp"
                  scraper_class: "PbpStatsEnhancedPbpScraper"
                  endpoint: "pbp_enhanced_pbp"
                  timeout: 600
                  critical: false
                result: enhanced_pbp_result
        except:
          as: e
          steps:
            - log_pbp_error:
                call: sys.log
                args:
                  text: "Enhanced PBP collection failed - will retry at 6AM"
                  severity: WARNING
            - assign_pbp_failure:
                assign:
                  - enhanced_pbp_result:
                      status: "failure"
                      error: ${e.message}
                      scraper_name: "pbp-enhanced-pbp"
                      timestamp: ${sys.now()}

    # PHASE 2: Post-Game Data Recovery (retry all post-game scrapers)
    - post_game_data_recovery:
        try:
          parallel:
            exception_policy: continueAll
            branches:
              - bdl_box_scores_recovery:
                  steps:
                    - call_bdl_box_scores:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-box-scores"
                          scraper_class: "BdlBoxScoresScraper"
                          endpoint: "bdl_box_scores"
                          timeout: 600
                          critical: false
                        result: bdl_box_scores_result

              - bdl_player_stats_recovery:
                  steps:
                    - call_bdl_player_box_scores:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-player-box-scores"
                          scraper_class: "BdlPlayerBoxScoresScraper"
                          endpoint: "bdl_player_box_scores"
                          timeout: 600
                          critical: false
                        result: bdl_player_box_result

              - player_averages_recovery:
                  steps:
                    - call_bdl_player_averages:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-player-averages"
                          scraper_class: "BdlPlayerAveragesScraper"
                          endpoint: "bdl_player_averages"
                          timeout: 300
                          critical: false
                        result: bdl_averages_result

              - advanced_stats_recovery:
                  steps:
                    - call_bdl_game_adv_stats:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-game-adv-stats"
                          scraper_class: "BdlGameAdvStatsScraper"
                          endpoint: "bdl_game_adv_stats"
                          timeout: 300
                          critical: false
                        result: bdl_adv_stats_result
        except:
          as: e
          steps:
            - log_post_game_error:
                call: sys.log
                args:
                  text: "Post-game data recovery had errors - non-critical"
                  severity: WARNING

    # PHASE 3: Status Updates Recovery (injury reports and player movement)
    - status_updates_recovery:
        try:
          parallel:
            exception_policy: continueAll
            branches:
              - injury_updates:
                  steps:
                    - call_injury_report:
                        call: run_scraper
                        args:
                          scraper_name: "nba-injury-report"
                          scraper_class: "GetNbaComInjuryReport"
                          endpoint: "nbac_injury_report"
                          timeout: 300
                          critical: false
                        result: injury_result

              - player_movement_updates:
                  steps:
                    - call_player_movement:
                        call: run_scraper
                        args:
                          scraper_name: "nba-player-movement"
                          scraper_class: "GetNbaComPlayerMovement"
                          endpoint: "nbac_player_movement"
                          timeout: 300
                          critical: false
                        result: movement_result
        except:
          as: e
          steps:
            - log_status_updates_error:
                call: sys.log
                args:
                  text: "Status updates recovery had errors - non-critical"
                  severity: WARNING

    # SUCCESS PATH
    - workflow_success:
        assign:
          - workflow_end: ${sys.now()}
          - total_duration: ${workflow_end - workflow_start}

    - write_status_success:
        call: write_status_to_gcs
        args:
          workflow_name: "late-night-recovery"
          execution_id: ${execution_id}
          execution_time: ${current_timestamp}
          workflow_start: ${workflow_start}
          total_duration: ${total_duration}
          scrapers:
            pbp_enhanced_pbp: ${enhanced_pbp_result}
            bdl_box_scores: ${bdl_box_scores_result}
            bdl_player_box_scores: ${bdl_player_box_result}
            bdl_player_averages: ${bdl_averages_result}
            bdl_game_adv_stats: ${bdl_adv_stats_result}
            nbac_injury_report: ${injury_result}
            nbac_player_movement: ${movement_result}
          recovery_strategy: "Option 1 - Run all relevant scrapers"
          status: "SUCCESS"

    - log_workflow_success:
        call: sys.log
        args:
          text: "NBA Late Night Recovery Workflow completed successfully"
          severity: INFO

    - return_success:
        return:
          status: "SUCCESS"
          message: "Late night recovery completed successfully"
          duration_seconds: ${total_duration}
          enhanced_pbp_status: ${enhanced_pbp_result.status}
          post_game_recovery_attempted: true
          status_updates_attempted: true
          timestamp: ${current_timestamp}

# Reusable subworkflow for running individual scrapers
run_scraper:
  params: [scraper_name, scraper_class, endpoint, timeout, critical]
  steps:
    - log_start:
        call: sys.log
        args:
          text: "Starting scraper"
          severity: INFO

    - call_scraper:
        try:
          call: http.post
          args:
            url: "https://nba-scrapers-756957797294.us-west2.run.app/scrape"
            query:
              scraper: ${endpoint}
            timeout: ${timeout}
            headers:
              Content-Type: "application/json"
          result: scraper_response
        except:
          as: e
          steps:
            - log_failure:
                call: sys.log
                args:
                  text: "Scraper failed"
                  severity: ERROR
            - return_failure:
                return:
                  status: "failure"
                  scraper_name: ${scraper_name}
                  error: ${e.message}
                  timestamp: ${sys.now()}

    - check_response:
        switch:
          - condition: ${scraper_response.code >= 200 AND scraper_response.code < 300}
            next: log_success
          - condition: true
            next: return_http_failure

    - return_http_failure:
        return:
          status: "failure"
          scraper_name: ${scraper_name}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

    - log_success:
        call: sys.log
        args:
          text: "Scraper completed successfully"
          severity: INFO

    - return_success:
        return:
          status: "success"
          scraper_name: ${scraper_name}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

# Subworkflow for writing status to GCS
write_status_to_gcs:
  params: [workflow_name, execution_id, execution_time, workflow_start, scrapers, status, total_duration, dependencies, recovery_strategy]
  steps:
    - calculate_duration:
        assign:
          - actual_duration: ${default(total_duration, sys.now() - workflow_start)}
          - current_time: ${sys.now()}
          - date_string: ${text.split(text.split(string(current_time), "T")[0], "-")}
          - year: ${date_string[0]}
          - month: ${date_string[1]} 
          - day: ${date_string[2]}
          - hour_min: ${text.replace_all(text.split(string(current_time), "T")[1], ":", "h")}
          - time_part: ${text.split(hour_min, ".")[0]}

    - build_status_object:
        assign:
          - status_data:
              workflow: ${workflow_name}
              execution_id: ${execution_id}
              execution_time: ${execution_time}
              total_duration: ${actual_duration}
              status: ${status}
              scrapers: ${scrapers}
              dependencies: ${default(dependencies, {})}
              recovery_strategy: ${default(recovery_strategy, "Standard execution")}

    - create_gcs_path:
        assign:
          - bucket_name: "nba-props-status"
          - file_path: ${"workflow-status/" + year + "-" + month + "-" + day + "/" + workflow_name + "-" + time_part + ".json"}
          - gcs_url: ${"gs://" + bucket_name + "/" + file_path}

    - write_to_gcs:
        try:
          call: http.post
          args:
            url: ${"https://storage.googleapis.com/upload/storage/v1/b/" + bucket_name + "/o"}
            query:
              uploadType: "media"
              name: ${file_path}
            headers:
              Content-Type: "application/json"
              Authorization: ${"Bearer " + sys.get_env("GOOGLE_CLOUD_ACCESS_TOKEN")}
            body: ${json.encode(status_data)}
        except:
          as: e
          steps:
            - log_gcs_failure:
                call: sys.log
                args:
                  text: "Failed to write status to GCS"
                  severity: WARNING

    - log_status_written:
        call: sys.log
        args:
          text: ${"Status written to GCS: " + gcs_url}
          severity: INFO
          