# workflows/operational/post-game-collection.yaml
# NBA Post-Game Collection Workflow - Core game data after games complete
# Runs at 8 PM PT (early games) and 11 PM PT (late games + retries)
# Same workflow triggered by 2 different schedulers
# SIMPLIFIED: Only 2 core scrapers for post-game data collection
# VERSION: 2.0 - Optimized based on workflows reference

main:
  params: [args]
  steps:
    - init:
        assign:
          - current_timestamp: ${sys.now()}
          - workflow_start: ${sys.now()}
          - execution_id: ${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}

    - log_workflow_start:
        call: sys.log
        args:
          text: "Starting NBA Post-Game Collection Workflow (2 scrapers)"
          severity: INFO

    # CORE POST-GAME DATA (parallel - no dependencies)
    - core_post_game_data:
        try:
          parallel:
            exception_policy: continueAll
            branches:
              - nba_official_scoreboard:
                  steps:
                    - call_nba_scoreboard:
                        call: run_scraper
                        args:
                          scraper_name: "nba-scoreboard"
                          scraper_class: "GetNbaComScoreboardV2"
                          endpoint: "nbac_scoreboard_v2"
                          timeout: 300
                          critical: false
                        result: nba_scoreboard_result

              - bdl_box_scores:
                  steps:
                    - call_bdl_box_scores:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-box-scores"
                          scraper_class: "BdlBoxScoresScraper"
                          endpoint: "bdl_box_scores"
                          timeout: 600
                          critical: false
                        result: bdl_box_scores_result
        except:
          as: e
          steps:
            - log_post_game_error:
                call: sys.log
                args:
                  text: "Post-game data collection had errors - non-critical"
                  severity: WARNING

    # SUCCESS PATH (always succeeds - best effort workflow)
    - workflow_success:
        assign:
          - workflow_end: ${sys.now()}
          - total_duration: ${workflow_end - workflow_start}

    - write_status_success:
        call: write_status_to_gcs
        args:
          workflow_name: "post-game-collection"
          execution_id: ${execution_id}
          execution_time: ${current_timestamp}
          workflow_start: ${workflow_start}
          total_duration: ${total_duration}
          scrapers:
            nba_scoreboard: ${default(nba_scoreboard_result.status, "not_run")}
            bdl_box_scores: ${default(bdl_box_scores_result.status, "not_run")}
          dependencies: "None - parallel execution"
          collection_strategy: "8PM early games, 11PM late games + retries"
          status: "SUCCESS"

    - log_workflow_success:
        call: sys.log
        args:
          text: "NBA Post-Game Collection Workflow completed (2/2 scrapers attempted)"
          severity: INFO

    - return_success:
        return:
          status: "SUCCESS"
          message: "Post-game collection completed successfully"
          duration_seconds: ${total_duration}
          scrapers_attempted: 2
          nba_scoreboard_status: ${default(nba_scoreboard_result.status, "not_run")}
          bdl_box_scores_status: ${default(bdl_box_scores_result.status, "not_run")}
          collection_window: "post_game_data"
          timestamp: ${current_timestamp}

# Reusable subworkflow for running individual scrapers
run_scraper:
  params: [scraper_name, scraper_class, endpoint, timeout, critical]
  steps:
    - log_start:
        call: sys.log
        args:
          text: ${"Starting scraper: " + scraper_name + " (" + scraper_class + ")"}
          severity: INFO

    - call_scraper:
        try:
          call: http.post
          args:
            url: "https://nba-scrapers-756957797294.us-west2.run.app/scrape"
            query:
              scraper: ${endpoint}
            timeout: ${timeout}
            headers:
              Content-Type: "application/json"
          result: scraper_response
        except:
          as: e
          steps:
            - log_failure:
                call: sys.log
                args:
                  text: ${"Scraper failed: " + scraper_name + " - " + e.message}
                  severity: ERROR
            - return_failure:
                return:
                  status: "failure"
                  scraper_name: ${scraper_name}
                  scraper_class: ${scraper_class}
                  error: ${e.message}
                  timestamp: ${sys.now()}

    - check_response:
        switch:
          - condition: ${scraper_response.code >= 200 AND scraper_response.code < 300}
            next: log_success
          - condition: true
            next: return_http_failure

    - return_http_failure:
        return:
          status: "failure"
          scraper_name: ${scraper_name}
          scraper_class: ${scraper_class}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

    - log_success:
        call: sys.log
        args:
          text: ${"Scraper completed successfully: " + scraper_name}
          severity: INFO

    - return_success:
        return:
          status: "success"
          scraper_name: ${scraper_name}
          scraper_class: ${scraper_class}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

# Subworkflow for writing status to GCS
write_status_to_gcs:
  params: [workflow_name, execution_id, execution_time, workflow_start, scrapers, status, total_duration, dependencies, collection_strategy]
  steps:
    - calculate_duration:
        assign:
          - actual_duration: ${default(total_duration, sys.now() - workflow_start)}
          - current_time: ${sys.now()}
          - bucket_name: "nba-props-status"
          - simple_filename: ${workflow_name + "-" + execution_id}
          - file_path: ${"workflow-status/" + simple_filename + ".json"}
          - gcs_url: ${"gs://" + bucket_name + "/" + file_path}

    - build_status_object:
        assign:
          - status_data:
              workflow: ${workflow_name}
              execution_id: ${execution_id}
              execution_time: ${execution_time}
              total_duration: ${actual_duration}
              status: ${status}
              scrapers: ${scrapers}
              dependencies: ${default(dependencies, null)}
              collection_strategy: ${default(collection_strategy, "Standard execution")}

    - write_to_gcs:
        try:
          call: http.post
          args:
            url: ${"https://storage.googleapis.com/upload/storage/v1/b/" + bucket_name + "/o"}
            query:
              uploadType: "media"
              name: ${file_path}
            headers:
              Content-Type: "application/json"
              Authorization: ${"Bearer " + sys.get_env("GOOGLE_CLOUD_ACCESS_TOKEN")}
            body: ${json.encode(status_data)}
        except:
          as: e
          steps:
            - log_gcs_failure:
                call: sys.log
                args:
                  text: ${"Failed to write status to GCS: " + e.message}
                  severity: WARNING

    - log_status_written:
        call: sys.log
        args:
          text: ${"Status written to GCS: " + gcs_url}
          severity: INFO
          