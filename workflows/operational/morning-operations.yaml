# workflows/morning-operations.yaml
# NBA Morning Operations Workflow - Daily setup and foundation data collection
# Runs daily at 8 AM PT to prepare for the day's activities
# Focuses on player intelligence, schedule monitoring, and foundation information
# VERSION: 3.0 - Streamlined to match optimized workflow reference

main:
  params: [args]
  steps:
    - init:
        assign:
          - current_timestamp: ${sys.now()}
          - workflow_start: ${sys.now()}
          - execution_id: ${sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")}

    - log_workflow_start:
        call: sys.log
        args:
          text: "Starting NBA Morning Operations Workflow"
          severity: INFO

    # PHASE 1: Player Movement + Schedule Monitoring
    - player_movement_and_schedule:
        try:
          parallel:
            exception_policy: continueAll
            branches:
              - player_movement:
                  steps:
                    - call_player_movement:
                        call: run_scraper
                        args:
                          scraper_name: "nba-player-movement"
                          scraper_class: "GetNbaComPlayerMovement"
                          endpoint: "nbac_player_movement"
                          timeout: 300
                          critical: false
                        result: movement_result

              - nba_schedule_api:
                  steps:
                    - call_nba_schedule_api:
                        call: run_scraper
                        args:
                          scraper_name: "nba-schedule-api"
                          scraper_class: "GetNbaComScheduleApi"
                          endpoint: "nbac_schedule_api"
                          timeout: 300
                          critical: true
                        result: schedule_api_result

              - nba_schedule_cdn:
                  steps:
                    - call_nba_schedule_cdn:
                        call: run_scraper
                        args:
                          scraper_name: "nba-schedule-cdn-backup"
                          scraper_class: "GetNbaComScheduleCdn"
                          endpoint: "nbac_schedule_cdn"
                          timeout: 300
                          critical: false
                        result: schedule_cdn_result
        except:
          as: e
          steps:
            - log_movement_schedule_error:
                call: sys.log
                args:
                  text: "Player movement and schedule updates had errors"
                  severity: WARNING

    # PHASE 2: Foundation Player Intelligence (Primary + Backup Sources)
    - foundation_player_intelligence:
        try:
          parallel:
            exception_policy: continueAll
            branches:
              - player_list_primary:
                  steps:
                    - call_player_list:
                        call: run_scraper
                        args:
                          scraper_name: "nba-player-list"
                          scraper_class: "GetNbaComPlayerList"
                          endpoint: "nbac_player_list"
                          timeout: 300
                          critical: true
                        result: player_list_result

              - injury_report_primary:
                  steps:
                    - call_injury_report:
                        call: run_scraper
                        args:
                          scraper_name: "nba-injury-report"
                          scraper_class: "GetNbaComInjuryReport"
                          endpoint: "nbac_injury_report"
                          timeout: 300
                          critical: true
                        result: injury_result

              - injury_report_backup:
                  steps:
                    - call_bdl_injuries:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-injuries-backup"
                          scraper_class: "BdlInjuriesScraper"
                          endpoint: "bdl_injuries"
                          timeout: 300
                          critical: false
                        result: bdl_injuries_result

              - bdl_players_validation:
                  steps:
                    - call_bdl_players:
                        call: run_scraper
                        args:
                          scraper_name: "bdl-active-players"
                          scraper_class: "BdlActivePlayersScraper"
                          endpoint: "bdl_active_players"
                          timeout: 300
                          critical: false
                        result: bdl_players_result
        except:
          as: e
          steps:
            - log_foundation_error:
                call: sys.log
                args:
                  text: "Foundation player intelligence had errors"
                  severity: WARNING

    # PHASE 3: League Context
    - league_context:
        try:
          steps:
            - call_bdl_standings:
                call: run_scraper
                args:
                  scraper_name: "bdl-standings"
                  scraper_class: "BdlStandingsScraper"
                  endpoint: "bdl_standings"
                  timeout: 300
                  critical: false
                result: standings_result
        except:
          as: e
          steps:
            - log_league_context_error:
                call: sys.log
                args:
                  text: "League context collection had errors - non-critical"
                  severity: WARNING
            - assign_standings_failure:
                assign:
                  - standings_result:
                      status: "failure"
                      error: ${e.message}
                      scraper_name: "bdl-standings"
                      timestamp: ${sys.now()}

    # SUCCESS PATH
    - workflow_success:
        assign:
          - workflow_end: ${sys.now()}
          - total_duration: ${workflow_end - workflow_start}

    - write_status_success:
        call: write_status_to_gcs
        args:
          workflow_name: "morning-operations"
          execution_id: ${execution_id}
          execution_time: ${current_timestamp}
          workflow_start: ${workflow_start}
          total_duration: ${total_duration}
          scrapers:
            nbac_player_movement: ${movement_result}
            nbac_schedule_api: ${schedule_api_result}
            nbac_schedule_cdn: ${schedule_cdn_result}
            nbac_player_list: ${player_list_result}
            nbac_injury_report: ${injury_result}
            bdl_injuries: ${bdl_injuries_result}
            bdl_active_players: ${bdl_players_result}
            bdl_standings: ${standings_result}
          dependencies: {}
          monitoring_strategy: "Schedule API vs CDN, NBA.com vs BDL injuries"
          status: "SUCCESS"

    - log_workflow_success:
        call: sys.log
        args:
          text: "NBA Morning Operations Workflow completed successfully"
          severity: INFO

    - return_success:
        return:
          status: "SUCCESS"
          message: "Morning operations completed successfully"
          duration_seconds: ${total_duration}
          player_movement_updated: true
          schedule_monitoring_active: true
          schedule_api_status: ${schedule_api_result.status}
          schedule_cdn_status: ${schedule_cdn_result.status}
          foundation_data_refreshed: true
          injury_monitoring_active: true
          injury_primary_status: ${injury_result.status}
          injury_backup_status: ${bdl_injuries_result.status}
          league_context_updated: true
          timestamp: ${current_timestamp}

# Reusable subworkflow for running individual scrapers
run_scraper:
  params: [scraper_name, scraper_class, endpoint, timeout, critical]
  steps:
    - log_start:
        call: sys.log
        args:
          text: "Starting scraper"
          severity: INFO

    - call_scraper:
        try:
          call: http.post
          args:
            url: "https://nba-scrapers-756957797294.us-west2.run.app/scrape"
            query:
              scraper: ${endpoint}
            timeout: ${timeout}
            headers:
              Content-Type: "application/json"
          result: scraper_response
        except:
          as: e
          steps:
            - log_failure:
                call: sys.log
                args:
                  text: "Scraper failed"
                  severity: ERROR
            - return_failure:
                return:
                  status: "failure"
                  scraper_name: ${scraper_name}
                  error: ${e.message}
                  timestamp: ${sys.now()}

    - check_response:
        switch:
          - condition: ${scraper_response.code >= 200 AND scraper_response.code < 300}
            next: log_success
          - condition: true
            next: return_http_failure

    - return_http_failure:
        return:
          status: "failure"
          scraper_name: ${scraper_name}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

    - log_success:
        call: sys.log
        args:
          text: "Scraper completed successfully"
          severity: INFO

    - return_success:
        return:
          status: "success"
          scraper_name: ${scraper_name}
          http_code: ${scraper_response.code}
          timestamp: ${sys.now()}

# Subworkflow for writing status to GCS
write_status_to_gcs:
  params: [workflow_name, execution_id, execution_time, workflow_start, scrapers, status, total_duration, dependencies, monitoring_strategy]
  steps:
    - calculate_duration:
        assign:
          - actual_duration: ${default(total_duration, sys.now() - workflow_start)}
          - current_time: ${sys.now()}
          - bucket_name: "nba-props-status"
          - simple_filename: ${workflow_name + "-" + execution_id}
          - file_path: ${"workflow-status/" + simple_filename + ".json"}
          - gcs_url: ${"gs://" + bucket_name + "/" + file_path}

    - build_status_object:
        assign:
          - status_data:
              workflow: ${workflow_name}
              execution_id: ${execution_id}
              execution_time: ${execution_time}
              total_duration: ${actual_duration}
              status: ${status}
              scrapers: ${scrapers}
              dependencies: ${default(dependencies, null)}
              monitoring_strategy: ${default(monitoring_strategy, "Standard execution")}

    - write_to_gcs:
        try:
          call: http.post
          args:
            url: ${"https://storage.googleapis.com/upload/storage/v1/b/" + bucket_name + "/o"}
            query:
              uploadType: "media"
              name: ${file_path}
            headers:
              Content-Type: "application/json"
              Authorization: ${"Bearer " + sys.get_env("GOOGLE_CLOUD_ACCESS_TOKEN")}
            body: ${json.encode(status_data)}
        except:
          as: e
          steps:
            - log_gcs_failure:
                call: sys.log
                args:
                  text: "Failed to write status to GCS"
                  severity: WARNING

    - log_status_written:
        call: sys.log
        args:
          text: "Status written to GCS"
          severity: INFO