-- schemas/bigquery/nba_orchestration/workflow_executions.sql
-- ============================================================================
-- NBA Props Platform - Phase 1 Orchestration: Workflow Executions
-- ============================================================================
-- Purpose: Track workflow execution attempts and link to scraper runs
-- Update: Every workflow execution (after decision to RUN)
-- Entities: All executed workflows
-- Retention: 90 days (partition expiration)
--
-- Version: 1.0
-- Date: November 12, 2025
-- Status: Production-Ready
--
-- Workflow Execution Flow:
--   1. Master Controller logs RUN decision → workflow_decisions
--   2. Workflow Executor reads RUN decision
--   3. Executor calls scrapers → scraper_execution_log
--   4. Executor logs execution → workflow_executions (THIS TABLE)
--
-- Use Cases:
--   - End-to-end execution tracking (decision → execution → scraper results)
--   - Workflow success rate analysis
--   - Execution duration monitoring
--   - Debugging failed workflows
--
-- Dependencies: workflow_decisions, scraper_execution_log
-- ============================================================================

CREATE TABLE IF NOT EXISTS `nba-props-platform.nba_orchestration.workflow_executions` (
  -- ==========================================================================
  -- IDENTIFIERS (2 fields)
  -- ==========================================================================
  
  execution_id STRING NOT NULL,
    -- Unique execution identifier (UUID)
    -- Format: Full UUID generated by workflow executor
    -- Example: "550e8400-e29b-41d4-a716-446655440000"
    -- Used for: Correlation, debugging specific executions
  
  execution_time TIMESTAMP NOT NULL,
    -- When workflow execution started in UTC
    -- Partition key (daily partitions)
    -- Used for: Time-based queries, duration analysis
    -- Always filter on this field for efficient queries
  
  -- ==========================================================================
  -- WORKFLOW CONTEXT (2 fields)
  -- ==========================================================================
  
  workflow_name STRING NOT NULL,
    -- Workflow being executed
    -- Examples:
    --   'betting_lines' (pre-game odds collection)
    --   'morning_operations' (daily foundation data)
    --   'post_game_window_1' (first post-game collection)
    -- Links to: workflow_decisions.workflow_name
    -- Used for: Grouping by workflow, success rate analysis
  
  decision_id STRING,
    -- Links to workflow_decisions table
    -- Format: UUID from workflow_decisions.decision_id
    -- NULL for: Manual executions without decision
    -- Used for: Tracing decision → execution flow
  
  -- ==========================================================================
  -- EXECUTION DETAILS (4 fields)
  -- ==========================================================================
  
  scrapers_requested ARRAY<STRING>,
    -- List of scraper names that should execute
    -- Examples:
    --   ['oddsa_events', 'oddsa_player_props']
    --   ['nbac_injury_report']
    -- From: workflow_decisions.scrapers_triggered
    -- Used for: Understanding workflow composition
  
  scrapers_triggered INT64 NOT NULL,
    -- Number of scrapers actually called
    -- Should equal: LENGTH(scrapers_requested)
    -- Less than requested if: errors occurred
    -- Used for: Detecting incomplete executions
  
  scrapers_succeeded INT64 NOT NULL,
    -- Number of scrapers that completed successfully or returned no_data
    -- Includes both 'success' and 'no_data' statuses from scraper_execution_log
    -- Used for: Success rate calculation
  
  scrapers_failed INT64 NOT NULL,
    -- Number of scrapers that failed
    -- Status 'failed' from scraper_execution_log
    -- Used for: Failure rate calculation
  
  -- ==========================================================================
  -- SCRAPER EXECUTION TRACKING (1 field)
  -- ==========================================================================
  
  scraper_execution_ids ARRAY<STRING>,
    -- Links to scraper_execution_log table
    -- Format: Array of execution_id values from scraper_execution_log
    -- Example: ["abc123", "def456", "ghi789"]
    -- Used for: Tracing workflow → scraper executions
    -- Query pattern:
    --   SELECT * FROM scraper_execution_log 
    --   WHERE execution_id IN UNNEST(scraper_execution_ids)
  
  -- ==========================================================================
  -- STATUS (3 fields)
  -- ==========================================================================
  
  status STRING NOT NULL,
    -- Overall workflow execution status
    -- Values:
    --   'started' = Execution began but not finished
    --   'in_progress' = Some scrapers complete, others running
    --   'completed' = All scrapers attempted (may have failures)
    --   'failed' = Workflow-level failure (couldn't start scrapers)
    -- Used for: Monitoring workflow health
  
  duration_seconds FLOAT64,
    -- Total execution duration in seconds
    -- From: execution_time to completion
    -- NULL if: Still in progress or failed before completion
    -- Used for: Performance analysis, timeout detection
  
  error_message STRING,
    -- Error message if workflow failed
    -- NULL if: status != 'failed'
    -- Example: "Failed to resolve parameters for scraper X"
    -- Used for: Debugging workflow failures
  
  -- ==========================================================================
  -- METADATA (1 field)
  -- ==========================================================================
  
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
    -- Row creation timestamp (auto-populated by BigQuery)
    -- Used for: Audit trail, data freshness checks

)
PARTITION BY DATE(execution_time)
CLUSTER BY workflow_name, status
OPTIONS(
  description = "Tracks workflow execution attempts and links to scraper runs. Enables end-to-end tracing from decision → execution → scraper results. Partition key: execution_time (daily). Cluster by: workflow_name, status. CRITICAL TABLE for Phase 1 orchestration monitoring.",
  partition_expiration_days = 90
);

-- ============================================================================
-- FIELD SUMMARY
-- ============================================================================
-- Total fields: 13
--   - Identifiers: 2 (execution_id, execution_time)
--   - Workflow context: 2 (workflow_name, decision_id)
--   - Execution details: 4 (scrapers_requested, scrapers_triggered, 
--                            scrapers_succeeded, scrapers_failed)
--   - Scraper tracking: 1 (scraper_execution_ids)
--   - Status: 3 (status, duration_seconds, error_message)
--   - Metadata: 1 (created_at)
-- ============================================================================

-- ============================================================================
-- SAMPLE ROW (Successful Execution)
-- ============================================================================
/*
{
  "execution_id": "550e8400-e29b-41d4-a716-446655440000",
  "execution_time": "2025-01-15T16:05:00Z",
  "workflow_name": "betting_lines",
  "decision_id": "660e8400-e29b-41d4-a716-446655440001",
  "scrapers_requested": ["oddsa_events", "oddsa_player_props", "oddsa_game_lines"],
  "scrapers_triggered": 3,
  "scrapers_succeeded": 3,
  "scrapers_failed": 0,
  "scraper_execution_ids": ["abc123def", "ghi456jkl", "mno789pqr"],
  "status": "completed",
  "duration_seconds": 12.5,
  "error_message": null,
  "created_at": "2025-01-15T16:05:12Z"
}
*/

-- ============================================================================
-- SAMPLE ROW (Partial Failure)
-- ============================================================================
/*
{
  "execution_id": "770e8400-e29b-41d4-a716-446655440002",
  "execution_time": "2025-01-15T22:00:00Z",
  "workflow_name": "post_game_window_1",
  "decision_id": "880e8400-e29b-41d4-a716-446655440003",
  "scrapers_requested": ["bdl_box_scores", "nbac_team_boxscore", "nbac_player_boxscore"],
  "scrapers_triggered": 3,
  "scrapers_succeeded": 2,
  "scrapers_failed": 1,
  "scraper_execution_ids": ["stu123vwx", "yz456abc", "def789ghi"],
  "status": "completed",
  "duration_seconds": 8.2,
  "error_message": null,
  "created_at": "2025-01-15T22:00:08Z"
}
*/

-- ============================================================================
-- SAMPLE ROW (Workflow Failure)
-- ============================================================================
/*
{
  "execution_id": "990e8400-e29b-41d4-a716-446655440004",
  "execution_time": "2025-01-15T10:00:00Z",
  "workflow_name": "morning_operations",
  "decision_id": "aa0e8400-e29b-41d4-a716-446655440005",
  "scrapers_requested": ["nbac_schedule_api", "nbac_player_list", "bdl_standings"],
  "scrapers_triggered": 0,
  "scrapers_succeeded": 0,
  "scrapers_failed": 0,
  "scraper_execution_ids": [],
  "status": "failed",
  "duration_seconds": null,
  "error_message": "Failed to build workflow context: Schedule service unavailable",
  "created_at": "2025-01-15T10:00:00Z"
}
*/

-- ============================================================================
-- VALIDATION QUERIES
-- ============================================================================

-- Query 1: Today's workflow executions summary
-- Purpose: Quick overview of all workflow activity today
-- Expected: All workflows executed, most completed successfully
SELECT 
  workflow_name,
  status,
  COUNT(*) as executions,
  AVG(duration_seconds) as avg_duration,
  SUM(scrapers_triggered) as total_scrapers,
  SUM(scrapers_succeeded) as total_succeeded,
  SUM(scrapers_failed) as total_failed
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) = CURRENT_DATE()
GROUP BY workflow_name, status
ORDER BY workflow_name, status;

-- Query 2: Link workflow execution to scraper results
-- Purpose: Trace complete execution flow
-- Expected: All scraper executions linked back to workflow
WITH workflow_exec AS (
  SELECT 
    execution_id,
    workflow_name,
    execution_time,
    scraper_execution_ids
  FROM `nba-props-platform.nba_orchestration.workflow_executions`
  WHERE execution_id = '550e8400-e29b-41d4-a716-446655440000'
)
SELECT 
  we.workflow_name,
  we.execution_time,
  sel.scraper_name,
  sel.status,
  sel.triggered_at,
  sel.duration_seconds
FROM workflow_exec we,
UNNEST(we.scraper_execution_ids) AS scraper_id
JOIN `nba-props-platform.nba_orchestration.scraper_execution_log` sel
  ON sel.execution_id = scraper_id
ORDER BY sel.triggered_at;

-- Query 3: Workflow success rate (last 7 days)
-- Purpose: Monitor workflow reliability
-- Expected: >95% success rate for critical workflows
SELECT 
  workflow_name,
  COUNT(*) as total_executions,
  COUNTIF(status = 'completed') as completed,
  COUNTIF(status = 'failed') as failed,
  ROUND(COUNTIF(status = 'completed') / COUNT(*) * 100, 1) as success_rate_pct
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY workflow_name
ORDER BY success_rate_pct ASC;

-- Query 4: Execution duration trends
-- Purpose: Detect slow workflows or performance issues
-- Expected: Stable durations, no sudden spikes
SELECT 
  workflow_name,
  DATE(execution_time) as date,
  COUNT(*) as executions,
  ROUND(AVG(duration_seconds), 1) as avg_duration,
  ROUND(MAX(duration_seconds), 1) as max_duration,
  ROUND(STDDEV(duration_seconds), 1) as stddev_duration
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
  AND status = 'completed'
GROUP BY workflow_name, DATE(execution_time)
ORDER BY workflow_name, date DESC;

-- Query 5: Failed workflow analysis
-- Purpose: Identify patterns in workflow failures
-- Expected: Few failures, clear error patterns
SELECT 
  workflow_name,
  error_message,
  COUNT(*) as failure_count,
  MAX(execution_time) as last_failure
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
  AND status = 'failed'
GROUP BY workflow_name, error_message
ORDER BY failure_count DESC;

-- Query 6: Scraper failure rates within workflows
-- Purpose: Identify which scrapers fail most often
-- Expected: Low failure rates, identify problem scrapers
WITH scraper_stats AS (
  SELECT 
    workflow_name,
    scrapers_triggered,
    scrapers_succeeded,
    scrapers_failed
  FROM `nba-props-platform.nba_orchestration.workflow_executions`
  WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
    AND status = 'completed'
)
SELECT 
  workflow_name,
  SUM(scrapers_triggered) as total_scrapers,
  SUM(scrapers_succeeded) as total_succeeded,
  SUM(scrapers_failed) as total_failed,
  ROUND(SUM(scrapers_failed) / SUM(scrapers_triggered) * 100, 1) as failure_rate_pct
FROM scraper_stats
GROUP BY workflow_name
ORDER BY failure_rate_pct DESC;

-- ============================================================================
-- MONITORING QUERIES
-- ============================================================================

-- Alert: Workflow not executed today despite decision
-- Threshold: Every RUN decision should have execution
SELECT 
  'd.workflow_name' as alert_source,
  d.workflow_name,
  d.decision_time,
  d.decision_id
FROM `nba-props-platform.nba_orchestration.workflow_decisions` d
LEFT JOIN `nba-props-platform.nba_orchestration.workflow_executions` e
  ON d.decision_id = e.decision_id
WHERE d.action = 'RUN'
  AND DATE(d.decision_time) = CURRENT_DATE()
  AND e.execution_id IS NULL;

-- Alert: High workflow failure rate
-- Threshold: >20% failures in last hour indicates system issues
SELECT 
  'workflow_executions' as alert_source,
  workflow_name,
  COUNT(*) as executions,
  COUNTIF(status = 'failed') as failures,
  ROUND(COUNTIF(status = 'failed') / COUNT(*) * 100, 1) as failure_rate_pct
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE execution_time > TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR)
GROUP BY workflow_name
HAVING failure_rate_pct > 20;

-- Alert: Slow workflow execution
-- Threshold: >2x average duration indicates performance issue
WITH avg_durations AS (
  SELECT 
    workflow_name,
    AVG(duration_seconds) as avg_duration
  FROM `nba-props-platform.nba_orchestration.workflow_executions`
  WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
    AND status = 'completed'
  GROUP BY workflow_name
)
SELECT 
  'workflow_executions' as alert_source,
  e.workflow_name,
  e.execution_id,
  e.execution_time,
  e.duration_seconds,
  a.avg_duration as avg_duration_7d
FROM `nba-props-platform.nba_orchestration.workflow_executions` e
JOIN avg_durations a ON e.workflow_name = a.workflow_name
WHERE DATE(e.execution_time) = CURRENT_DATE()
  AND e.status = 'completed'
  AND e.duration_seconds > a.avg_duration * 2;

-- ============================================================================
-- HELPER VIEWS
-- ============================================================================

-- View: Today's execution summary
CREATE OR REPLACE VIEW `nba-props-platform.nba_orchestration.v_todays_executions` AS
SELECT 
  workflow_name,
  COUNT(*) as total_executions,
  COUNTIF(status = 'completed') as completed,
  COUNTIF(status = 'failed') as failed,
  ROUND(AVG(duration_seconds), 1) as avg_duration_seconds,
  SUM(scrapers_triggered) as total_scrapers_triggered,
  SUM(scrapers_succeeded) as total_scrapers_succeeded,
  SUM(scrapers_failed) as total_scrapers_failed,
  MAX(execution_time) as last_execution
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) = CURRENT_DATE()
GROUP BY workflow_name
ORDER BY workflow_name;

-- View: Recent workflow failures with details
CREATE OR REPLACE VIEW `nba-props-platform.nba_orchestration.v_recent_execution_failures` AS
SELECT 
  workflow_name,
  execution_id,
  execution_time,
  scrapers_requested,
  scrapers_triggered,
  error_message,
  decision_id
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time) >= DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY)
  AND status = 'failed'
ORDER BY execution_time DESC;

-- ============================================================================
-- DEPLOYMENT CHECKLIST
-- ============================================================================
-- [ ] Create table in nba_orchestration dataset
-- [ ] Verify partitioning (daily on execution_time)
-- [ ] Verify clustering (workflow_name, status)
-- [ ] Test with sample insert
-- [ ] Test scraper_execution_ids array field
-- [ ] Test join with scraper_execution_log
-- [ ] Test join with workflow_decisions
-- [ ] Enable monitoring queries
-- [ ] Configure alerts in Grafana
-- [ ] Document alert thresholds
-- [ ] Create helper views
-- ============================================================================
