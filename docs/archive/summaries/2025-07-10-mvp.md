# NBA Analytics Platform - Project Overview & Development History

**Document Created:** July 2025  
**Project Status:** Functional MVP with containerized microservices  
**Last Updated:** July 10, 2025  

## üèÄ Project Vision & Goals

### Primary Objective
Build a comprehensive NBA analytics platform that provides data-driven insights for NBA player prop bets by aggregating data from multiple sources, processing it through BigQuery, and serving predictions through a modern web interface.

### Core Features
- **Data Ingestion**: Scrape NBA player stats, game data, and prop bet odds from multiple APIs
- **Data Processing**: Transform raw data into actionable insights using BigQuery analytics
- **Prediction Engine**: Generate forecasts for player performance and prop bet recommendations
- **Real-time Updates**: Live data feeds for game day insights and prop bet monitoring
- **Web Interface**: User-friendly dashboard for viewing predictions and analytics

## üèóÔ∏è Architecture Overview

### Technology Stack Decision
After evaluating multiple approaches, we selected a **cloud-native, serverless architecture**:

**Storage Strategy:**
- **Raw Data**: Google Cloud Storage (JSON files)
- **Analytics**: BigQuery (all heavy SQL computations)
- **Serving**: Firestore (hot data with 3-day TTL) + GCS (detailed reports)
- **Local Development**: PostgreSQL, Redis, MinIO (mirrors production)

**Compute Strategy:**
- **Cloud Run**: Containerized microservices for scalability
- **Pub/Sub**: Event-driven orchestration between services
- **Docker**: Local development parity with production

**Key Architecture Principle**: *"Use managed services for heavy lifting, containers for business logic"*

### Microservices Architecture
```
External APIs ‚Üí Scrapers ‚Üí GCS ‚Üí Processors ‚Üí BigQuery ‚Üí ReportGen ‚Üí Firestore + Frontend
                   ‚Üì            ‚Üì              ‚Üì              ‚Üì
              (Cloud Run)  (Raw JSON)   (SQL Analytics)  (Hot Data)
```

## üìä Data Sources & APIs

### Primary Data Sources
1. **The Odds API** - Prop bet odds and lines (500 requests/month free tier)
2. **Ball Don't Lie API** - NBA player and game statistics  
3. **Big Ball Data** - Enhanced play-by-play data (2-hour delay)
4. **NBA.com + ESPN.com** - Backup data via scraping for roster updates

### Data Flow Pipeline
1. **Scrapers** retrieve raw JSON data from APIs daily
2. **Raw data** stored in GCS with timestamp-based organization
3. **Processors** trigger BigQuery jobs for data transformation
4. **Analytics** compute player trends, prop bet accuracy, predictions
5. **ReportGen** writes hot data to Firestore for fast frontend access

## üê≥ Development Environment

### Docker-Based Development
**Motivation**: Achieve production parity while enabling fast local development

**Container Architecture:**
- **Base Image** (`nba-base`): Shared dependencies and utilities (~200MB)
- **Scrapers Service**: API integration and data collection (~400MB total)
- **Processors Service**: Data orchestration and BigQuery integration (~350MB total)
- **ReportGen Service**: Analytics and Firestore updates (~300MB total)

**Local Services:**
- **PostgreSQL**: Development database for SQL testing
- **Redis**: Caching and message queuing
- **MinIO**: Local object storage (S3-compatible)

### Development Workflow
```bash
# Complete environment setup
nba-setup                    # Activates venv, switches GCP config, loads tools

# Development cycle  
nba-up                      # Start all containers
nba-test-events            # Test scraper functionality
nba-logs                   # Monitor real-time activity
nba-data                   # Check scraped data files
nba-down                   # Stop services
```

## üîß Technical Implementation

### Scraper Service Details
**Built with Flask microservice pattern**

**Two main scrapers:**
1. **Events Scraper** (`oddsa_events_his.py`): Retrieves NBA game schedules
   - Endpoint: `POST /scrape` (no eventId parameter)
   - Returns: Game events with IDs for props scraping
   
2. **Player Props Scraper** (`oddsa_player_props_his.py`): Gets betting odds
   - Endpoint: `POST /scrape` (requires eventId parameter)  
   - Returns: Prop bet lines for specific games

**Key Features:**
- **Auto-routing**: Request parameters determine which scraper runs
- **Error handling**: Comprehensive retry logic with exponential backoff
- **Export flexibility**: Development (local files) vs Production (GCS)
- **Health monitoring**: Built-in health checks and structured logging

### Sentry Integration
**Comprehensive monitoring and error tracking**

**Features:**
- **Transaction tracking**: Full request lifecycle monitoring
- **Error capture**: Automatic exception reporting with context
- **Performance monitoring**: Response times and resource usage
- **Custom tags**: Scraper type, sport, success/failure status
- **Environment-aware**: Different sampling rates for dev/staging/prod

**Benefits:**
- Proactive error detection before users report issues
- Performance bottleneck identification
- Usage pattern analysis for optimization

### Database Strategy Evolution
**Initial Consideration**: Complex local processing with DuckDB/Parquet  
**Final Decision**: BigQuery-centric approach

**Rationale:**
- **Simplicity**: SQL queries instead of complex pandas operations
- **Scalability**: BigQuery handles TB+ datasets automatically
- **Cost-effectiveness**: ~$15/month vs dedicated infrastructure
- **Maintenance**: Managed service reduces operational overhead

**Implementation:**
- **Processors become orchestrators**: Trigger BigQuery jobs, don't process data
- **BigQuery does heavy lifting**: Aggregations, analytics, ML features
- **Firestore serves results**: Fast reads for frontend (<10ms response)

## üõ†Ô∏è Monitoring & Operations

### Comprehensive Monitoring Toolkit
**Goal**: Enable efficient debugging and performance monitoring

**Tools Developed:**
1. **System Status Script** (`monitoring/scripts/system_status.sh`)
   - Container health and resource usage
   - API connectivity testing  
   - Recent activity summary
   - Error rate analysis

2. **Debug Analysis** (`monitoring/scripts/scraper_debug.sh`)
   - Log pattern analysis
   - Request lifecycle tracking
   - Performance bottleneck identification

3. **Shell Integration** (`.zshrc` additions)
   - One-command environment setup (`nba-setup`)
   - Quick status checks (`nba-status`, `nba-health`)
   - Development shortcuts (`nba-up`, `nba-test-events`)

### Operational Philosophy
**"Make the common case fast, the debugging case clear"**

- **Daily operations**: Single commands for status/testing
- **Problem diagnosis**: Structured logs with correlation IDs
- **Development velocity**: Hot reloading with volume mounts
- **Production readiness**: Same containers, different backends

## üìà Current Status & Achievements

### ‚úÖ Successfully Implemented
- **Containerized microservices** running on Docker with health checks
- **Working scrapers** successfully pulling real NBA data from Odds API
- **Data export system** with development/production environment switching
- **Comprehensive monitoring** with Sentry integration and custom tooling
- **Local development environment** with production parity
- **Error handling** with retry logic, debug data capture, and structured logging

### üß™ Validated & Tested
- **API connectivity**: Successfully retrieving 16+ NBA events from Odds API
- **Data processing**: JSON parsing and transformation working correctly
- **Container orchestration**: All services healthy and communicating
- **Export functionality**: Local file creation and GCS integration ready
- **Shell workflow**: Complete development environment setup in one command

### üìä Real Data Flowing
**Sample Success (from actual test):**
```json
{
  "status": "success", 
  "message": "Events scraping completed successfully",
  "scraper": "odds_api_historical_events",
  "data_summary": {
    "rowCount": 16,
    "sport": "basketball_nba", 
    "snapshot": "2025-01-08T23:55:38Z"
  }
}
```

## üöÄ Next Steps & Roadmap

### Phase 1: Complete Local MVP (Next 1-2 weeks)
- [ ] Test player props scraper with real eventIds from events data
- [ ] Implement basic data processing workflow (GCS ‚Üí BigQuery ‚Üí Firestore)
- [ ] Create simple analytics queries for player performance trends
- [ ] Build basic web interface for viewing scraped data

### Phase 2: Production Deployment (Next 2-4 weeks)  
- [ ] Deploy containers to Google Cloud Run
- [ ] Set up Cloud Scheduler for automated daily scraping
- [ ] Implement Pub/Sub orchestration between services
- [ ] Create BigQuery datasets and analytics tables
- [ ] Deploy Firestore for production data serving

### Phase 3: Advanced Analytics (Next 1-2 months)
- [ ] Develop player performance prediction models
- [ ] Implement prop bet recommendation engine
- [ ] Add historical accuracy tracking for predictions
- [ ] Create advanced analytics dashboard
- [ ] Implement real-time game day updates

### Phase 4: Production Features (Future)
- [ ] User authentication and personalized recommendations
- [ ] Mobile app development
- [ ] Advanced machine learning models
- [ ] Integration with additional data sources
- [ ] Automated prop bet tracking and results analysis

## üí° Key Learnings & Decisions

### Architecture Decisions
1. **Microservices over Monolith**: Enables independent scaling and deployment
2. **BigQuery over Local Processing**: Leverages managed services for complex analytics
3. **Docker for Development**: Achieves production parity and eliminates "works on my machine"
4. **Firestore for Serving**: Optimizes for fast reads over complex queries

### Development Practices
1. **Environment Parity**: Same containers locally and in production
2. **Comprehensive Monitoring**: Sentry integration from day one prevents blind spots
3. **Error Handling First**: Retry logic and debug data capture built into base classes
4. **Shell Integration**: Custom tooling reduces friction for daily development

### Technical Insights
1. **API Rate Limiting**: Odds API provides good free tier but requires careful usage
2. **Data Volume**: NBA generates significant data; efficient storage/processing essential
3. **Real-time Constraints**: Game day updates require sub-minute data freshness
4. **Cost Optimization**: Cloud usage patterns significantly impact monthly costs

## üèÜ Success Metrics

### Development Velocity
- **Environment setup**: Reduced from ~30 minutes to single command (`nba-setup`)
- **Testing cycle**: Reduced from manual multi-step process to one command (`nba-test-events`)
- **Debugging**: Structured logs enable rapid issue identification

### System Reliability  
- **Uptime**: All containers consistently healthy in local development
- **Error handling**: Graceful degradation with comprehensive error capture
- **Data consistency**: Successful API calls with proper data validation

### Cost Efficiency
- **Infrastructure**: Projected ~$15/month for full production deployment
- **Development**: Local development with no cloud costs
- **API usage**: Efficient API calls within free tier limits

## üìÅ Project Structure
```
nba-stats-scraper/
‚îú‚îÄ‚îÄ scrapers/           # Data collection microservice
‚îú‚îÄ‚îÄ processors/         # Data orchestration microservice  
‚îú‚îÄ‚îÄ reportgen/          # Analytics and serving microservice
‚îú‚îÄ‚îÄ shared/             # Common utilities and configurations
‚îú‚îÄ‚îÄ monitoring/         # Status monitoring and debugging tools
‚îú‚îÄ‚îÄ tools/              # Development helpers and testing scripts
‚îú‚îÄ‚îÄ docs/               # Documentation and guides
‚îú‚îÄ‚îÄ infra/              # Terraform infrastructure as code
‚îî‚îÄ‚îÄ docker-compose.dev.yml  # Local development environment
```

## üéØ Project Philosophy

**"Build for scale, develop for speed, monitor for reliability"**

This NBA analytics platform represents a modern approach to sports data analytics, combining:
- **Cloud-native architecture** for unlimited scalability
- **Local development parity** for rapid iteration
- **Comprehensive monitoring** for operational excellence
- **Data-driven decision making** for competitive advantage

The foundation is solid, the data is flowing, and the path to production is clear. üèÄ

---

*This document serves as both a historical record of development decisions and a reference for future development phases. It captures the journey from initial concept to working MVP, highlighting key technical decisions and their rationale.*
