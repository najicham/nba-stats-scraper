# Master Investigation and Fix Plan
**Date**: 2026-01-02 (Created) | 2026-01-03 (Updated)
**Status**: âœ… PHASE 1 COMPLETE - Root Cause Identified
**Next**: ðŸ”´ P0 - Execute Data Backfill (Phase 2)

---

## ðŸš¨ EXECUTIVE SUMMARY

**UPDATE (2026-01-03)**: âœ… **PHASE 1 INVESTIGATION COMPLETE**

**CRITICAL FINDING**: 95% of training data has missing values for key feature (`minutes_avg_last_10`), causing ML models to train on **imputed fake data** instead of real patterns.

**ROOT CAUSE (CONFIRMED)**: Historical data was NEVER processed/backfilled. Current processor works perfectly (validated on Nov 2025+ data showing ~40% NULL = legitimate DNP players).

**SOLUTION**: Backfill 2021-2024 data using current working processor. No code changes needed.

**TIMELINE UPDATE**: Originally estimated 40-60 hours (Phases 1-2). **Actual: 6-12 hours backfill only!** Massive time savings.

**NEXT ACTION**: Execute player_game_summary backfill (Step 1 in Phase 2 below).

---

## ðŸ“‹ INVESTIGATION ROADMAP

### Phase 1: Root Cause Investigation âœ… COMPLETE (Jan 2-3, 2026)

#### 1.1 Data Source Health Check

**Objective**: Determine where minutes_played data is lost in the pipeline

**Investigations**:

```sql
-- Investigation 1: Check raw data sources
-- Question: Do balldontlie, nba.com APIs provide minutes_played?

-- Check balldontlie boxscores
SELECT
  COUNT(*) as total_games,
  SUM(CASE WHEN min IS NULL THEN 1 ELSE 0 END) as null_minutes,
  SUM(CASE WHEN min IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100 as null_pct
FROM `nba-props-platform.nba_raw.bdl_player_boxscores`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01';

-- Check nba.com boxscores
SELECT
  COUNT(*) as total_games,
  SUM(CASE WHEN minutes IS NULL THEN 1 ELSE 0 END) as null_minutes,
  SUM(CASE WHEN minutes IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100 as null_pct
FROM `nba-props-platform.nba_raw.nbac_player_boxscores`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01';

-- Check gamebook data
SELECT
  COUNT(*) as total_games,
  SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) as null_minutes,
  SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100 as null_pct
FROM `nba-props-platform.nba_raw.nbac_gamebook_player_stats`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01';
```

**Expected Output**: Identify which raw source has minutes data

**Action Items**:
- [x] Run all 3 queries âœ…
- [x] Compare NULL rates across sources âœ…
- [x] Identify best source for minutes_played âœ… (BDL: 0% NULL, NBA.com: 0.42% NULL)
- [x] Document findings âœ…

**Results (Jan 3, 2026)**:
- BDL: 0.0% NULL (PERFECT)
- NBA.com: 0.42% NULL (EXCELLENT)
- Gamebook: 37.07% NULL (POOR)
- **Conclusion**: Raw data quality is excellent. Issue not at source layer.

---

#### 1.2 ETL Pipeline Trace

**Objective**: Find where data is dropped in transformation

**Investigations**:

```bash
# Investigation 2: Trace player_game_summary processor
# Location: data_processors/analytics/player_game_summary/

# Check processor logic
grep -r "minutes_played" data_processors/analytics/player_game_summary/

# Check if processor is selecting minutes field
# Expected: Should be pulling from one of the raw sources

# Check processor logs for errors
gcloud logging read 'resource.labels.service_name="nba-phase3-analytics-processors"
  AND textPayload=~"player_game_summary"
  AND severity>=WARNING' --limit 100
```

**Questions to Answer**:
1. Is minutes_played being selected from raw tables?
2. Is there a JOIN condition that's filtering out rows?
3. Are there data type conversion errors?
4. Has the processor logic changed recently?

**Action Items**:
- [x] Read player_game_summary processor code âœ…
- [x] Check SQL query for minutes_played selection âœ…
- [x] Review processor error logs âœ…
- [x] Compare current code vs git history âœ…
- [x] Document pipeline flow âœ…

**Results (Jan 3, 2026)**:
- Processor correctly selects minutes from raw tables
- Correctly parses minutes to decimal
- Correctly maps to minutes_played field
- **Conclusion**: No bugs found in current processor code

---

#### 1.3 Historical Gap vs Recent Regression

**Objective**: Determine if this is a new issue or longstanding gap

**Investigations**:

```sql
-- Investigation 3: Check minutes_played over time
SELECT
  DATE_TRUNC(game_date, MONTH) as month,
  COUNT(*) as total_games,
  SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) as null_count,
  ROUND(SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100, 2) as null_pct
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01'
GROUP BY month
ORDER BY month;

-- Check when data starts appearing
SELECT
  MIN(game_date) as first_date_with_data,
  MAX(game_date) as last_date_with_data,
  COUNT(DISTINCT game_date) as dates_with_data
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE minutes_played IS NOT NULL
  AND game_date >= '2021-10-01';
```

**Expected Output**: Timeline showing when NULL rate increased

**Scenarios**:
- **Scenario A**: Sudden jump in NULL rate at specific date â†’ Recent regression (rollback code)
- **Scenario B**: Consistently high NULL rate â†’ Historical gap (backfill needed)
- **Scenario C**: Gradually increasing NULL rate â†’ Data source degradation

**Action Items**:
- [x] Run temporal analysis query âœ…
- [x] Identify pattern âœ… (HISTORICAL GAP, not regression)
- [x] Plan backfill strategy âœ…
- [x] Document timeline âœ…

**Results (Jan 3, 2026)**:
- 2021-10 to 2024-04: 95-100% NULL (historical gap)
- 2024-10 to 2025-11: 95-100% NULL (still broken)
- 2025-12 to 2026-01: ~40% NULL (WORKING!)
- **Pattern**: Scenario B - Historical gap (data never processed)
- **Solution**: Backfill using current working processor

---

#### 1.4 Usage Rate Investigation

**Objective**: Determine why usage_rate is 100% NULL

**Investigations**:

```sql
-- Investigation 4: Check if usage_rate exists anywhere
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN usage_rate IS NULL THEN 1 ELSE 0 END) as null_count
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE game_date >= '2021-10-01';

-- Check raw sources for usage_rate
SELECT COUNT(*), SUM(CASE WHEN usg_pct IS NULL THEN 1 ELSE 0 END)
FROM `nba-props-platform.nba_raw.bdl_player_boxscores`
WHERE game_date >= '2021-10-01';

-- Check if we have components to calculate usage_rate
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN fg_attempts IS NULL THEN 1 ELSE 0 END) as null_fga,
  SUM(CASE WHEN ft_attempts IS NULL THEN 1 ELSE 0 END) as null_fta,
  SUM(CASE WHEN turnovers IS NULL THEN 1 ELSE 0 END) as null_tov
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE game_date >= '2021-10-01';
```

**Usage Rate Formula**:
```
USG% = 100 * ((FGA + 0.44*FTA + TOV) * (Tm_MP/5)) / (MP * (Tm_FGA + 0.44*Tm_FTA + Tm_TOV))
```

**Questions**:
1. Is usage_rate calculated or pulled from API?
2. Do we have all component fields (FGA, FTA, TOV, MP, team totals)?
3. If components exist, why isn't it calculated?

**Action Items**:
- [ ] Check if usage_rate is in raw data
- [ ] Verify component fields exist (FGA, FTA, TOV, MP)
- [ ] Check if team-level stats available for calculation
- [ ] Determine if we need to implement calculation
- [ ] Document requirements for usage_rate

---

#### 1.5 Precompute Pipeline Coverage Audit

**Objective**: Understand why precompute tables have 11-37% NULL rates

**Investigations**:

```sql
-- Investigation 5: Check precompute table coverage
-- Player composite factors
SELECT
  DATE_TRUNC(game_date, MONTH) as month,
  COUNT(*) as total,
  SUM(CASE WHEN fatigue_score IS NULL THEN 1 ELSE 0 END) as null_fatigue,
  SUM(CASE WHEN shot_zone_mismatch_score IS NULL THEN 1 ELSE 0 END) as null_zone
FROM `nba-props-platform.nba_precompute.player_composite_factors`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01'
GROUP BY month
ORDER BY month;

-- Team defense zone analysis
SELECT
  DATE_TRUNC(analysis_date, MONTH) as month,
  COUNT(*) as total,
  COUNT(DISTINCT team_abbr) as teams_covered
FROM `nba-props-platform.nba_precompute.team_defense_zone_analysis`
WHERE analysis_date >= '2021-10-01' AND analysis_date < '2024-05-01'
GROUP BY month
ORDER BY month;

-- Player daily cache
SELECT
  DATE_TRUNC(cache_date, MONTH) as month,
  COUNT(*) as total,
  SUM(CASE WHEN team_pace_last_10 IS NULL THEN 1 ELSE 0 END) as null_pace,
  SUM(CASE WHEN team_off_rating_last_10 IS NULL THEN 1 ELSE 0 END) as null_off
FROM `nba-props-platform.nba_precompute.player_daily_cache`
WHERE cache_date >= '2021-10-01' AND cache_date < '2024-05-01'
GROUP BY month
ORDER BY month;
```

**Questions**:
1. Were precompute tables backfilled for 2021-2024?
2. Are they only running for recent dates?
3. Are there dependency issues (Phase 3 data missing â†’ Phase 4 can't run)?

**Action Items**:
- [ ] Run coverage analysis for all 3 precompute tables
- [ ] Identify gaps in temporal coverage
- [ ] Check if processors ran for historical dates
- [ ] Determine if backfill is needed vs fix forward
- [ ] Document coverage gaps

---

### Phase 2: Data Pipeline Fixes (CRITICAL - Weeks 2-4)

#### 2.1 Fix minutes_played Collection

**Objective**: Restore minutes_played data to >95% completeness

**Implementation Plan**:

**Option A: If raw data has it** (Best case)
```python
# Fix player_game_summary processor to select minutes_played
# File: data_processors/analytics/player_game_summary/player_game_summary_processor.py

# Current (likely missing):
SELECT
  player_id,
  game_date,
  points,
  # Missing: minutes_played
  ...

# Fixed:
SELECT
  player_id,
  game_date,
  points,
  COALESCE(bdl.min, nbac.minutes, gamebook.minutes_played) as minutes_played,  # Add this
  ...
FROM ...
LEFT JOIN nba_raw.bdl_player_boxscores bdl ...
LEFT JOIN nba_raw.nbac_player_boxscores nbac ...
LEFT JOIN nba_raw.nbac_gamebook_player_stats gamebook ...
```

**Option B: If raw data doesn't have it** (Harder)
- Investigate why balldontlie/nba.com APIs aren't providing minutes
- Check if field name changed
- Consider alternative data sources

**Backfill Strategy**:
```bash
# Reprocess 2021-2024 data after fix
./bin/analytics/reprocess_player_game_summary.sh \
  --start-date 2021-10-01 \
  --end-date 2024-05-01 \
  --backfill-mode
```

**Validation**:
```sql
-- After fix, verify NULL rate <5%
SELECT
  COUNT(*) as total,
  SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) as null_count,
  ROUND(SUM(CASE WHEN minutes_played IS NULL THEN 1 ELSE 0 END) / COUNT(*) * 100, 2) as null_pct
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE game_date >= '2021-10-01' AND game_date < '2024-05-01';

-- Target: null_pct < 5%
```

**Action Items**:
- [ ] Identify best data source for minutes_played
- [ ] Update player_game_summary processor SQL
- [ ] Test on sample date range
- [ ] Run backfill for 2021-2024
- [ ] Validate data quality post-fix
- [ ] Document fix in processor changelog

**Estimated Effort**: 4-8 hours (depends on complexity)
**Expected Impact**: +7-12% MAE improvement

---

#### 2.2 Implement usage_rate Calculation

**Objective**: Calculate usage_rate from component stats

**Implementation**:

```sql
-- Add to player_game_summary processor
-- Formula: 100 * ((FGA + 0.44*FTA + TOV) * (Tm_MP/5)) / (MP * (Tm_FGA + 0.44*Tm_FTA + Tm_TOV))

WITH team_totals AS (
  SELECT
    game_id,
    team_abbr,
    SUM(minutes_played) as team_minutes,
    SUM(fg_attempts) as team_fga,
    SUM(ft_attempts) as team_fta,
    SUM(turnovers) as team_tov
  FROM player_game_summary_raw
  GROUP BY game_id, team_abbr
)
SELECT
  pgs.*,
  CASE
    WHEN pgs.minutes_played > 0 THEN
      100 * ((pgs.fg_attempts + 0.44*pgs.ft_attempts + pgs.turnovers) * (tt.team_minutes/5))
      / (pgs.minutes_played * (tt.team_fga + 0.44*tt.team_fta + tt.team_tov))
    ELSE NULL
  END as usage_rate
FROM player_game_summary_raw pgs
LEFT JOIN team_totals tt
  ON pgs.game_id = tt.game_id
  AND pgs.team_abbr = tt.team_abbr
```

**Validation**:
```sql
-- Check usage_rate distribution (should be 10-40% for most players)
SELECT
  APPROX_QUANTILES(usage_rate, 100) as percentiles
FROM `nba-props-platform.nba_analytics.player_game_summary`
WHERE usage_rate IS NOT NULL
  AND game_date >= '2021-10-01';

-- Expected: p50 ~20-25%, p95 ~35-40%
```

**Action Items**:
- [ ] Verify component fields exist (fg_attempts, ft_attempts, turnovers, minutes_played)
- [ ] Implement usage_rate calculation in processor
- [ ] Test on sample games
- [ ] Validate against known values (if available)
- [ ] Backfill for historical data
- [ ] Document calculation method

**Estimated Effort**: 4-6 hours
**Expected Impact**: +2-3% MAE improvement

---

#### 2.3 Fix Precompute Pipeline Coverage

**Objective**: Ensure precompute tables have >90% coverage for 2021-2024

**Strategy**:

**For player_composite_factors**:
```bash
# Backfill command (if processor exists)
./bin/precompute/backfill_player_composite_factors.sh \
  --start-date 2021-10-01 \
  --end-date 2024-05-01
```

**For team_defense_zone_analysis**:
```bash
# Check if processor was run for historical dates
gcloud logging read 'resource.labels.service_name="nba-phase4-precompute-processors"
  AND textPayload=~"team_defense_zone_analysis"
  AND timestamp>="2021-10-01"' --limit 1000

# If no logs: processor never ran for historical dates
# Action: Run backfill
```

**For player_daily_cache**:
```sql
-- Check dependency: Does it require upstream_player_game_context?
-- If yes, and upstream only has future dates, need to generate historical context

-- Alternative: Calculate directly from player_game_summary for training
```

**Action Items**:
- [ ] Identify why precompute tables have gaps
- [ ] Determine if backfill is possible
- [ ] Run backfill for each table
- [ ] Validate coverage after backfill
- [ ] Update processors to prevent future gaps

**Estimated Effort**: 8-12 hours
**Expected Impact**: +2-4% MAE improvement

---

### Phase 3: Quick Win Implementations (HIGH ROI - Weeks 3-4)

#### 3.1 Minute Threshold Filter

**Objective**: Don't predict for players with <15 minutes (low signal)

**Implementation**:

```python
# File: predictions/worker/prediction_systems/base_predictor.py

def should_make_prediction(self, player_data: dict) -> bool:
    """Filter low-signal predictions"""

    # Filter 1: Minimum minutes
    if player_data.get('minutes_avg_last_10', 0) < 15:
        self.logger.info(f"Skipping {player_data['player_name']}: minutes < 15")
        return False

    # Filter 2: Minimum games played
    if player_data.get('games_played', 0) < 5:
        self.logger.info(f"Skipping {player_data['player_name']}: insufficient games")
        return False

    return True
```

**Measurement**:
```sql
-- Before: Measure baseline MAE for low-minute players
SELECT AVG(ABS(predicted_points - actual_points)) as mae
FROM prediction_accuracy
WHERE minutes_avg_last_10 < 15
  AND game_date >= CURRENT_DATE - 30;

-- After: Measure MAE for remaining predictions
-- Expected: MAE improves 5-10% overall by removing noisy predictions
```

**Action Items**:
- [ ] Implement minute threshold filter
- [ ] Test on last 30 days data
- [ ] Measure impact on MAE
- [ ] Deploy to production
- [ ] Monitor for 1 week

**Estimated Effort**: 2 hours
**Expected Impact**: +5-10% MAE improvement

---

#### 3.2 Confidence Threshold Filter

**Objective**: Only show predictions with high confidence (>0.7)

**Implementation**:

```python
def calculate_confidence_score(self, player_data: dict, prediction: float) -> float:
    """Calculate prediction confidence based on data quality"""

    confidence = 1.0

    # Penalize missing data
    if player_data.get('minutes_avg_last_10') is None:
        confidence *= 0.5

    # Penalize high volatility
    points_std = player_data.get('points_std_last_10', 0)
    if points_std > 7:
        confidence *= 0.7

    # Penalize insufficient history
    games_played = player_data.get('games_played', 0)
    if games_played < 10:
        confidence *= 0.6

    # Bonus for data completeness
    if player_data.get('feature_quality_score', 0) > 90:
        confidence *= 1.1

    return min(confidence, 1.0)


def should_surface_prediction(self, prediction: dict) -> bool:
    """Only show high-confidence predictions"""
    return prediction['confidence'] >= 0.70
```

**Action Items**:
- [ ] Implement confidence scoring
- [ ] Test on historical data
- [ ] Measure MAE by confidence bucket
- [ ] Deploy confidence filter
- [ ] Monitor prediction coverage (ensure not filtering too many)

**Estimated Effort**: 2 hours
**Expected Impact**: +5-10% MAE improvement

---

#### 3.3 Injury Data Integration

**Objective**: Incorporate injury status into predictions (huge signal)

**Current State**:
- Injury discovery workflow exists (fixed Jan 3)
- Data collected but not integrated into predictions

**Implementation**:

```python
# Add injury adjustment to prediction
def apply_injury_adjustment(self, player_data: dict, base_prediction: float) -> float:
    """Adjust prediction based on injury status"""

    injury_status = player_data.get('injury_status')

    if injury_status == 'OUT':
        return 0.0  # Won't play

    elif injury_status == 'DOUBTFUL':
        # 75% chance won't play, downgrade heavily
        return base_prediction * 0.25

    elif injury_status == 'QUESTIONABLE':
        # May play limited minutes
        minutes_reduction = 0.7  # Assume 30% fewer minutes
        return base_prediction * minutes_reduction

    elif injury_status == 'PROBABLE':
        # Slight downgrade
        return base_prediction * 0.95

    # Also check for recently returned from injury
    days_since_injury_return = player_data.get('days_since_injury_return', 999)
    if days_since_injury_return < 3:
        # First games back, reduce prediction
        return base_prediction * 0.85

    return base_prediction
```

**Data Integration**:
```sql
-- Join injury data to prediction query
LEFT JOIN `nba-props-platform.nba_analytics.player_injury_status` inj
  ON pp.player_lookup = inj.player_lookup
  AND pp.game_date = inj.effective_date
```

**Action Items**:
- [ ] Review injury data schema
- [ ] Implement injury adjustment logic
- [ ] Test on games with known injuries
- [ ] Measure impact (compare pre/post injury return)
- [ ] Deploy to production

**Estimated Effort**: 10-15 hours
**Expected Impact**: +5-15% MAE improvement

---

### Phase 4: Model Retraining (Weeks 4-5)

#### 4.1 Retrain XGBoost with Clean Data

**Objective**: Measure true ML performance with quality data

**Action Items**:
- [ ] Wait for Phase 2 fixes to complete (data quality >95%)
- [ ] Update training script to use fixed tables
- [ ] Retrain XGBoost v3
- [ ] Evaluate on test set
- [ ] Compare to mock baseline

**Success Criteria**:
- Test MAE < 4.00 (beats mock's 4.33)
- Feature importance more balanced (not 75% in top 3)
- Validation MAE close to test MAE (good generalization)

**Expected Performance**: 3.80-4.10 MAE

---

#### 4.2 Train CatBoost Model

**Objective**: Try algorithm with better categorical feature handling

```python
import catboost as cb

model = cb.CatBoostRegressor(
    iterations=500,
    learning_rate=0.05,
    depth=6,
    loss_function='MAE',
    cat_features=['player_lookup', 'team_abbr', 'opponent_team_abbr'],
    verbose=20
)

model.fit(X_train, y_train, eval_set=(X_val, y_val))
```

**Action Items**:
- [ ] Install catboost library
- [ ] Identify categorical features
- [ ] Train model with proper encoding
- [ ] Evaluate and compare to XGBoost

**Expected Performance**: 3.75-4.00 MAE (5-10% better than XGBoost)

---

#### 4.3 Train LightGBM Model

**Objective**: Fast training, different tree growth patterns

```python
import lightgbm as lgb

model = lgb.LGBMRegressor(
    num_leaves=31,
    learning_rate=0.05,
    n_estimators=500,
    categorical_feature=['player_lookup', 'team_abbr', 'opponent_team_abbr']
)

model.fit(X_train, y_train, eval_set=[(X_val, y_val)])
```

**Action Items**:
- [ ] Install lightgbm library
- [ ] Train model
- [ ] Compare to XGBoost and CatBoost

**Expected Performance**: 3.78-4.05 MAE

---

### Phase 5: Feature Engineering (Weeks 5-7)

#### 5.1 Interaction Features

**Objective**: Explicitly encode mock model's interactions

```python
# Create interaction features
df['fatigue_b2b'] = df['fatigue_score'] * (0.5 if df['back_to_back'] else 1.0)
df['pace_usage'] = df['pace_score'] * (1.5 if df['usage_rate'] > 28 else 1.0)
df['paint_defense'] = df['paint_rate_last_10'] * (df['opponent_def_rating'] - 112)
df['home_b2b'] = df['is_home'].astype(int) * df['back_to_back'].astype(int)
df['rest_fatigue'] = df['days_rest'] * df['fatigue_score'] / 100
```

**Action Items**:
- [ ] Identify key interactions from mock model
- [ ] Create interaction features
- [ ] Add to training data
- [ ] Measure feature importance
- [ ] Retrain with interactions

**Expected Impact**: +5-8% improvement

---

#### 5.2 Player Embeddings/Clustering

**Objective**: Group similar players, use cluster patterns

```python
from sklearn.cluster import KMeans

# Cluster players by career stats
player_features = [
    'career_ppg', 'career_usage', 'career_minutes',
    'primary_position', 'years_experience'
]

kmeans = KMeans(n_clusters=10, random_state=42)
player_clusters = kmeans.fit_predict(player_stats[player_features])

# Add cluster as feature
df['player_cluster'] = df['player_lookup'].map(player_to_cluster)
```

**Action Items**:
- [ ] Define player similarity metrics
- [ ] Cluster players (K-means or hierarchical)
- [ ] Add cluster ID as feature
- [ ] Test if improves predictions

**Expected Impact**: +3-5% improvement

---

#### 5.3 Temporal Trend Features

**Objective**: Capture momentum, hot/cold streaks

```python
# Momentum features
df['points_trend'] = df['points_avg_last_5'] - df['points_avg_last_10']
df['minutes_trend'] = df['minutes_avg_last_3'] - df['minutes_avg_last_10']
df['hot_streak'] = (df['points_last_3_games'] > df['points_avg_season']).astype(int)
df['cold_streak'] = (df['points_last_3_games'] < df['points_avg_season'] * 0.8).astype(int)
```

**Action Items**:
- [ ] Create trend features
- [ ] Validate no data leakage
- [ ] Add to training
- [ ] Measure impact

**Expected Impact**: +2-4% improvement

---

### Phase 6: Hybrid Ensemble (Weeks 7-9)

#### 6.1 Stacked Ensemble Implementation

**Objective**: Combine mock + XGBoost + CatBoost + LightGBM

```python
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge

base_models = [
    ('mock', MockModelWrapper()),
    ('xgboost', XGBRegressor(...)),
    ('catboost', CatBoostRegressor(...)),
    ('lightgbm', LGBMRegressor(...))
]

meta_model = Ridge(alpha=1.0)

stacked = StackingRegressor(
    estimators=base_models,
    final_estimator=meta_model,
    cv=5  # Cross-validation for meta-features
)

stacked.fit(X_train, y_train)
```

**Action Items**:
- [ ] Wrap mock model in sklearn-compatible interface
- [ ] Collect predictions from all base models
- [ ] Train meta-learner
- [ ] Evaluate on holdout set
- [ ] Analyze when each model is trusted

**Expected Performance**: 3.40-3.60 MAE (20-25% better than mock)

---

#### 6.2 Conditional Routing

**Objective**: Route to best model per situation

```python
def route_prediction(player_data, situation):
    """Intelligently route to best model"""

    # Back-to-backs: Trust mock's hard-coded -2.2 penalty
    if situation['back_to_back']:
        return mock_model.predict(player_data)

    # Stars with history: Trust ML
    if player_data['tier'] == 'star' and player_data['games_played'] > 20:
        return xgboost_model.predict(player_data)

    # Rookies/new players: Use similarity
    if player_data['games_played'] < 10:
        return similarity_model.predict(player_data)

    # Default: Ensemble
    return ensemble.predict(player_data)
```

**Action Items**:
- [ ] Define routing rules
- [ ] Implement router
- [ ] Test on validation set
- [ ] Measure improvement over single best model

**Expected Impact**: +3-5% improvement over best single model

---

### Phase 7: Production Infrastructure (Weeks 10-18)

#### 7.1 Model Registry

**Implementation**:
```sql
CREATE TABLE ml_models.model_registry (
  model_id STRING,
  version INT,
  created_at TIMESTAMP,
  model_type STRING,
  test_mae FLOAT,
  status STRING,  -- 'staging', 'production', 'archived'
  gcs_path STRING,
  metadata JSON
);
```

**Action Items**:
- [ ] Create registry table
- [ ] Build model upload script
- [ ] Build model download script
- [ ] Implement versioning logic

---

#### 7.2 Data Validation (Great Expectations)

**Implementation**:
```python
import great_expectations as gx

suite = gx.ExpectationSuite(name="training_data_quality")

suite.expect_column_values_to_not_be_null('minutes_avg_last_10', mostly=0.95)
suite.expect_column_values_to_be_between('points_avg_last_10', 0, 50)
suite.expect_column_mean_to_be_between('fatigue_score', 30, 90)

# Run before every training
results = suite.validate(training_data)
if not results.success:
    raise DataQualityError("Training data failed validation")
```

**Action Items**:
- [ ] Install Great Expectations
- [ ] Define expectation suites for all tables
- [ ] Integrate into training pipeline
- [ ] Set up alerts for failures

---

#### 7.3 Drift Monitoring

**Implementation**:
```sql
-- Daily drift monitoring
CREATE VIEW ml_monitoring.daily_drift AS
SELECT
  DATE(prediction_timestamp) as pred_date,
  AVG(absolute_error) as daily_mae,
  LAG(AVG(absolute_error), 7) OVER (ORDER BY DATE(prediction_timestamp)) as mae_7d_ago,
  (AVG(absolute_error) - LAG(AVG(absolute_error), 7) OVER (ORDER BY DATE(prediction_timestamp)))
    / LAG(AVG(absolute_error), 7) OVER (ORDER BY DATE(prediction_timestamp)) * 100 as pct_change
FROM nba_predictions.prediction_accuracy
WHERE prediction_timestamp >= CURRENT_DATE - 30
GROUP BY pred_date;

-- Alert if drift > 10%
```

**Action Items**:
- [ ] Create drift monitoring queries
- [ ] Set up Cloud Function to run daily
- [ ] Configure Slack/email alerts
- [ ] Build drift dashboard

---

## ðŸ“š DOCUMENTATION UPDATES NEEDED

### Critical Documentation (Week 1)

1. **Root Cause Investigation Report**
   - File: `docs/09-handoff/2026-01-02-DATA-QUALITY-ROOT-CAUSE.md`
   - Content: Timeline, findings, fix plan
   - Owner: Whoever investigates

2. **Data Quality Investigation Runbook**
   - File: `docs/runbooks/data-quality-investigation.md`
   - Content: Step-by-step queries, expected outputs
   - Purpose: Repeatable process for future issues

3. **Update ML Training Guide**
   - File: `docs/08-projects/current/ml-model-development/04-REAL-MODEL-TRAINING.md`
   - Add: Data quality requirements, validation steps
   - Note: "Do NOT train until data quality >95%"

### Important Documentation (Weeks 2-4)

4. **Data Pipeline Fix Documentation**
   - File: `docs/08-projects/current/data-quality/pipeline-fixes-jan-2026.md`
   - Content: What was fixed, how, validation results

5. **Mock Model Analysis**
   - File: `docs/08-projects/current/ml-model-development/05-MOCK-MODEL-ANALYSIS.md`
   - Content: Rules, thresholds, domain knowledge captured
   - Purpose: Preserve expertise for future ML work

6. **Feature Engineering Guide**
   - File: `docs/08-projects/current/ml-model-development/06-FEATURE-ENGINEERING.md`
   - Content: Interaction features, player embeddings, temporal features
   - Include: Code snippets, validation methods

7. **Hybrid Ensemble Architecture**
   - File: `docs/08-projects/current/ml-model-development/07-HYBRID-ENSEMBLE.md`
   - Content: Design, implementation, routing logic
   - Include: Diagram of system flow

### Ongoing Documentation (Weeks 5+)

8. **Model Comparison Report**
   - File: `docs/09-handoff/model-comparison-jan-2026.md`
   - Content: Mock vs XGBoost vs CatBoost vs LGBM vs Ensemble
   - Update after each model training

9. **Production Deployment Guide**
   - File: `docs/deployment/ml-model-deployment.md`
   - Content: A/B testing, rollback, monitoring
   - Include: Runbook for common issues

10. **Quick Wins Results**
    - File: `docs/08-projects/current/ml-model-development/08-QUICK-WINS.md`
    - Content: Filters implemented, measured impact
    - Purpose: Show ROI of simple improvements

---

## ðŸŽ¯ SUCCESS CRITERIA BY PHASE

### Phase 1: Investigation Complete
- âœ… Root cause identified for 95% NULL issue
- âœ… ETL pipeline traced end-to-end
- âœ… Timeline established (regression vs historical gap)
- âœ… Fix plan documented
- âœ… Stakeholders informed

### Phase 2: Data Fixed
- âœ… minutes_played NULL rate < 5%
- âœ… usage_rate calculated for 90%+ of records
- âœ… Precompute tables >90% coverage
- âœ… Validation queries pass
- âœ… Backfill complete for 2021-2024

### Phase 3: Quick Wins Deployed
- âœ… Filters implemented and tested
- âœ… MAE improved by 13-25% (via filters alone)
- âœ… Injury data integrated
- âœ… Production monitoring shows stable improvement
- âœ… Documentation updated

### Phase 4: Models Retrained
- âœ… XGBoost v3 MAE < 4.00 (beats mock)
- âœ… CatBoost trained and evaluated
- âœ… LightGBM trained and evaluated
- âœ… Best single model identified
- âœ… Results documented

### Phase 5: Feature Engineering Complete
- âœ… Interaction features added
- âœ… Player embeddings created
- âœ… Temporal features engineered
- âœ… Model performance improved 5-10%
- âœ… No data leakage detected

### Phase 6: Hybrid Ensemble Deployed
- âœ… Stacked ensemble trained
- âœ… Meta-learner weights interpretable
- âœ… Ensemble MAE < 3.60 (20%+ better than mock)
- âœ… A/B test shows improvement in production
- âœ… Conditional routing optimized

### Phase 7: Production Infrastructure Complete
- âœ… Model registry operational
- âœ… Data validation running daily
- âœ… Drift monitoring with alerts
- âœ… A/B testing framework working
- âœ… Automated retraining pipeline
- âœ… Full documentation

---

## â±ï¸ ESTIMATED TIMELINE

| Phase | Duration | Effort | Dependencies |
|-------|----------|--------|--------------|
| Phase 1: Investigation | Week 1 | 20-30 hours | None |
| Phase 2: Data Fixes | Weeks 2-4 | 20-30 hours | Phase 1 complete |
| Phase 3: Quick Wins | Weeks 3-4 | 15-20 hours | Phase 2 in progress |
| Phase 4: Retraining | Weeks 4-5 | 12-16 hours | Phase 2 complete |
| Phase 5: Feature Engineering | Weeks 5-7 | 40-60 hours | Phase 4 complete |
| Phase 6: Hybrid Ensemble | Weeks 7-9 | 40-60 hours | Phase 5 complete |
| Phase 7: Production Infra | Weeks 10-18 | 80-120 hours | System 90%+ mature |

**Total Timeline**: 18 weeks (4-5 months)
**Total Effort**: 227-336 hours (2-3 months full-time equivalent)

---

## ðŸš¨ BLOCKING DEPENDENCIES

### Must Complete Before ANY ML Work
- âœ… Phase 1 investigation (understand the problem)
- âœ… Phase 2 data fixes (fix the foundation)

### Must Complete Before Production Deployment
- âœ… Quick wins validated (prove data quality matters)
- âœ… Models retrained with clean data (verify improvement)

### Must Complete Before Advanced ML
- âœ… System maturity hits 90%+ (stable infrastructure)
- âœ… Basic monitoring operational (can detect issues)

---

## ðŸ“Š EXPECTED CUMULATIVE IMPACT

| Milestone | Expected MAE | vs Mock (4.33) | Cumulative Gain |
|-----------|--------------|----------------|-----------------|
| **Current (broken data)** | 4.63 | -6.9% | - |
| After Phase 2 (data fixed) | 3.80-4.10 | +6-12% | +13-18% vs current |
| After Phase 3 (quick wins) | 3.20-3.60 | +17-26% | +22-31% vs current |
| After Phase 4 (retraining) | 3.40-3.70 | +15-21% | +20-28% vs current |
| After Phase 5 (feature eng) | 3.30-3.60 | +17-24% | +22-29% vs current |
| **After Phase 6 (ensemble)** | **3.40-3.60** | **+17-22%** | **+22-27% vs current** |

**Note**: Phases 4-6 happen in parallel/overlap, so final result is Phase 6, not cumulative of all.

---

## ðŸŽ¯ PRIORITIZATION FRAMEWORK

### P0 - CRITICAL (Do First)
- Phase 1: Investigation
- Phase 2: Data fixes
- Documentation: Root cause, runbooks

### P1 - HIGH (Do Next)
- Phase 3: Quick wins
- Phase 4: Retraining with clean data
- Documentation: Pipeline fixes, model comparison

### P2 - MEDIUM (Do When Ready)
- Phase 5: Feature engineering
- Phase 6: Hybrid ensemble
- Documentation: Architecture, deployment guides

### P3 - NICE TO HAVE (Do Last)
- Phase 7: Full production infrastructure
- Advanced techniques (LSTM, Transformers)
- Documentation: Advanced topics

---

## ðŸ“ TRACKING & REPORTING

### Weekly Check-ins
- Review todo list progress
- Update estimated completion dates
- Flag blockers early
- Document learnings

### Key Metrics to Track
- Data quality: NULL rates by table
- Model performance: MAE by model type
- Business impact: Prediction accuracy, coverage
- System health: Uptime, error rates

### Decision Points
- **After Phase 1**: Go/no-go on data fixes
- **After Phase 2**: Measure impact, decide on quick wins
- **After Phase 4**: Evaluate if ensemble is needed
- **After Phase 6**: Decide on production infrastructure investment

---

**END OF MASTER PLAN**

This document serves as the single source of truth for all ML improvement work. Update regularly as investigations proceed and findings emerge.
