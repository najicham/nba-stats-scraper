# Session 117 Handoff - Data Quality Validation Gap Investigation

**Date:** February 5, 2026
**Session Type:** Investigation & Architecture Review
**Status:** ‚ö†Ô∏è CRITICAL ISSUE IDENTIFIED - Requires Implementation
**Next Session Priority:** HIGH - Implement defense-in-depth validation

---

## Executive Summary

Investigation into Feb 3 data quality issue uncovered a **systemic design flaw** in the data pipeline: the "presence equals validity" anti-pattern. The system checks if data exists but never validates if it's correct. This affects multiple processors and phases.

**Impact:**
- 20 players (PHX/POR game) had NULL usage_rate on Feb 3
- 10% data corruption rate went undetected
- Bad data propagated through Phase 3 ‚Üí Phase 4 ‚Üí Phase 5
- Similar vulnerability likely exists in other processors

**Root Cause:** Three validation gaps converged:
1. `FallbackSourceMixin` checks for non-empty DataFrames, not data quality
2. Completeness validation checks row count, not value validity
3. No pre-write validation rules block bad data

**Recommended Fix:** Defense-in-depth validation at 3 layers (source, consumer, write)

**Opus Agent Review:** Confirmed systemic issue, recommended ensemble validation + inter-phase validation gateway

---

## Table of Contents

1. [Investigation Timeline](#investigation-timeline)
2. [Root Cause Analysis](#root-cause-analysis)
3. [Technical Deep Dive](#technical-deep-dive)
4. [Strategic Assessment (Opus Review)](#strategic-assessment-opus-review)
5. [Recommended Solution Architecture](#recommended-solution-architecture)
6. [Implementation Roadmap](#implementation-roadmap)
7. [Files to Review](#files-to-review)
8. [Next Session Plan](#next-session-plan)

---

## Investigation Timeline

### Session Start (1:42 PM PST)
- **Trigger:** Daily validation for Feb 5
- **Expected:** Feb 4 games scheduled, Feb 5 predictions don't exist yet (normal)
- **Found:** Feb 3 usage_rate coverage at 49% (CRITICAL alert)

### Initial Investigation
```sql
-- Found PHX/POR game with 0% usage_rate coverage
SELECT game_id, active_players, has_usage_rate, usage_rate_pct
FROM (... player_game_summary grouped by game ...)
WHERE game_date = '2026-02-03'

-- Result: 20260203_PHX_POR = 20 players, 0 with usage_rate (0.0%)
```

### Manual Reprocessing Attempt
```bash
# Triggered TeamOffenseGameSummaryProcessor manually
curl -X POST "${ANALYTICS_URL}/process-date-range" \
  -d '{"start_date": "2026-02-03", "processors": ["TeamOffenseGameSummaryProcessor"]}'

# Result: "success" but still wrote 0 values!
```

### Discovery of Root Cause

**Initial hypothesis:** Table `nba_raw.nbac_team_boxscore` doesn't exist
**Actual finding:** Table EXISTS but contains bad data (points=0, fg_attempts=0)

```sql
-- Verification query
SELECT team_abbr, points, fg_attempted, turnovers
FROM nba_raw.nbac_team_boxscore
WHERE game_date = '2026-02-03' AND team_abbr IN ('PHX', 'POR')

-- Result:
-- PHX: points=0, fg_attempted=0, turnovers=0, minutes=NULL
-- POR: points=0, fg_attempted=0, turnovers=0, minutes=NULL
```

**But reconstruction works perfectly:**
```sql
-- Manual reconstruction test
SELECT team_abbr, SUM(points), SUM(field_goals_attempted)
FROM nba_raw.bdl_player_boxscores
WHERE game_date = '2026-02-03' AND team_abbr IN ('PHX', 'POR')
GROUP BY team_abbr

-- Result:
-- PHX: 130 points, 97 FGA ‚úÖ
-- POR: 125 points, 95 FGA ‚úÖ
```

### Agent Investigation (Opus)
- Spawned general-purpose agent to trace fallback chain bug
- Agent confirmed: Fallback chain sees "non-empty DataFrame = success"
- Never validates data quality, so 0-value records pass through
- Spawned Opus agent for strategic architectural review
- Opus confirmed systemic issue, recommended defense-in-depth

---

## Root Cause Analysis

### The Bug Chain

```
1. Phase 2 writes BAD DATA
   ‚îî‚îÄ> nba_raw.nbac_team_boxscore: PHX/POR with points=0
   ‚îî‚îÄ> Likely cause: Game incomplete when scraped, scraper wrote placeholder

2. Phase 3 fallback chain sees "SUCCESS"
   ‚îî‚îÄ> FallbackSourceMixin line 186: if df is not None and not df.empty:
   ‚îî‚îÄ> 2 rows exist ‚Üí returns success with 0-value data
   ‚îî‚îÄ> NEVER tries _reconstruct_team_from_players()

3. Completeness validation PASSES
   ‚îî‚îÄ> Line 380: if team_count < MIN_TEAMS_THRESHOLD:
   ‚îî‚îÄ> 20 teams >= 10 threshold ‚Üí no issue detected
   ‚îî‚îÄ> Only checks row COUNT, not data VALIDITY

4. No pre-write validation
   ‚îî‚îÄ> Log: "No validation rules defined for table: team_offense_game_summary"
   ‚îî‚îÄ> Bad data written to team_offense_game_summary without checks

5. Bad data propagates
   ‚îî‚îÄ> possessions = NULL (can't calculate from 0 FGA)
   ‚îî‚îÄ> usage_rate = NULL for 20 players
   ‚îî‚îÄ> ML features degraded for those players
```

### Data Quality Stats

**Feb 3 Overall:**
- Total teams: 20
- Teams with 0 points: 2 (PHX, POR)
- Corruption rate: **10%**
- Other 18 teams: Correct data ‚úÖ

**Impact Scope:**
- Players affected: 20 (PHX/POR game only)
- Overall usage_rate coverage: 88% (206/234) - above 80% threshold
- **Not critical** but quality degradation significant

### Why Fallback Didn't Work

**Current logic in FallbackSourceMixin:**
```python
# shared/processors/patterns/fallback_source_mixin.py:186
if df is not None and not df.empty:
    # Success! (even if all values are 0)
    return FallbackResult(success=True, data=df, source_used='nbac_team_boxscore')
```

**Problem:** Checks for **existence** of rows, not **quality** of data

**Expected behavior:**
```python
if df is not None and not df.empty:
    # NEW: Validate data quality
    if self._validate_data_quality(df):
        return FallbackResult(success=True, ...)
    else:
        # Bad data, try next source
        continue
```

### Why Completeness Check Failed

**Current logic in TeamOffenseGameSummaryProcessor:**
```python
# team_offense_game_summary_processor.py:380-387
if fallback_result.source_used == 'nbac_team_boxscore':
    team_count = len(self.raw_data)

    if team_count < MIN_TEAMS_THRESHOLD:  # 20 >= 10
        # Force reconstruction
```

**Problem:** Only validates **quantity** (row count), not **quality** (data values)

**Expected behavior:**
```python
if fallback_result.source_used == 'nbac_team_boxscore':
    valid_teams = self.raw_data[(self.raw_data['points'] > 0) &
                                 (self.raw_data['fg_attempted'] > 0)]
    valid_count = len(valid_teams)

    if valid_count < MIN_TEAMS_THRESHOLD:  # Check VALID data
        # Force reconstruction
```

---

## Technical Deep Dive

### Affected Code Paths

#### 1. FallbackSourceMixin (Shared Component)

**File:** `shared/processors/patterns/fallback_source_mixin.py`

**Current implementation:**
```python
def try_fallback_chain(
    self,
    chain_name: str,
    extractors: Dict[str, Callable[[], pd.DataFrame]],
    context: Dict[str, Any] = None,
) -> FallbackResult:
    for source_name in chain.sources:
        extractor = extractors[source_name]

        try:
            df = extractor()

            # ISSUE: Only checks if DataFrame is non-empty
            if df is not None and not df.empty:
                return FallbackResult(success=True, data=df, source_used=source_name)
        except Exception as e:
            continue

    return FallbackResult(success=False, ...)
```

**Gap:** No quality validation hook

**Impact:** Every processor using this mixin has the same vulnerability:
- `TeamOffenseGameSummaryProcessor` ‚úÖ (confirmed affected)
- `TeamDefenseGameSummaryProcessor` (likely affected)
- `PlayerGameSummaryProcessor` (likely affected)
- Others using FallbackSourceMixin

#### 2. TeamOffenseGameSummaryProcessor Extractor

**File:** `data_processors/analytics/team_offense_game_summary/team_offense_game_summary_processor.py`

**Lines 429-553:** `_extract_from_nbac_team_boxscore()`

```python
def _extract_from_nbac_team_boxscore(self, start_date: str, end_date: str) -> pd.DataFrame:
    query = f"""
    SELECT game_id, team_abbr, points, fg_attempted, ...
    FROM `{self.project_id}.nba_raw.nbac_team_boxscore`
    WHERE game_date BETWEEN '{start_date}' AND '{end_date}'
    """

    try:
        df = self.bq_client.query(query).to_dataframe()
        logger.info(f"Extracted {len(df)} team-game records from nbac_team_boxscore")
        return df  # ISSUE: No quality validation
    except Exception as e:
        logger.error(f"Failed to extract from nbac_team_boxscore: {e}")
        return pd.DataFrame()
```

**Gap:** Returns any non-empty DataFrame, even if all values are 0

**Fix location:** Add quality filter before returning

#### 3. Reconstruction Fallback (Works Correctly!)

**File:** Same file, lines 555-700

**Lines 555-634:** `_reconstruct_team_from_players()`

```python
def _reconstruct_team_from_players(self, start_date: str, end_date: str) -> pd.DataFrame:
    query = f"""
    WITH player_stats AS (
        SELECT game_id, team_abbr,
            SUM(points) as points,
            SUM(field_goals_attempted) as fg_attempted,
            ...
        FROM nba_raw.bdl_player_boxscores
        WHERE game_date BETWEEN '{start_date}' AND '{end_date}'
        GROUP BY game_id, team_abbr
        HAVING COUNT(*) >= 5  # At least 5 players
    )
    SELECT * FROM player_stats
    """

    df = self.bq_client.query(query).to_dataframe()
    return df
```

**Status:** Works perfectly ‚úÖ
- Tested: PHX=130 pts, POR=125 pts (correct!)
- Uses player boxscore aggregation
- Has quality filter: `HAVING COUNT(*) >= 5`

**Issue:** Never gets called when primary source returns non-empty (even bad) data

#### 4. Pre-Write Validation (Missing)

**Evidence from logs:**
```
WARNING:shared.validation.pre_write_validator:No validation rules defined for table: team_offense_game_summary
```

**Expected location:** `shared/validation/rules/team_offense_game_summary.yaml`

**Status:** File doesn't exist

**Impact:** Bad data written to BigQuery without checks

### Emergency Override (Already Exists!)

**File:** `team_offense_game_summary_processor.py:323-347`

```python
# FIX #3: EMERGENCY OVERRIDE - Force reconstruction
if os.environ.get('FORCE_TEAM_RECONSTRUCTION', 'false').lower() == 'true':
    logger.info(
        "üîß FORCE_TEAM_RECONSTRUCTION enabled - bypassing nbac_team_boxscore entirely. "
        "Using reconstruction from player boxscores only."
    )
    self.raw_data = self._reconstruct_team_from_players(start_date, end_date)

    if self.raw_data is not None and not self.raw_data.empty:
        self._source_used = 'reconstructed_team_from_players (forced by env var)'
        return  # Skip fallback chain entirely
```

**Status:** Available but not enabled

**Use case:** When `nbac_team_boxscore` is known to be unreliable

**Short-term fix:** Enable this for reprocessing Feb 3

---

## Strategic Assessment (Opus Review)

### Is This Systemic or One-Time?

**Verdict: SYSTEMIC**

Three architectural gaps converged:

| Gap | Location | Pattern |
|-----|----------|---------|
| No quality validation in fallback | `FallbackSourceMixin._try_source()` | Checks `df is not None` only |
| Row count vs. data quality | Completeness check | `count >= threshold` ignores values |
| No pre-write validation | Write path | "No validation rules defined" |

**Evidence this is systemic:**
1. `FallbackSourceMixin` is a **shared mixin** - every processor using it has this gap
2. Completeness checks across processors likely all use row counts only
3. Pre-write validation is opt-in and clearly underutilized

### Architectural Implications

**The current architecture assumes upstream data is trustworthy.**

This is the **"trust the source" anti-pattern**. In reality:
- Scrapers can fail partially (2/20 teams ‚Üí 0 values)
- External APIs can return valid JSON with invalid data
- Network timeouts can cause partial writes

**This violates the principle of defense-in-depth.**

### Other Vulnerable Locations

Based on the pattern, similar gaps likely exist in:
- ‚úÖ `PlayerGameSummaryProcessor` (also uses FallbackSourceMixin)
- ‚úÖ Any processor with `MIN_*_THRESHOLD` completeness checks
- ‚úÖ Phase 4 precompute processors that aggregate Phase 3 data
- ‚úÖ Phase 5 prediction pipeline (trusts Phase 4 features)

### Anti-Pattern: "Presence Equals Validity"

**Current assumption:**
```python
if data.exists():
    # Data must be good!
    return success
```

**Reality:**
```python
if data.exists():
    if data.is_valid():
        return success
    else:
        # Data exists but is corrupted
        try_next_source()
```

**Core principle violated:** Always validate at trust boundaries

A **trust boundary** is any point where data crosses systems:
- Scraper ‚Üí Raw tables (Phase 1‚Üí2)
- Raw tables ‚Üí Analytics (Phase 2‚Üí3)
- Analytics ‚Üí Precompute (Phase 3‚Üí4)

---

## Recommended Solution Architecture

### Core Principle: Validate at Every Trust Boundary

**Opus Recommendation:** Defense-in-depth with 3 validation layers

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 1: SOURCE VALIDATION (Phase 2)                       ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Scraper detects partial failures                        ‚îÇ ‚îÇ
‚îÇ ‚îÇ Rejects writes with points=0 or fg_attempts=0           ‚îÇ ‚îÇ
‚îÇ ‚îÇ Logs error, retries later                               ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚Üì Stops bad data at origin                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 2: CONSUMER VALIDATION (Phase 3 Extractor)           ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Quality-aware fallback in extractor                     ‚îÇ ‚îÇ
‚îÇ ‚îÇ Filters out rows where points=0 or fg_attempts=0        ‚îÇ ‚îÇ
‚îÇ ‚îÇ If >50% invalid, returns empty ‚Üí triggers fallback      ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚Üì Defense when source fails                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Layer 3: WRITE VALIDATION (Phase 3 Pre-Write)              ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ YAML validation rules                                    ‚îÇ ‚îÇ
‚îÇ ‚îÇ ERROR if points_scored=0                                ‚îÇ ‚îÇ
‚îÇ ‚îÇ ERROR if fg_attempts=0                                  ‚îÇ ‚îÇ
‚îÇ ‚îÇ ERROR if possessions IS NULL                            ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚Üì Last line of defense                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚Üì
                   BigQuery Tables
```

### Why All Three Layers?

| If Only... | Gap |
|-----------|-----|
| Layer 1 (Source) | Can't control all sources (NBA API failures) |
| Layer 2 (Consumer) | Other processors may have same gap |
| Layer 3 (Write) | Reactive (data already processed, wasted compute) |

**All three together:** Robust, self-healing system

### Key Innovation: Ensemble Validation

**Opus recommendation:** Don't switch primary source, **cross-validate** them

```python
# Extract from BOTH sources
team_data = extract_from_nbac_team_boxscore()
reconstructed = reconstruct_from_players()

# Cross-validate
if team_data and reconstructed:
    discrepancy = compare(team_data, reconstructed)

    if discrepancy > 5%:  # Points differ by >5%
        logger.warning(
            f"Source discrepancy detected: official={team_data['points']}, "
            f"reconstructed={reconstructed['points']}"
        )
        return reconstructed  # Trust aggregated player data

    return team_data  # Sources agree, use official

elif reconstructed:
    return reconstructed  # Only reconstruction available

elif team_data:
    logger.warning("Only official data available, reconstruction failed")
    return team_data  # Risky, but better than nothing

else:
    raise DataUnavailable("No valid data from any source")
```

**Benefits:**
1. Official team data is valuable when correct (has pace, possessions calculated)
2. Reconstruction depends on player data existing (no circular dependency)
3. Cross-validation catches anomalies **neither source detects alone**
4. Auto-healing: system switches to reconstruction when official data is bad

**Better than:**
- Switching to reconstruction as primary (loses official data benefits)
- Keeping official as primary only (vulnerable to bad data)

---

## Implementation Roadmap

### Day 1: Quick Wins (4 hours)

**Priority: Immediate protection against recurrence**

#### Task 1.1: Add Quality Validation to Extractor (2 hours)

**File:** `data_processors/analytics/team_offense_game_summary/team_offense_game_summary_processor.py`

**Location:** Line 547-553 in `_extract_from_nbac_team_boxscore()`

**Change:**
```python
def _extract_from_nbac_team_boxscore(self, start_date: str, end_date: str) -> pd.DataFrame:
    """Extract team offensive data from nbac_team_boxscore (PRIMARY source)."""
    query = f"""
    ... existing query ...
    """

    try:
        df = self.bq_client.query(query).to_dataframe()

        # NEW: Quality validation before returning
        if df is None or df.empty:
            logger.info("No data returned from nbac_team_boxscore")
            return pd.DataFrame()

        # Filter out invalid rows (0 values likely indicate placeholder/incomplete data)
        valid_mask = (df['points'] > 0) & (df['fg_attempted'] > 0)
        invalid_rows = df[~valid_mask]

        if len(invalid_rows) > 0:
            invalid_teams = invalid_rows['team_abbr'].tolist()
            logger.warning(
                f"‚ö†Ô∏è  QUALITY CHECK: Found {len(invalid_rows)} teams with invalid data "
                f"(0 points or 0 FGA): {invalid_teams}. These will be filtered out and "
                f"reconstructed from player stats."
            )
            df = df[valid_mask]

        # If >50% of expected data is invalid, treat entire source as failed
        # Expected: ~2 teams per game for date range
        expected_teams = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days * 2

        if len(df) < expected_teams * 0.5:
            logger.error(
                f"‚ùå QUALITY CHECK FAILED: Only {len(df)} valid teams found, "
                f"expected ~{expected_teams}. Returning empty to trigger fallback."
            )
            return pd.DataFrame()

        logger.info(f"‚úÖ Extracted {len(df)} valid team-game records from nbac_team_boxscore")
        return df

    except Exception as e:
        logger.error(f"Failed to extract from nbac_team_boxscore: {e}")
        return pd.DataFrame()
```

**Testing:**
```bash
# After deployment, reprocess Feb 3
curl -X POST "${ANALYTICS_URL}/process-date-range" \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type: application/json" \
  -d '{
    "start_date": "2026-02-03",
    "end_date": "2026-02-03",
    "processors": ["TeamOffenseGameSummaryProcessor"],
    "backfill_mode": true,
    "trigger_reason": "session_117_quality_validation_fix"
  }'

# Verify PHX/POR now have correct data
bq query "SELECT team_abbr, points_scored, fg_attempts, possessions
FROM nba_analytics.team_offense_game_summary
WHERE game_date = '2026-02-03' AND team_abbr IN ('PHX', 'POR')"

# Expected: PHX ~130 pts, POR ~125 pts, possessions ~100
```

#### Task 1.2: Add Pre-Write Validation Rules (1 hour)

**New file:** `shared/validation/rules/team_offense_game_summary.yaml`

```yaml
# Validation rules for team_offense_game_summary table
# Prevents bad data from being written to BigQuery

table: team_offense_game_summary
enabled: true

rules:
  # Critical: Teams must score points
  - name: points_not_zero
    level: ERROR
    field: points_scored
    condition: "points_scored = 0"
    message: "Team scored 0 points - likely bad source data or placeholder record"

  # Critical: Teams must attempt field goals
  - name: fg_attempts_not_zero
    level: ERROR
    field: fg_attempts
    condition: "fg_attempts = 0"
    message: "Team has 0 FG attempts - likely bad source data or placeholder record"

  # Critical: Possessions required for usage_rate calculation
  - name: possessions_required
    level: ERROR
    field: possessions
    condition: "possessions IS NULL"
    message: "Possessions is NULL - cannot calculate pace or usage_rate downstream"

  # Warning: Unusually low scoring (possible but rare)
  - name: unusually_low_score
    level: WARNING
    field: points_scored
    condition: "points_scored > 0 AND points_scored < 80"
    message: "Team scored less than 80 points - unusual but possible (verify game wasn't shortened)"

  # Warning: Unusually high scoring (possible but rare)
  - name: unusually_high_score
    level: WARNING
    field: points_scored
    condition: "points_scored > 180"
    message: "Team scored more than 180 points - unusual but possible (verify data accuracy)"

  # Info: New team or exhibition game
  - name: exhibition_game_flag
    level: INFO
    field: season_year
    condition: "season_year < 2020"
    message: "Historical game data - ensure stats are comparable to modern era"
```

**Testing:**
```python
# Manual validation test
from shared.validation.pre_write_validator import PreWriteValidator

validator = PreWriteValidator('team_offense_game_summary')

# Test record with 0 points (should fail)
bad_record = {
    'team_abbr': 'PHX',
    'points_scored': 0,
    'fg_attempts': 0,
    'possessions': None
}

result = validator.validate(bad_record)
assert result.has_errors() == True
assert 'points_not_zero' in result.error_names

# Test valid record (should pass)
good_record = {
    'team_abbr': 'PHX',
    'points_scored': 130,
    'fg_attempts': 97,
    'possessions': 102
}

result = validator.validate(good_record)
assert result.has_errors() == False
```

#### Task 1.3: Regenerate Feb 3 Data (30 minutes)

**Option A: Using FORCE_TEAM_RECONSTRUCTION (safest)**

```bash
# Enable emergency override
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --update-env-vars FORCE_TEAM_RECONSTRUCTION=true

# Reprocess Feb 3
ANALYTICS_URL="https://nba-phase3-analytics-processors-756957797294.us-west2.run.app"
TOKEN=$(gcloud auth print-identity-token)

curl -X POST "${ANALYTICS_URL}/process-date-range" \
  -H "Authorization: Bearer ${TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{
    "start_date": "2026-02-03",
    "end_date": "2026-02-03",
    "processors": ["TeamOffenseGameSummaryProcessor"],
    "backfill_mode": true,
    "trigger_reason": "session_117_regenerate_with_reconstruction"
  }'

# Disable override after reprocessing
gcloud run services update nba-phase3-analytics-processors \
  --region=us-west2 \
  --update-env-vars FORCE_TEAM_RECONSTRUCTION=false
```

**Option B: After implementing Task 1.1 (quality validation)**

```bash
# Deploy with fix
./bin/deploy-service.sh nba-phase3-analytics-processors

# Reprocess (will auto-filter bad data and use reconstruction)
curl -X POST "${ANALYTICS_URL}/process-date-range" ...
```

**Verification:**
```bash
# Check PHX/POR team stats
bq query "
SELECT team_abbr, points_scored, fg_attempts, possessions, primary_source_used
FROM nba_analytics.team_offense_game_summary
WHERE game_date = '2026-02-03' AND team_abbr IN ('PHX', 'POR')
"
# Expected: points_scored ~125-130, fg_attempts ~95-97, possessions ~100-105

# Check player usage_rate coverage
bq query "
SELECT game_id,
  COUNTIF(is_dnp = FALSE) as active_players,
  COUNTIF(is_dnp = FALSE AND usage_rate IS NOT NULL) as has_usage_rate
FROM nba_analytics.player_game_summary
WHERE game_date = '2026-02-03'
GROUP BY game_id
HAVING game_id = '20260203_PHX_POR'
"
# Expected: 20 active players, 20 with usage_rate (100%)
```

---

### Week 1: Systemic Fixes (3-4 days)

**Priority: Fix the shared mixin and audit other processors**

#### Task 2.1: Enhance FallbackSourceMixin with Quality Hooks (1 day)

**File:** `shared/processors/patterns/fallback_source_mixin.py`

**Enhancement:** Add optional quality validator

```python
from typing import Callable, Optional
import pandas as pd

class QualityValidator:
    """Base class for data quality validators."""

    def validate(self, df: pd.DataFrame) -> tuple[bool, str]:
        """
        Validate DataFrame quality.

        Returns:
            (is_valid, reason): True if data passes quality checks
        """
        raise NotImplementedError


class FallbackSourceMixin:
    """Enhanced mixin with quality validation hooks."""

    def try_fallback_chain(
        self,
        chain_name: str,
        extractors: Dict[str, Callable[[], pd.DataFrame]],
        context: Dict[str, Any] = None,
        quality_validators: Optional[Dict[str, QualityValidator]] = None,
    ) -> FallbackResult:
        """
        Try sources in fallback chain order until one succeeds.

        Args:
            chain_name: Name of fallback chain
            extractors: Dict mapping source names to extractor functions
            context: Additional context for logging
            quality_validators: Optional dict mapping source names to validators

        Returns:
            FallbackResult with data and quality information
        """
        quality_validators = quality_validators or {}

        for source_name in chain.sources:
            if source_name not in extractors:
                continue

            extractor = extractors[source_name]

            try:
                logger.debug(f"Trying source: {source_name}")
                df = extractor()

                # Check if we got data
                if df is None or df.empty:
                    logger.debug(f"Source {source_name} returned no data")
                    continue

                # NEW: Quality validation hook
                if source_name in quality_validators:
                    validator = quality_validators[source_name]
                    is_valid, reason = validator.validate(df)

                    if not is_valid:
                        logger.warning(
                            f"Source {source_name} failed quality validation: {reason}. "
                            f"Trying next source..."
                        )
                        # Log quality failure event
                        self._log_quality_event(
                            source_name=source_name,
                            event='quality_validation_failed',
                            reason=reason,
                            context=context
                        )
                        continue

                # Success!
                is_fallback = not source_config.is_primary
                logger.info(
                    f"‚úÖ Using source: {source_name} "
                    f"({'PRIMARY' if not is_fallback else 'FALLBACK'})"
                )

                return FallbackResult(
                    success=True,
                    data=df,
                    source_used=source_name,
                    quality_tier='gold' if not is_fallback else 'silver',
                    ...
                )

            except Exception as e:
                logger.error(f"Source {source_name} failed: {e}")
                continue

        # All sources failed
        logger.error(f"All sources in chain '{chain_name}' failed")
        return FallbackResult(success=False, data=None, source_used=None)
```

**Usage in TeamOffenseGameSummaryProcessor:**

```python
class TeamStatsQualityValidator(QualityValidator):
    """Validates team statistics data quality."""

    def validate(self, df: pd.DataFrame) -> tuple[bool, str]:
        if df.empty:
            return False, "Empty DataFrame"

        # Check for 0-value records
        invalid_mask = (df['points'] == 0) | (df['fg_attempted'] == 0)
        invalid_count = invalid_mask.sum()

        if invalid_count > 0:
            invalid_pct = invalid_count / len(df) * 100

            # If >50% invalid, fail entire dataset
            if invalid_pct > 50:
                return False, f"{invalid_pct:.1f}% of records have 0 points/FGA"

            # If some invalid, filter them out but warn
            logger.warning(f"Filtering {invalid_count} invalid records from {len(df)} total")
            # Note: Filtering happens in extractor, not here

        return True, "Quality checks passed"


# In extract_raw_data():
fallback_result = self.try_fallback_chain(
    chain_name='team_boxscores',
    extractors={
        'nbac_team_boxscore': lambda: self._extract_from_nbac_team_boxscore(start_date, end_date),
        'reconstructed_team_from_players': lambda: self._reconstruct_team_from_players(start_date, end_date),
    },
    quality_validators={
        'nbac_team_boxscore': TeamStatsQualityValidator(),
    },
    context={'game_date': start_date},
)
```

**Benefits:**
- Systemic fix for all processors using FallbackSourceMixin
- Opt-in: processors can add validators without changing mixin
- Backward compatible: existing code works without validators

#### Task 2.2: Audit Other Processors (1-2 days)

**Processors to audit:**

1. **PlayerGameSummaryProcessor** (HIGH PRIORITY)
   - Also uses FallbackSourceMixin
   - Check if has similar row count validation gap
   - Add quality validators

2. **TeamDefenseGameSummaryProcessor** (MEDIUM)
   - Likely same pattern as TeamOffense
   - Check completeness validation logic

3. **Phase 4 Processors** (MEDIUM)
   - ML Feature Store
   - Player Daily Cache
   - Check if they validate Phase 3 data quality

**Audit checklist:**
```markdown
For each processor:
- [ ] Uses FallbackSourceMixin?
- [ ] Has completeness validation?
- [ ] Completeness checks row count only?
- [ ] Has pre-write validation rules?
- [ ] Vulnerable to 0-value data?
- [ ] Needs quality validator?
```

#### Task 2.3: Add Monitoring (1 day)

**New Cloud Function:** `data-quality-monitor`

```python
def check_team_offense_quality(game_date: str):
    """Check for 0-value team records."""
    query = f"""
    SELECT
        COUNT(*) as total_teams,
        COUNTIF(points_scored = 0) as zero_points,
        COUNTIF(fg_attempts = 0) as zero_fga,
        ROUND(100.0 * COUNTIF(points_scored = 0) / COUNT(*), 1) as corruption_pct
    FROM nba_analytics.team_offense_game_summary
    WHERE game_date = '{game_date}'
    """

    result = bq_client.query(query).to_dataframe().iloc[0]

    if result['corruption_pct'] > 5.0:
        send_alert(
            severity='ERROR',
            title=f"Data Quality: {result['corruption_pct']}% team records have 0 values",
            message=f"Date: {game_date}, Zero points: {result['zero_points']}/{result['total_teams']}",
            channel='#app-error-alerts'
        )
    elif result['corruption_pct'] > 0:
        send_alert(
            severity='WARNING',
            title=f"Data Quality: Found {result['zero_points']} teams with 0 values",
            message=f"Date: {game_date}, Check if reconstruction fallback working",
            channel='#nba-alerts'
        )
```

**Scheduler:**
```bash
gcloud scheduler jobs create http data-quality-monitor \
  --schedule="0 8 * * *" \
  --uri="https://us-west2-nba-props-platform.cloudfunctions.net/data-quality-monitor" \
  --time-zone="America/Los_Angeles" \
  --location=us-west2
```

---

### Month 1: Advanced Validation (2-3 weeks)

**Priority: Ensemble validation and inter-phase gateways**

#### Task 3.1: Implement Ensemble Validation (1 week)

**File:** `data_processors/analytics/team_offense_game_summary/team_offense_game_summary_processor.py`

**New method:**
```python
def _ensemble_validate(
    self,
    official_data: pd.DataFrame,
    reconstructed_data: pd.DataFrame
) -> tuple[pd.DataFrame, dict]:
    """
    Cross-validate official team data against player-aggregated reconstruction.

    Returns:
        (selected_data, metadata): DataFrame to use and validation metadata
    """
    if official_data.empty and reconstructed_data.empty:
        return pd.DataFrame(), {'source': 'none', 'reason': 'no_data'}

    if official_data.empty:
        return reconstructed_data, {'source': 'reconstructed', 'reason': 'official_empty'}

    if reconstructed_data.empty:
        logger.warning("Reconstruction failed, using official data (risky)")
        return official_data, {'source': 'official', 'reason': 'reconstruction_failed'}

    # Both sources available - cross-validate
    merged = official_data.merge(
        reconstructed_data,
        on=['game_id', 'team_abbr'],
        suffixes=('_official', '_recon')
    )

    discrepancies = []
    for _, row in merged.iterrows():
        points_diff = abs(row['points_official'] - row['points_recon'])
        fga_diff = abs(row['fg_attempted_official'] - row['fg_attempted_recon'])

        if points_diff > row['points_official'] * 0.05:  # >5% difference
            discrepancies.append({
                'team': row['team_abbr'],
                'game': row['game_id'],
                'field': 'points',
                'official': row['points_official'],
                'reconstructed': row['points_recon'],
                'diff_pct': points_diff / row['points_official'] * 100
            })

    if discrepancies:
        logger.warning(
            f"‚ö†Ô∏è  ENSEMBLE VALIDATION: Found {len(discrepancies)} discrepancies between "
            f"official and reconstructed data. Using reconstructed data for safety."
        )

        for disc in discrepancies:
            logger.warning(
                f"  {disc['team']} {disc['field']}: "
                f"official={disc['official']}, reconstructed={disc['reconstructed']} "
                f"(diff={disc['diff_pct']:.1f}%)"
            )

        # Log to monitoring
        self._log_ensemble_discrepancy(discrepancies)

        return reconstructed_data, {
            'source': 'reconstructed',
            'reason': 'discrepancy_detected',
            'discrepancies': discrepancies
        }

    # Sources agree, use official
    logger.info("‚úÖ Ensemble validation passed - sources agree")
    return official_data, {'source': 'official', 'reason': 'ensemble_validated'}
```

**Integration in extract_raw_data():**
```python
# Extract from both sources
official_data = self._extract_from_nbac_team_boxscore(start_date, end_date)
reconstructed_data = self._reconstruct_team_from_players(start_date, end_date)

# Ensemble validation
self.raw_data, ensemble_meta = self._ensemble_validate(official_data, reconstructed_data)
self._source_used = ensemble_meta['source']
self._ensemble_metadata = ensemble_meta
```

#### Task 3.2: Inter-Phase Validation Gateway (1 week)

**New component:** `shared/validation/phase_gateway.py`

```python
class PhaseValidationGateway:
    """Validates data quality between pipeline phases."""

    def __init__(self, project_id: str):
        self.project_id = project_id
        self.bq_client = bigquery.Client(project=project_id)

    def validate_phase_transition(
        self,
        source_phase: int,
        target_phase: int,
        table: str,
        data: pd.DataFrame
    ) -> ValidationResult:
        """
        Validate data before allowing phase transition.

        Example:
            gateway.validate_phase_transition(
                source_phase=3,
                target_phase=4,
                table='player_game_summary',
                data=df
            )
        """
        contract = self._load_contract(table)
        violations = contract.validate(data)

        if violations.has_errors():
            raise DataQualityError(
                f"Phase {source_phase}‚Üí{target_phase} transition blocked for {table}. "
                f"Errors: {violations.errors}"
            )

        if violations.has_warnings():
            logger.warning(
                f"Phase {source_phase}‚Üí{target_phase} quality warnings for {table}: "
                f"{violations.warnings}"
            )

        # Log transition event
        self._log_phase_transition(
            source_phase=source_phase,
            target_phase=target_phase,
            table=table,
            record_count=len(data),
            quality_score=violations.quality_score,
            timestamp=datetime.now(timezone.utc)
        )

        return ValidationResult(
            passed=True,
            warnings=violations.warnings,
            quality_score=violations.quality_score
        )
```

**Usage in processors:**
```python
# Before saving analytics data
from shared.validation.phase_gateway import PhaseValidationGateway

gateway = PhaseValidationGateway(self.project_id)

# Validate Phase 2‚Üí3 transition
gateway.validate_phase_transition(
    source_phase=2,
    target_phase=3,
    table='team_offense_game_summary',
    data=self.analytics_data
)

# If no errors raised, proceed with save
self.save_analytics()
```

#### Task 3.3: Circuit Breaker Pattern (1 week)

**Automatically disable sources with high failure rates**

```python
class SourceCircuitBreaker:
    """
    Monitors data source reliability and auto-disables unreliable sources.

    Tracks:
    - Success rate over rolling 7-day window
    - Quality failure rate (data exists but invalid)
    - Consecutive failures

    Actions:
    - OPEN circuit: Disable source after 3 consecutive quality failures
    - HALF-OPEN: Re-try after 24 hours
    - CLOSED: Normal operation
    """

    def check_source(self, source_name: str, game_date: str) -> CircuitState:
        """Check if source should be used."""
        state = self._get_circuit_state(source_name)

        if state == CircuitState.OPEN:
            # Check if cooldown period passed
            if self._can_retry(source_name):
                logger.info(f"Circuit breaker for {source_name}: HALF-OPEN (testing)")
                return CircuitState.HALF_OPEN

            logger.warning(f"Circuit breaker for {source_name}: OPEN (disabled)")
            return CircuitState.OPEN

        return CircuitState.CLOSED

    def record_success(self, source_name: str, game_date: str):
        """Record successful extraction with valid data."""
        self._record_event(source_name, game_date, success=True, quality_valid=True)

        # Reset circuit if was half-open
        if self._get_circuit_state(source_name) == CircuitState.HALF_OPEN:
            logger.info(f"Circuit breaker for {source_name}: CLOSED (recovered)")
            self._set_circuit_state(source_name, CircuitState.CLOSED)

    def record_quality_failure(self, source_name: str, game_date: str, reason: str):
        """Record that source returned data but it failed quality checks."""
        self._record_event(source_name, game_date, success=True, quality_valid=False)

        # Check if should open circuit
        consecutive_failures = self._get_consecutive_quality_failures(source_name)

        if consecutive_failures >= 3:
            logger.error(
                f"Circuit breaker for {source_name}: OPENING "
                f"(3 consecutive quality failures)"
            )
            self._set_circuit_state(source_name, CircuitState.OPEN)
            self._send_alert(
                severity='ERROR',
                title=f"Data source {source_name} circuit breaker OPENED",
                message=f"3 consecutive quality failures. Auto-disabled for 24 hours."
            )
```

**Integration:**
```python
# In TeamOffenseGameSummaryProcessor
circuit_breaker = SourceCircuitBreaker(self.project_id)

# Before trying source
if circuit_breaker.check_source('nbac_team_boxscore', game_date) == CircuitState.OPEN:
    logger.info("Skipping nbac_team_boxscore (circuit breaker open)")
    # Use reconstruction directly
    self.raw_data = self._reconstruct_team_from_players(start_date, end_date)
else:
    # Normal fallback chain
    fallback_result = self.try_fallback_chain(...)

    # Record result
    if fallback_result.success and fallback_result.source_used == 'nbac_team_boxscore':
        if self._data_quality_valid(fallback_result.data):
            circuit_breaker.record_success('nbac_team_boxscore', game_date)
        else:
            circuit_breaker.record_quality_failure('nbac_team_boxscore', game_date, 'zero_values')
```

---

### Long-Term: Architecture Enhancements (Quarter 2)

#### Validation Contracts for All Tables

**Goal:** Every major table has a YAML validation contract

**Structure:**
```
shared/validation/contracts/
‚îú‚îÄ‚îÄ phase2/
‚îÇ   ‚îú‚îÄ‚îÄ nbac_team_boxscore.yaml
‚îÇ   ‚îú‚îÄ‚îÄ bdl_player_boxscores.yaml
‚îÇ   ‚îî‚îÄ‚îÄ odds_api_player_points_props.yaml
‚îú‚îÄ‚îÄ phase3/
‚îÇ   ‚îú‚îÄ‚îÄ player_game_summary.yaml
‚îÇ   ‚îú‚îÄ‚îÄ team_offense_game_summary.yaml
‚îÇ   ‚îî‚îÄ‚îÄ team_defense_game_summary.yaml
‚îú‚îÄ‚îÄ phase4/
‚îÇ   ‚îú‚îÄ‚îÄ player_daily_cache.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ml_feature_store_v2.yaml
‚îî‚îÄ‚îÄ phase5/
    ‚îî‚îÄ‚îÄ player_prop_predictions.yaml
```

**Example contract:**
```yaml
# shared/validation/contracts/phase3/player_game_summary.yaml
table: player_game_summary
phase: 3
description: "Player performance analytics per game"

required_fields:
  - player_lookup
  - game_id
  - game_date
  - points
  - minutes_played

domain_constraints:
  - field: points
    type: integer
    min: 0
    max: 100
    error_if: "points > 100"  # Physically impossible
    warn_if: "points > 70"    # Extremely rare

  - field: usage_rate
    type: float
    min: 0.0
    max: 100.0
    error_if: "usage_rate > 100"  # Calculation error
    warn_if: "usage_rate > 50"    # Very unusual

  - field: is_dnp
    type: boolean
    validation: "is_dnp IS NOT NULL"
    error_if: "is_dnp IS NULL"

relational_integrity:
  - constraint: team_exists
    sql: |
      team_abbr IN (
        SELECT team_abbr FROM nba_reference.teams
        WHERE is_active = TRUE
      )
    error_message: "Team not found in reference data"

statistical_plausibility:
  - check: minutes_played_reasonable
    sql: "minutes_played BETWEEN 0 AND 60"
    warn_if_fails: true
    message: "Player minutes outside normal range (0-60)"
```

#### Data Lineage Tracking

**Track which source/scraper produced each record**

```python
# Add to every table
lineage_fields = {
    'source_scraper': str,      # Which scraper produced raw data
    'source_timestamp': datetime, # When scraped
    'processor_version': str,    # Code version that processed
    'data_generation': int,      # 1=original, 2=reprocessed, etc.
}
```

**Benefits:**
- Debug data quality issues faster
- Know when to trust data (recent scrape vs old)
- Audit trail for compliance

---

## Files to Review

### Critical Files (Must Review Before Implementation)

| File | Purpose | Lines to Focus |
|------|---------|----------------|
| `shared/processors/patterns/fallback_source_mixin.py` | Shared fallback logic | 140-230 (try_fallback_chain) |
| `data_processors/analytics/team_offense_game_summary/team_offense_game_summary_processor.py` | Team stats processor | 323-347 (emergency override)<br>350-420 (fallback chain)<br>429-553 (extractor)<br>555-700 (reconstruction) |
| `shared/validation/pre_write_validator.py` | Pre-write validation framework | Review entire file |

### Supporting Files

| File | Purpose |
|------|---------|
| `shared/processors/patterns/quality_mixin.py` | Quality tracking |
| `data_processors/analytics/analytics_base.py` | Base processor class |
| `schemas/team_offense_game_summary.json` | BigQuery schema |

### Reference Documentation

| Doc | Relevant Sections |
|-----|-------------------|
| `docs/05-development/PROCESSOR-PATTERNS.md` | Fallback pattern usage |
| `docs/02-operations/troubleshooting-matrix.md` | Data quality issues |
| `docs/01-architecture/PHASE-3-ANALYTICS.md` | Phase 3 architecture |

---

## Next Session Plan

### Session Goals

**Primary:** Implement Day 1 quick wins (4 hours)
**Secondary:** Begin Week 1 systemic fixes

### Pre-Session Checklist

1. **Review Opus strategic assessment** (Section 5)
2. **Read technical deep dive** (Section 3)
3. **Review implementation roadmap** (Section 6)
4. **Check current state:**
   ```bash
   # Is Feb 3 data still broken?
   bq query "SELECT team_abbr, points_scored FROM nba_analytics.team_offense_game_summary WHERE game_date = '2026-02-03' AND team_abbr IN ('PHX', 'POR')"

   # Is Feb 4 data affected?
   bq query "SELECT team_abbr, points_scored FROM nba_analytics.team_offense_game_summary WHERE game_date = '2026-02-04'"
   ```

### Session Workflow

**Hour 1: Quick Fix**
- [ ] Enable `FORCE_TEAM_RECONSTRUCTION=true`
- [ ] Regenerate Feb 3 data
- [ ] Verify PHX/POR stats correct
- [ ] Verify usage_rate coverage restored

**Hour 2: Extractor Quality Validation**
- [ ] Implement quality filter in `_extract_from_nbac_team_boxscore()`
- [ ] Add unit tests for quality validation
- [ ] Deploy to staging
- [ ] Test with Feb 3 data

**Hour 3: Pre-Write Validation**
- [ ] Create `team_offense_game_summary.yaml` validation rules
- [ ] Integrate with pre-write validator
- [ ] Test with bad data (should reject)
- [ ] Test with good data (should accept)

**Hour 4: Deploy & Verify**
- [ ] Deploy to production
- [ ] Reprocess Feb 3 (without FORCE_TEAM_RECONSTRUCTION)
- [ ] Verify extractor filters bad data
- [ ] Verify fallback to reconstruction works
- [ ] Monitor logs for quality warnings

### Decision Points

**After Hour 1:**
- Continue to systemic fix? OR
- Just use FORCE_TEAM_RECONSTRUCTION as workaround?
- **Recommendation:** Continue - this will happen again

**After Day 1:**
- Continue to Week 1 tasks? OR
- Write detailed spec for Week 1 work?
- **Recommendation:** Continue if time permits, ensemble validation is high value

### Success Criteria

**Day 1 Complete:**
- [ ] Feb 3 data corrected (PHX=130, POR=125 points)
- [ ] Usage_rate coverage 100% for PHX/POR game
- [ ] Quality validation in extractor prevents future 0-value propagation
- [ ] Pre-write validation blocks bad writes
- [ ] No new bugs introduced

**Week 1 Complete:**
- [ ] FallbackSourceMixin enhanced with quality hooks
- [ ] PlayerGameSummaryProcessor audited
- [ ] Monitoring alerts for data quality
- [ ] Documentation updated

---

## Key Learnings

### What Went Well (Session 117)

1. ‚úÖ **Systematic investigation** - Didn't stop at surface issue, traced to root cause
2. ‚úÖ **Agent usage** - Spawned specialized agents for complex analysis
3. ‚úÖ **Opus strategic review** - High-level architectural thinking caught systemic issues
4. ‚úÖ **Manual verification** - Tested reconstruction manually, proved it works
5. ‚úÖ **Comprehensive documentation** - This handoff doc will save hours in next session

### What to Improve

1. ‚ö†Ô∏è **Validation gaps** - Need systemic approach, not reactive fixes
2. ‚ö†Ô∏è **Testing** - Should have unit tests for quality validation
3. ‚ö†Ô∏è **Monitoring** - No alerts for data quality issues
4. ‚ö†Ô∏è **Documentation** - Validation rules should be in code (YAML), not just docs

### Prevention for Future Sessions

**Before making changes:**
- [ ] Check if similar pattern exists elsewhere (use agents to search)
- [ ] Consider systemic fix, not just local patch
- [ ] Add monitoring/alerting for new failure modes
- [ ] Add validation rules, not just error handling

**After making changes:**
- [ ] Add unit tests for edge cases
- [ ] Document known failure modes
- [ ] Add runbook for when it fails again
- [ ] Update architecture docs

---

## Appendix A: Data Quality Stats

### Feb 3 Corruption Analysis

```sql
-- Full analysis query
WITH feb3_teams AS (
  SELECT
    team_abbr,
    points_scored,
    fg_attempts,
    possessions,
    primary_source_used,
    CASE
      WHEN points_scored = 0 OR fg_attempts = 0 THEN 'CORRUPTED'
      ELSE 'VALID'
    END as data_quality
  FROM nba_analytics.team_offense_game_summary
  WHERE game_date = '2026-02-03'
)
SELECT
  data_quality,
  COUNT(*) as teams,
  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER(), 1) as pct,
  STRING_AGG(team_abbr ORDER BY team_abbr) as teams_list
FROM feb3_teams
GROUP BY data_quality
```

**Results:**
- VALID: 18 teams (90%)
- CORRUPTED: 2 teams (10%) - PHX, POR

### Historical Corruption Analysis (TODO for next session)

Check if this is recurring:
```sql
SELECT
  game_date,
  COUNT(*) as total_teams,
  COUNTIF(points_scored = 0) as corrupted,
  ROUND(100.0 * COUNTIF(points_scored = 0) / COUNT(*), 1) as corruption_pct
FROM nba_analytics.team_offense_game_summary
WHERE game_date >= '2026-01-01'
GROUP BY game_date
HAVING corrupted > 0
ORDER BY game_date DESC
```

---

## Appendix B: Quick Reference

### Commands

**Check data quality:**
```bash
# Team offense quality for a date
bq query "SELECT team_abbr, points_scored, fg_attempts, possessions FROM nba_analytics.team_offense_game_summary WHERE game_date = 'YYYY-MM-DD' ORDER BY points_scored ASC"

# Usage rate coverage for a date
bq query "SELECT ROUND(100.0 * COUNTIF(usage_rate IS NOT NULL) / COUNT(*), 1) as coverage FROM nba_analytics.player_game_summary WHERE game_date = 'YYYY-MM-DD' AND is_dnp = FALSE"
```

**Regenerate with reconstruction:**
```bash
# Enable override
gcloud run services update nba-phase3-analytics-processors --region=us-west2 --update-env-vars FORCE_TEAM_RECONSTRUCTION=true

# Reprocess
curl -X POST "https://nba-phase3-analytics-processors-756957797294.us-west2.run.app/process-date-range" \
  -H "Authorization: Bearer $(gcloud auth print-identity-token)" \
  -H "Content-Type: application/json" \
  -d '{"start_date": "YYYY-MM-DD", "end_date": "YYYY-MM-DD", "processors": ["TeamOffenseGameSummaryProcessor"], "backfill_mode": true}'

# Disable override
gcloud run services update nba-phase3-analytics-processors --region=us-west2 --update-env-vars FORCE_TEAM_RECONSTRUCTION=false
```

### Key Metrics to Monitor

| Metric | Threshold | Query |
|--------|-----------|-------|
| Team corruption rate | <5% | `COUNTIF(points = 0) / COUNT(*)` |
| Usage rate coverage | >80% | `COUNTIF(usage_rate IS NOT NULL AND is_dnp = FALSE) / COUNTIF(is_dnp = FALSE)` |
| Ensemble discrepancy rate | <10% | Track in monitoring |
| Circuit breaker open | Alert if >1 source | Check Firestore |

---

## Related Sessions

- **Session 116:** Feb 3 data gap investigation (84% ‚Üí 100% recovery)
- **Session 115:** DNP architecture validation
- **Session 105:** Team stats completeness check (missing teams)
- **Session 97:** ML Feature Store quality gate
- **Session 28:** Model drift monitoring

---

## Contact for Questions

**Key files changed:** None (investigation only)
**Services deployed:** None
**Data regenerated:** None (TODO for next session)
**Tests added:** None (TODO for next session)

**Handoff prepared by:** Session 117 (Claude Sonnet 4.5)
**Date:** February 5, 2026
**Next session priority:** HIGH
**Estimated implementation time:** 1-2 days (Day 1: 4 hours, Week 1: 1-2 days)

---

**üöÄ Ready for implementation!**
