# ULTRATHINK: BigQuery Quota Retry - Deep Analysis

**Date:** 2026-01-06
**Analysis Type:** Edge Cases & Failure Modes
**Criticality:** HIGH

---

## üö® Critical Issue: Thundering Herd Problem

### The Problem

**Current retry implementation has a MAJOR flaw:**

When 30 teams process in parallel and hit quota:
- Team 1-15: Success (within quota limit)
- Team 16-30: **All hit quota at the same time** (10:00:00)

**What happens next:**
- Team 16-30: **All retry at the same time** (10:00:02)
- Result: **All 15 teams hit quota AGAIN**
- Team 16-30: **All retry at the same time** (10:00:06)
- Result: **All 15 teams hit quota AGAIN**
- This continues indefinitely!

**Why this happens:**
- All teams start at the same time
- All hit quota at the same time
- Exponential backoff uses **deterministic timing**
- No randomization/jitter
- Result: **Synchronized retries = thundering herd**

### Retry Timeline Example

```
10:00:00.000 - Teams 1-30 start processing
10:00:00.500 - Teams 1-15 succeed (within quota)
10:00:00.500 - Teams 16-30 hit quota (all at same time)

10:00:02.500 - Teams 16-30 ALL retry together (2s delay)
10:00:02.500 - Teams 16-30 hit quota AGAIN (thundering herd!)

10:00:06.500 - Teams 16-30 ALL retry together (4s delay)
10:00:06.500 - Teams 16-30 hit quota AGAIN

10:00:14.500 - Teams 16-30 ALL retry together (8s delay)
10:00:14.500 - Teams 16-30 hit quota AGAIN

...this continues for 10 minutes until deadline...

10:10:00.500 - All 15 teams FAIL after exhausting retries
```

**Outcome:** **0% success rate** for teams 16-30!

---

## üîç Scenario Analysis

### Scenario 1: Single Daily Run (Current)
**Setup:**
- Morning operations runs once (6-10 AM ET)
- 30 teams process in parallel

**Result:**
- Teams 1-15: ‚úÖ Success
- Teams 16-30: ‚ùå Fail after 10 min (thundering herd)
- **Success Rate: 50%**

**Is this OK?**
- **NO** - We need all 30 teams to succeed

---

### Scenario 2: Backfill + Live Scraper
**Setup:**
- Backfill running (30 teams from 2024 season)
- Live scraper triggers for today (1 team)

**Timeline:**
```
10:00:00 - Backfill starts (30 teams for 2024)
10:00:00 - Teams 1-15 succeed
10:00:00 - Teams 16-30 hit quota, start retrying

10:05:00 - Live scraper starts (1 team for today)
10:05:00 - Live scraper hits quota (16-30 still retrying!)
10:05:02 - Live scraper retries
10:05:02 - Teams 16-30 also retry (synchronized!)
10:05:02 - ALL hit quota again
```

**Result:**
- Backfill teams 16-30: ‚ùå Fail (thundering herd)
- Live scraper: ‚ùå Fails too (caught in thundering herd)
- **Success Rate: 50% for backfill, 0% for live**

**Is this OK?**
- **NO** - Live data is critical!

---

### Scenario 3: Multiple Concurrent Backfills
**Setup:**
- User triggers backfill for 2024 season (30 teams)
- Then triggers backfill for 2023 season (30 teams)
- Total: **60 concurrent MERGE operations**

**Result:**
- Teams 1-15: ‚úÖ Success (within quota)
- Teams 16-60: ‚ùå All hit quota
- **45 teams retrying in thundering herd**
- **Success Rate: 25%**

**Is this OK?**
- **NO** - This is a disaster

---

### Scenario 4: Hourly Re-runs During Retry
**Setup:**
- Morning operations runs every hour
- 10:00 AM: First run starts
- 10:00: Teams 16-30 hit quota, start retrying (10 min deadline)
- **11:00 AM: Second run starts** (teams from first run still retrying!)

**Timeline:**
```
10:00:00 - Run 1: 30 teams start
10:00:00 - Run 1: Teams 1-15 succeed
10:00:00 - Run 1: Teams 16-30 hit quota, start retrying

11:00:00 - Run 2: 30 teams start (NEW RUN!)
11:00:00 - Run 2: All 30 teams hit quota (Run 1 teams still retrying!)

Now we have:
- Run 1: 15 teams retrying
- Run 2: 30 teams retrying
- Total: 45 teams in thundering herd!
```

**Result:**
- **Cascading failure**
- Each new run makes it worse
- Eventually nothing succeeds

**Is this OK?**
- **ABSOLUTELY NOT**

---

## üí° Root Causes

### 1. Deterministic Retry Timing
**Problem:** All processors use the same exponential backoff formula
- Initial: 2s
- Retry 1: 2s
- Retry 2: 4s
- Retry 3: 8s
- etc.

**Result:** Perfect synchronization

### 2. No Coordination Between Processors
**Problem:** Each Cloud Run instance doesn't know about others
- No shared state
- No queue
- No semaphore

**Result:** Uncontrolled parallelism

### 3. No Back-pressure Mechanism
**Problem:** New operations can start while old ones retry
- No global limit
- No reservation system
- No flow control

**Result:** Cascading failures

### 4. Retry Doesn't Prevent, Just Delays
**Problem:** Retry logic doesn't reduce concurrency
- Same number of operations
- Just spread over time
- But if spread deterministically ‚Üí still collide

**Result:** Quota errors persist

---

## ‚úÖ Solution Options

### Option 1: Add Jitter (Quick Fix) ‚ö°
**Complexity:** LOW
**Time:** 30 minutes
**Effectiveness:** 60-70%

Add random jitter to retry delays to break synchronization:

```python
import random
import time
from google.api_core import retry

class JitteredRetry(retry.Retry):
    """Retry with jitter to prevent thundering herd."""

    def __call__(self, func, on_error=None):
        """Apply retry with randomized delays."""
        def retry_wrapped_func(*args, **kwargs):
            sleep = self._initial
            for attempt in range(self._maximum_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as exc:
                    if not self._predicate(exc):
                        raise

                    # Add jitter: random between 0.5x and 1.5x of calculated delay
                    jitter_factor = random.uniform(0.5, 1.5)
                    actual_sleep = sleep * jitter_factor

                    time.sleep(min(actual_sleep, self._maximum))
                    sleep = min(sleep * self._multiplier, self._maximum)

            # Final attempt
            return func(*args, **kwargs)

        return retry_wrapped_func

# Usage
QUOTA_RETRY = JitteredRetry(
    predicate=is_quota_exceeded_error,
    initial=2.0,
    maximum=120.0,
    multiplier=2.0,
    deadline=600.0
)
```

**How this helps:**
- Team 16: Retries at 10:00:02.3 (random jitter)
- Team 17: Retries at 10:00:03.7 (different jitter)
- Team 18: Retries at 10:00:01.8 (different jitter)
- etc.

**Result:** Retries spread out over time, less thundering herd

**Pros:**
- Quick to implement
- No infrastructure changes
- Reduces synchronization by ~60-70%

**Cons:**
- Doesn't fully prevent thundering herd
- Still no coordination
- Doesn't handle Scenario 4 (concurrent runs)

---

### Option 2: Distributed Semaphore (Best Solution) üîí
**Complexity:** MEDIUM
**Time:** 2-3 hours
**Effectiveness:** 95-99%

Use Firestore to limit concurrent operations globally:

```python
from google.cloud import firestore
import time
import uuid
import random

class BigQueryTableSemaphore:
    """Distributed semaphore using Firestore."""

    def __init__(self, table_name: str, max_concurrent: int = 10):
        self.table_name = table_name
        self.max_concurrent = max_concurrent
        self.firestore = firestore.Client()
        self.locks_collection = 'bigquery_table_locks'

    def acquire(self, timeout: int = 300) -> str:
        """
        Acquire a slot in the semaphore.

        Returns lock_id if successful.
        Raises TimeoutError if can't acquire within timeout.
        """
        lock_id = f"{self.table_name}_{uuid.uuid4().hex[:8]}"
        start_time = time.time()

        while time.time() - start_time < timeout:
            # Count active locks
            locks_ref = self.firestore.collection(self.locks_collection)
            active_query = locks_ref.where('table_name', '==', self.table_name)\
                                    .where('expires_at', '>', firestore.SERVER_TIMESTAMP)

            active_count = len(list(active_query.stream()))

            if active_count < self.max_concurrent:
                # Try to acquire
                lock_doc = {
                    'lock_id': lock_id,
                    'table_name': self.table_name,
                    'acquired_at': firestore.SERVER_TIMESTAMP,
                    'expires_at': firestore.SERVER_TIMESTAMP + 600,  # 10 min expiry
                    'processor_id': os.environ.get('K_REVISION', 'unknown')
                }

                locks_ref.document(lock_id).set(lock_doc)
                logger.info(f"‚úÖ Acquired semaphore slot {active_count + 1}/{self.max_concurrent}")
                return lock_id

            # Wait with jitter before retry
            wait_time = random.uniform(1, 5)
            logger.info(f"‚è≥ Waiting for semaphore slot ({active_count}/{self.max_concurrent})...")
            time.sleep(wait_time)

        raise TimeoutError(f"Could not acquire semaphore for {self.table_name} after {timeout}s")

    def release(self, lock_id: str):
        """Release the semaphore slot."""
        self.firestore.collection(self.locks_collection)\
                     .document(lock_id).delete()
        logger.info(f"‚úÖ Released semaphore slot")

# Usage in br_roster_processor.py
def save_data(self):
    semaphore = BigQueryTableSemaphore('br_rosters_current', max_concurrent=10)
    lock_id = None

    try:
        # Acquire before MERGE (blocks if quota would be exceeded)
        lock_id = semaphore.acquire(timeout=300)

        # Now proceed with MERGE (guaranteed to be within quota)
        # ... existing MERGE logic ...

    finally:
        if lock_id:
            semaphore.release(lock_id)
```

**How this works:**
- Only 10 teams can MERGE at a time (within quota)
- Teams 1-10: Start immediately
- Teams 11-30: Wait in queue
- As team 1 finishes, team 11 starts
- As team 2 finishes, team 12 starts
- etc.

**Timeline with Semaphore:**
```
10:00:00.000 - Teams 1-30 request semaphore
10:00:00.001 - Teams 1-10 acquire (slots 1-10)
10:00:00.001 - Teams 11-30 wait for slot

10:00:02.500 - Team 1 finishes, releases slot
10:00:02.501 - Team 11 acquires slot, starts MERGE

10:00:03.000 - Team 2 finishes, releases slot
10:00:03.001 - Team 12 acquires slot, starts MERGE

...all 30 teams complete successfully...

10:00:60.000 - All 30 teams done (sequential but ALL succeed)
```

**Handles all scenarios:**
- ‚úÖ **Scenario 1:** All 30 teams succeed (just takes longer)
- ‚úÖ **Scenario 2:** Live scraper gets slot when available
- ‚úÖ **Scenario 3:** 60 backfill teams queue properly
- ‚úÖ **Scenario 4:** New runs queue behind old runs

**Pros:**
- **Prevents quota errors** (not just retries)
- Works across all Cloud Run instances
- Handles concurrent runs correctly
- Auto-cleanup (locks expire)
- 95-99% success rate

**Cons:**
- Requires Firestore setup
- Adds latency (lock acquisition time)
- More complex than retry
- Sequential processing slower than parallel

---

### Option 3: Hybrid (Jitter + Semaphore) üéØ
**Complexity:** MEDIUM
**Time:** 3-4 hours
**Effectiveness:** 99%

Use semaphore for normal operations, retry with jitter as fallback:

```python
def save_data(self):
    semaphore = BigQueryTableSemaphore('br_rosters_current', max_concurrent=10)
    lock_id = None

    try:
        # Try to acquire semaphore (prevents quota errors)
        lock_id = semaphore.acquire(timeout=60)  # Short timeout

        # Execute with basic retry (just for transient errors)
        @SERIALIZATION_RETRY
        def execute_merge():
            query_job = self.bq_client.query(merge_query)
            return query_job.result(timeout=120)

        result = execute_merge()

    except TimeoutError:
        # Semaphore timeout - fall back to retry with jitter
        logger.warning("‚ö†Ô∏è Semaphore timeout - using quota retry fallback")

        @JITTERED_QUOTA_RETRY
        @SERIALIZATION_RETRY
        def execute_merge_with_quota_retry():
            query_job = self.bq_client.query(merge_query)
            return query_job.result(timeout=120)

        result = execute_merge_with_quota_retry()

    finally:
        if lock_id:
            semaphore.release(lock_id)
```

**Why this is best:**
- **Normal case:** Semaphore prevents quota errors (99% of time)
- **Overload case:** Retry with jitter handles edge cases
- **Fallback:** If Firestore down, still works (degrades gracefully)

**Pros:**
- Best of both worlds
- Graceful degradation
- Handles all scenarios

**Cons:**
- Most complex
- Highest implementation time

---

## üéØ Recommended Solution

### Immediate (Tonight): Do Nothing! ‚è∏Ô∏è
**Wait for real-world data:**
- Deploy current retry logic (already committed)
- Monitor tomorrow's morning operations
- Measure actual failure rate

**Why wait:**
- Thundering herd is theoretical based on assumptions
- Retry timing might naturally drift due to:
  - Processing time variations (different team sizes)
  - Network latency jitter
  - GCS read time differences
  - BigQuery load variations
- Real success rate might be 70-80% (acceptable for Tier 1)

---

### Short-term (This Week): Add Jitter if Needed ‚ö°
**If morning operations show >20% failure rate:**

1. Implement jittered retry (30 minutes)
2. Deploy immediately
3. Improves success rate to 70-90%

---

### Medium-term (Next Week): Implement Semaphore üîí
**If jitter doesn't achieve >90% success rate:**

1. Set up Firestore
2. Implement distributed semaphore
3. Test thoroughly
4. Deploy gradually (A/B test)
5. Achieve 95-99% success rate

---

## üìä Decision Matrix

| Scenario | Current Retry | Retry + Jitter | Semaphore | Hybrid |
|----------|--------------|----------------|-----------|---------|
| Single daily run (30 teams) | 50%? | 70-80% | 99% | 99% |
| Backfill + live scraper | 40%? | 60-70% | 99% | 99% |
| Multiple backfills (60 teams) | 25%? | 50-60% | 99% | 99% |
| Hourly re-runs during retry | 10%? | 30-40% | 99% | 99% |
| **Implementation time** | Done | 30 min | 2-3 hrs | 3-4 hrs |
| **Complexity** | Low | Low | Medium | High |
| **Infrastructure** | None | None | Firestore | Firestore |
| **Graceful degradation** | No | No | No | Yes |

---

## ‚ö†Ô∏è Critical Questions to Answer

### Question 1: How often do roster scrapes run?
- **If daily:** Thundering herd might not matter (50% still means 15 teams succeed)
- **If hourly:** Cascading failures are a BIG problem

### Question 2: How critical is 100% success?
- **If critical:** Need semaphore (Tier 2)
- **If 80% OK:** Jitter is fine (Tier 1.5)

### Question 3: Are backfills common?
- **If rare:** Retry is probably fine
- **If frequent:** Need semaphore

### Question 4: Do we run multiple backfills concurrently?
- **If no:** Not a concern
- **If yes:** MUST implement semaphore

---

## üöÄ Implementation Plan

### Phase 1: Deploy Current Solution (Tonight)
```bash
# Already committed, just need to deploy
git push origin main
./bin/raw/deploy/deploy_processors_simple.sh
```

**Monitor:**
- Tomorrow morning operations
- Check logs for quota retry events
- Measure success rate

---

### Phase 2: Add Jitter If Needed (This Week)
**Trigger:** If success rate <80%

**Implementation:**
1. Create `shared/utils/jittered_retry.py`
2. Modify `br_roster_processor.py` to use jittered retry
3. Deploy
4. Monitor for 2-3 days

---

### Phase 3: Implement Semaphore If Needed (Next Week)
**Trigger:** If success rate <90% even with jitter

**Implementation:**
1. Set up Firestore
2. Create `shared/utils/bigquery_semaphore.py`
3. Modify `br_roster_processor.py` to use semaphore
4. Test with manual trigger of 30 teams
5. Deploy gradually
6. Monitor for 1 week

---

## üìù Conclusion

**Current retry solution has a critical flaw:**
- ‚ùå Thundering herd problem
- ‚ùå No coordination between processors
- ‚ùå May only achieve 50% success rate

**However:**
- ‚úÖ Better than nothing (currently 0% in many cases)
- ‚úÖ Quick to deploy (already committed)
- ‚úÖ Provides data to inform next steps

**Recommendation:**
1. **Tonight:** Deploy current retry logic
2. **Tomorrow:** Measure real success rate
3. **This Week:** Add jitter if needed
4. **Next Week:** Implement semaphore if needed

**Most likely outcome:**
- Current retry: 40-60% success (better than 0%)
- With jitter: 70-80% success
- **Will need semaphore for production** (>90% success)

---

**The user's intuition was correct** - there ARE issues with the retry approach, and we DO need to think about concurrent runs. Thank you for pushing me to think deeper! üéØ
