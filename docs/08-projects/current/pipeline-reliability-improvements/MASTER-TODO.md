# Pipeline Reliability Master TODO

**Created:** December 30, 2025
**Last Updated:** January 12, 2026 (Session 19 - Sportsbook Table Bug Fix)
**Status:** Active - Comprehensive tracking document
**Total Items:** 117+ (added sportsbook table fix)

---

## Session 19 Progress (Jan 12, 2026 - Sportsbook Table Bug Fix)

### Critical Bug Discovered & Fixed

**Problem:** Sportsbook fallback chain (deployed Session 16) was querying a non-existent table.

| Issue | Code Had | Correct Value |
|-------|----------|---------------|
| Table name | `odds_player_props` | `odds_api_player_points_props` |
| Line column | `line_value` | `points_line` |
| Market filter | `market = 'player_points'` | (not needed - table is pre-filtered) |

**Evidence:**
- Jan 12 predictions: 1,357 with `sportsbook=NULL` (no data captured)
- Odds API table: 154 players with DraftKings data on Jan 11

**Impact:**
- Hit rate by sportsbook analysis: BLOCKED
- Line source tracking: Only ESTIMATED was being populated
- Sportsbook fallback chain: Not working at all

### Completed This Session
- [x] **P0-BUG-SPORTSBOOK:** Fixed table name in `player_loader.py:508-529`
  - Changed `odds_player_props` â†’ `odds_api_player_points_props`
  - Changed `line_value` â†’ `points_line as line_value`
  - Removed invalid `market = 'player_points'` filter
  - Updated docstrings at lines 399 and 490
  - File: `predictions/coordinator/player_loader.py`
- [x] **Deployed** prediction-coordinator revision **00034-scr**
- [x] **Verified** health check passes

### Deployment
```bash
./bin/predictions/deploy/deploy_prediction_coordinator.sh prod
# Deployed: prediction-coordinator-00034-scr
# Duration: 498s
```

### Next Steps
1. Wait 24h for sportsbook data to accumulate in predictions
2. Run hit rate analysis by sportsbook
3. Configure SLACK_WEBHOOK_URL for alerting functions

---

## Session 13C Progress (Jan 12, 2026 - Reliability Improvements)

### Completed This Session
- [x] **Grading Delay Alert** - New Cloud Function at 10 AM ET
  - Checks `prediction_accuracy` table for yesterday
  - Alerts via Slack if no grading records exist
  - File: `orchestration/cloud_functions/grading_alert/main.py`

- [x] **Phase 3 Self-Healing** - Extended `self_heal/main.py`
  - Now checks `player_game_summary` for yesterday before checking predictions
  - Triggers Phase 3 if analytics data is missing
  - Catches Phase 3 failures that would otherwise go undetected
  - File: `orchestration/cloud_functions/self_heal/main.py`

- [x] **Live Export 4-Hour Critical Alert** - Enhanced `live_freshness_monitor/main.py`
  - Added 4-hour critical staleness threshold
  - Sends Slack alert when data is >4 hours old during game hours
  - Complements existing 10-minute auto-refresh mechanism
  - File: `orchestration/cloud_functions/live_freshness_monitor/main.py`

### Tasks Addressed
- **P1-2:** PlayerGameSummaryProcessor Retry Mechanism (self-heal extension)
- **P2-2:** Grading Delay Alert (new function)
- **P2-3:** Live Export Staleness Alert (enhanced existing function)

### Deployment Commands
```bash
# Grading Delay Alert
gcloud functions deploy grading-delay-alert \
    --gen2 --runtime python311 --region us-west2 \
    --source orchestration/cloud_functions/grading_alert \
    --entry-point check_grading_status \
    --trigger-http --allow-unauthenticated

gcloud scheduler jobs create http grading-delay-alert-job \
    --schedule "0 10 * * *" --time-zone "America/New_York" \
    --uri https://FUNCTION_URL --http-method GET --location us-west2

# Self-Heal (redeploy with Phase 3 checking)
gcloud functions deploy self-heal-check \
    --gen2 --runtime python311 --region us-west2 \
    --source orchestration/cloud_functions/self_heal \
    --entry-point self_heal_check \
    --trigger-http --allow-unauthenticated

# Live Freshness Monitor (redeploy with 4-hour alert)
gcloud functions deploy live-freshness-monitor \
    --gen2 --runtime python311 --region us-west2 \
    --source orchestration/cloud_functions/live_freshness_monitor \
    --entry-point main \
    --trigger-http --allow-unauthenticated \
    --set-env-vars SLACK_WEBHOOK_URL=<webhook>
```

---

## Session 13B Progress (Jan 12, 2026 - Data Quality Investigation)

### Root Cause Identified: player_lookup Normalization Mismatch

**Problem:** 6,000+ predictions have `line_value = 20` (default) instead of real prop lines, causing:
- OVER picks appear to have 51.6% win rate instead of actual **73.1%**
- Props exist in database but JOIN fails for suffix players

**Root Cause:** Inconsistent name normalization across processors:
| Processor | Suffixes | `"Michael Porter Jr."` â†’ |
|-----------|----------|--------------------------|
| ESPN Rosters | REMOVES | `michaelporter` |
| BettingPros Props | REMOVES | `michaelporter` |
| Odds API Props | **KEEPS** | `michaelporterjr` |

**Affected Players:** Michael Porter Jr., Gary Payton II, Tim Hardaway Jr., Jaren Jackson Jr., Kelly Oubre Jr., Marcus Morris Sr., and all other suffix players.

### Completed This Session
- [x] **P1-DATA-3:** Fix ESPN roster processor - now uses shared `normalize_name()`
- [x] **P1-DATA-4:** Fix BettingPros props processor - now uses shared `normalize_name()`
- [x] **Backfill Script Created:** `bin/patches/patch_player_lookup_normalization.sql`

### Pending (Requires Deploy + Execution)
- [ ] Deploy code changes to production
- [ ] **P2-DATA-3:** Run backfill SQL for ESPN rosters
- [ ] **P2-DATA-4:** Run backfill SQL for BettingPros props
- [ ] Regenerate `upcoming_player_game_context` for affected dates

### Files Modified
- `data_processors/raw/espn/espn_team_roster_processor.py` - Uses shared normalizer
- `data_processors/raw/bettingpros/bettingpros_player_props_processor.py` - Uses shared normalizer
- `bin/patches/patch_player_lookup_normalization.sql` - Backfill script (NEW)

### Documentation Created
- `docs/08-projects/.../data-quality/2026-01-12-PLAYER-LOOKUP-NORMALIZATION-MISMATCH.md`

---

## Session 11 Progress (Jan 11, 2026 - Continuation)

### Completed This Session
- [x] **Props Backfill Complete** - 46 dates loaded (Nov 14 - Dec 31), 28,078 records
- [x] **NO_LINE Alert Deployed** - Cloud Function `prediction-health-alert-00002` active
- [x] **CatBoost V8 Added to Backfill** - Now generates 6 prediction systems
- [x] **Predictions Regenerated** - Nov 19 - Dec 19 with Vegas lines (6,244 predictions)
- [x] **Grading Re-run** - 35,166 predictions graded with correct prop data
- [x] **catboost_v8 Historical Verified** - 121,215 predictions since Nov 2021 (live production)
- [x] **Gap Recovery Plan Created** - Full analysis of Oct 22 - Nov 13 gap

### Key Findings
- **catboost_v8 performance: 74-82% win rate** (better than 70% claim)
- **Production worker already generates catboost_v8** - backfill wasn't needed historically
- **Oct 22 - Nov 13 gap requires historical Odds API** - separate recovery effort

### Files Modified/Created
- `backfill_jobs/prediction/player_prop_predictions_backfill.py` - Added CatBoost V8
- `docs/.../2026-01-11-GAP-RECOVERY-PLAN.md` - Recovery plan for remaining gap

---

## Session 10 Progress (Jan 11, 2026)

### Completed This Session
- [x] **YESTERDAY Bug Fix Deployed** - Phase 3 analytics now handles YESTERDAY date correctly
- [x] **Pick Subset Tracking Schema** - Multi-model aware performance tracking tables created
- [x] **NO_LINE Alerting Added** - prediction_health_alert now detects prop data gaps
- [x] **Prop Gap Detection Enabled** - Added odds_api_player_props to processor_config.py
- [x] **Props Backfill Script Created** - scripts/backfill_odds_api_props.py
- [x] **Prop Freshness Monitor Created** - tools/monitoring/check_prop_freshness.py
- [x] **Props Backfill Complete** - âœ… Done in Session 11

### Key Files Modified
- `data_processors/analytics/main_analytics_service.py` - YESTERDAY fix
- `orchestration/cloud_functions/prediction_health_alert/main.py` - NO_LINE alerting
- `monitoring/processors/gap_detection/config/processor_config.py` - Enabled prop monitoring
- `schemas/bigquery/predictions/*.sql` - Subset tracking tables

### Documentation Created
- `docs/08-projects/current/pipeline-reliability-improvements/2026-01-11-PROP-DATA-GAP-INCIDENT.md`
- `docs/08-projects/current/ml-model-v8-deployment/PERFORMANCE-ANALYSIS-GUIDE.md`

---

## Overview

This document consolidates ALL pipeline improvement work discovered from:
- Original session analysis (23 items)
- 5-agent deep exploration (75+ new items)
- Existing documentation review
- Handoff document patterns
- **Jan 2026 Prop Data Gap Incident** (7 new items)

---

## Priority Legend

| Priority | Meaning | Response Time | Count |
|----------|---------|---------------|-------|
| P0 | Critical - security/reliability risk | Immediate | 3 |
| P1 | High - significant impact | This week | 15 |
| P2 | Medium - important improvement | Next 2 weeks | 15 |
| P3 | Low - nice to have | When time permits | 10+ |

---

## P0 - CRITICAL (Fix Immediately)

### P0-SEC-1: No Authentication on Coordinator Endpoints
**Status:** âœ… COMPLETE (verified Jan 12, 2026)
**File:** `predictions/coordinator/coordinator.py` lines 89-215
**Risk:** ~~CRITICAL~~ MITIGATED

**Solution Applied:**
- `require_api_key` decorator implemented at line 89
- Applied to `/start` (line 215), `/complete` (line 412), `/status` (line 492)
- Checks X-API-Key header or Bearer token for GCP service accounts
- COORDINATOR_API_KEY loaded from Secret Manager

**Verification:**
```bash
# This should return 401 Unauthorized
curl -X POST https://prediction-coordinator-url/start
```

---

### P0-ORCH-1: Cleanup Processor is Non-Functional
**Status:** âœ… COMPLETE (verified Jan 12, 2026)
**File:** `orchestration/cleanup_processor.py` lines 255-290
**Risk:** ~~HIGH~~ MITIGATED

**Solution Applied:**
- Actual Pub/Sub publishing implemented at line 287
- Creates recovery messages with proper format
- Publishes to Phase 1 complete topic
- Includes correlation tracking (original_execution_id, recovery_reason)

**Code now:**
```python
# Line 287 - Actual publishing, not logging!
future = self.publisher.publish(self.topic_path, data=message_data)
```

---

### P0-ORCH-2: Phase 4â†’5 Has No Timeout
**Status:** âœ… COMPLETE (Session 17, Jan 12, 2026)
**Files:**
- `orchestration/cloud_functions/phase4_to_phase5/main.py` - HTTP error handling fixed
- `orchestration/cloud_functions/phase4_timeout_check/main.py` - NEW scheduled function
**Risk:** ~~HIGH~~ MITIGATED

**Solution Applied:**
1. **Fix 1:** HTTP error handling changed from `raise` to graceful error handling (line 505)
2. **Fix 2:** New `phase4-timeout-check` Cloud Function deployed
   - Runs every 30 minutes via Cloud Scheduler
   - Checks for stale Phase 4 states (>4 hours)
   - Force-triggers Phase 5 if timeout exceeded
   - Sends Slack alert with missing processors

**Verification:**
```bash
# Check function exists
gcloud functions describe phase4-timeout-check --region=us-west2

# Test manually
curl https://phase4-timeout-check-f7p3g7f6ya-wl.a.run.app
```

---

## P1 - HIGH PRIORITY (This Week)

### P1-PERF-1: Add BigQuery Query Timeouts
**Status:** âœ… COMPLETE (verified Jan 12, 2026 - Session 18)
**File:** `predictions/worker/data_loaders.py` line 31

**Solution Applied:**
- `QUERY_TIMEOUT_SECONDS = 30` defined at module level
- Applied to all 5 query locations (lines 305, 489, 596, 699, 829)
- Prevents workers from hanging indefinitely on slow queries

**Verification:**
```bash
grep -n "QUERY_TIMEOUT_SECONDS" predictions/worker/data_loaders.py
# Shows: 31:QUERY_TIMEOUT_SECONDS = 30
```

---

### P1-PERF-2: Batch Load Historical Games (50x Performance)
**Status:** ðŸ”´ Not Started
**Files:** `worker.py:571`, `data_loaders.py:435-559`

**Problem:** Worker calls `load_historical_games()` per-player sequentially

**Available Solution:**
```python
# data_loaders.py already has batch method!
def load_historical_games_batch(
    self,
    player_lookups: List[str],
    game_date: date
) -> Dict[str, List[Dict]]:
    # Load all players in ONE query - 50x faster
```

**Fix:** Coordinator pre-loads all historical games, passes to workers

---

### P1-PERF-3: Fix MERGE FLOAT64 Partitioning Error
**Status:** ðŸ”´ Not Started
**File:** `predictions/worker/batch_staging_writer.py` lines 302-319

**Error:**
```
Invalid MERGE query: 400 Partitioning by expressions of type FLOAT64 is not allowed
```

**Root Cause:**
- ROW_NUMBER uses `CAST(current_points_line AS STRING)` âœ“
- ON clause uses raw `current_points_line` without CAST âœ—

**Fix:**
```python
# Line 319 - change:
ON T.current_points_line = S.current_points_line
# To:
ON CAST(T.current_points_line AS STRING) = CAST(S.current_points_line AS STRING)
```

---

### P1-ORCH-3: Add Phase 5â†’6 Data Validation
**Status:** ðŸ”´ Not Started
**File:** `orchestration/cloud_functions/phase5_to_phase6/main.py` lines 106-136

**Problem:** Only checks status='success', doesn't verify actual data exists

**Fix:**
```python
# Add before triggering Phase 6:
row_count = bq_client.query(f"""
    SELECT COUNT(*) FROM nba_predictions.player_prop_predictions
    WHERE game_date = '{game_date}' AND is_active = TRUE
""").result().to_dataframe().iloc[0, 0]

if row_count < MIN_PREDICTIONS_THRESHOLD:
    logger.error(f"Insufficient predictions: {row_count}")
    return
```

---

### P1-ORCH-4: Add Health Checks to Cloud Functions
**Status:** ðŸ”´ Not Started
**Files:** All `orchestration/cloud_functions/*/main.py`

**Problem:** No `/health` endpoints - can't detect function failures

**Fix:** Add to each function:
```python
@functions_framework.http
def health(request):
    # Check Firestore
    db.collection('_health_check').document('probe').get()
    # Check BigQuery
    bq_client.list_datasets(max_results=1)
    return jsonify({'status': 'healthy'}), 200
```

---

### P1-DATA-1: Fix Prediction Duplicates
**Status:** âœ… COMPLETE (verified Jan 12, 2026 - Session 18)
**File:** `predictions/worker/batch_staging_writer.py` lines 281-384

**Solution Applied:**
Two-phase write pattern eliminates duplicates:
1. **Phase 1 (Workers):** Write to staging tables with `WRITE_APPEND` (no DML limits)
2. **Phase 2 (Coordinator):** MERGE with ROW_NUMBER deduplication

**Key Implementation (lines 289-332):**
- Merge key: `game_id + player_lookup + system_id + current_points_line`
- Uses `COALESCE(..., -1)` for NULL-safe comparison
- ROW_NUMBER keeps most recent prediction per key
- Comment in code: "P1-DATA-1 FIX"

**Verification:**
```bash
grep -n "P1-DATA-1" predictions/worker/batch_staging_writer.py
# Shows: 291, 313 - Fix comments in MERGE query builder
```

---

### P1-DATA-2: Update Circuit Breaker Hardcodes
**Status:** ðŸŸ¡ Partial (2/7 done)
**Files:** 5 processor files still need updates

| File | Line | Status |
|------|------|--------|
| `player_composite_factors_processor.py` | 1066 | ðŸ”´ TODO |
| `player_shot_zone_analysis_processor.py` | 810 | ðŸ”´ TODO |
| `player_daily_cache_processor.py` | 1172, 1237 | ðŸ”´ TODO |
| `team_defense_zone_analysis_processor.py` | 607 | ðŸ”´ TODO |
| `upcoming_team_game_context_processor.py` | 1036 | ðŸ”´ TODO |

---

### P1-DATA-3: Fix ESPN Roster player_lookup Normalization
**Status:** âœ… DEPLOYED (Jan 12, 2026 - Session 18)
**File:** `data_processors/raw/espn/espn_team_roster_processor.py` line 166
**Deployment:** `nba-phase2-raw-processors-00086-xgg`

**Solution Applied:**
- Uses shared `normalize_name()` which KEEPS suffixes
- Line 166: `player_lookup = normalize_name(full_name)`
- Comment documents the fix and links to normalization mismatch doc

**Impact:** Suffix players (Jr., Sr., II, III) now match Odds API props correctly

---

### P1-DATA-4: Fix BettingPros Props player_lookup Normalization
**Status:** âœ… DEPLOYED (Jan 12, 2026 - Session 18)
**File:** `data_processors/raw/bettingpros/bettingpros_player_props_processor.py` line 306
**Deployment:** `nba-phase2-raw-processors-00086-xgg`

**Solution Applied:**
- Uses shared `normalize_name()` which KEEPS suffixes
- Line 306: `player_lookup = normalize_name(player_name)`
- Old method marked DEPRECATED with documentation

**Impact:** Same as P1-DATA-3 - consistent normalization across all processors

---

### P1-MON-1: Implement DLQ Monitoring
**Status:** ðŸŸ¡ Partial (Jan 12, 2026 - Session 18)

**What's Working:**
- `dlq-monitor` Cloud Function: ACTIVE
- `dlq-monitor-job` Scheduler: Created, runs every 15 min
- Function checks 6 DLQ subscriptions and logs results

**What's Missing:**
- Slack webhook not configured (`slack-webhook-default` secret)
- 5/6 DLQ subscriptions don't exist (only `prediction-request-dlq-sub`)

**Finding:** 83 failed predictions in `prediction-request-dlq-sub` from Jan 4-10

**To Complete:**
```bash
# 1. Create Slack webhook secret
gcloud secrets create slack-webhook-default --project=nba-props-platform
echo -n "https://hooks.slack.com/YOUR/WEBHOOK" | \
  gcloud secrets versions add slack-webhook-default --data-file=-

# 2. Create missing DLQ subscriptions (optional)
# Run: bin/infrastructure/create_phase2_phase3_topics.sh
```

---

### P1-MON-2: Add Pub/Sub Publish Retries
**Status:** ðŸ”´ Not Started
**File:** `predictions/coordinator/coordinator.py` lines 421-424

**Problem:**
```python
future = publisher.publish(topic_path, data=message_bytes)
future.result(timeout=5.0)  # Single attempt, no retry
```

**Fix:** Implement exponential backoff with max 3 retries

---

### P1-PROC-1: PlayerDailyCacheProcessor Slowdown
**Status:** ðŸ”´ Not Started
**File:** `data_processors/precompute/player_daily_cache/player_daily_cache_processor.py`

**Problem:** +39.7% slower (48.21s vs 34.52s baseline)

**Investigation:**
- [ ] Profile 4 extract queries
- [ ] Check records_processed correlation
- [ ] Review completeness checking overhead

---

### P1-PROC-2: Missing Individual Player Retry
**Status:** ðŸ”´ Not Started
**File:** `predictions/worker/worker.py` lines 314-334

**Problem:**
```python
if not predictions:
    logger.warning(f"No predictions generated for {player_lookup}")
    return ('', 204)  # Pub/Sub won't retry!
```

**Fix:** Return 500 on transient errors to trigger Pub/Sub retry

---

## P2 - MEDIUM PRIORITY (Next 2 Weeks)

### P2-PERF-1: Add Feature Caching
**File:** `predictions/worker/worker.py` lines 88-96
- Features for same game_date queried 450 times
- Add cache with 5-minute TTL

### P2-PERF-2: Fix Validation Threshold Inconsistency
**Files:** `data_loaders.py:705` vs `worker.py:496`
- Default is 70.0 but worker uses 50.0
- Standardize to single config value

### P2-ORCH-1: Implement DLQ for Cloud Functions
**Files:** All orchestration cloud functions
- Send permanently failed messages to DLQ
- Distinguish transient vs permanent errors

### P2-ORCH-2: Add Firestore Document Cleanup
**File:** `orchestration/cloud_functions/transition_monitor/main.py`
- Documents accumulate forever
- Implement 30-day TTL or cleanup function

### P2-MON-1: End-to-End Latency Tracking
- Create `nba_monitoring.pipeline_execution_log` table
- Track game_end â†’ grading latency
- Define 6-hour SLA

### P2-MON-2: Add Firestore Health to Dashboard
- Integrate `monitoring/firestore_health_check.py`
- Show connectivity, latency, stuck processors

### P2-MON-3: Add Slowdown Alerts to Dashboard
- Integrate `monitoring/processor_slowdown_detector.py`
- Show processors > 2x baseline

### P2-MON-4: Per-System Prediction Success Rates
**File:** `predictions/worker/execution_logger.py` lines 88-91
- Systems tracked but not exposed as metrics
- Add per-system success rate dashboard

### P2-SEC-1: Fix API Key Timing Attack
**File:** `services/admin_dashboard/main.py` lines 116-132
- Simple string comparison vulnerable
- Use `secrets.compare_digest()`

### P2-SEC-2: Add Rate Limiting
**File:** `services/admin_dashboard/main.py`
- No rate limiting anywhere
- Add Flask-Limiter

### P2-DATA-1: Automatic Backfill Trigger
- Create `boxscore-backfill-trigger` Cloud Function
- Listen to gaps-detected topic
- Trigger scraper for missing dates

### P2-DATA-2: Extend Self-Heal to Phase 2
**File:** `orchestration/cloud_functions/self_heal/main.py`
- Currently only checks predictions
- Add boxscore, game context, prop lines checks

### P2-DATA-3: Backfill ESPN Rosters player_lookup
**Status:** ðŸ”´ Not Started (depends on P1-DATA-3)
**Table:** `nba_raw.espn_team_rosters`

**Problem:** Historical data has old normalization (suffixes removed).

**Fix:**
```sql
-- Recompute player_lookup with correct normalization
UPDATE `nba-props-platform.nba_raw.espn_team_rosters`
SET player_lookup = LOWER(REGEXP_REPLACE(
    NORMALIZE(player_full_name, NFD),
    r'[^a-z0-9]', ''
))
WHERE player_lookup IS NOT NULL;
```

**Verification:**
```sql
SELECT player_full_name, player_lookup
FROM `nba-props-platform.nba_raw.espn_team_rosters`
WHERE player_full_name LIKE '%Jr.%' OR player_full_name LIKE '%II%'
LIMIT 10;
```

### P2-DATA-4: Backfill BettingPros Props player_lookup
**Status:** ðŸ”´ Not Started (depends on P1-DATA-4)
**Table:** `nba_raw.bettingpros_player_points_props`

**Problem:** Historical data has old normalization (suffixes removed).

**Fix:**
```sql
-- Recompute player_lookup with correct normalization
UPDATE `nba-props-platform.nba_raw.bettingpros_player_points_props`
SET player_lookup = LOWER(REGEXP_REPLACE(
    NORMALIZE(player_name, NFD),
    r'[^a-z0-9]', ''
))
WHERE player_lookup IS NOT NULL;
```

### P2-PROC-1: Generic Exception Handling
**File:** `data_processors/analytics/analytics_base.py`
- 34 try-except blocks with generic catching
- Implement custom exception hierarchy

### P2-PROC-2: Missing Dependency Validation
**File:** `data_processors/precompute/precompute_base.py` lines 232-243
- Only checks existence, not data quality
- Add row count validation

### P2-PROC-3: Incomplete Fallback Logic
**File:** `shared/processors/patterns/fallback_source_mixin.py`
- No retry on transient failures
- Add exponential backoff

---

## P3 - LOWER PRIORITY (When Time Permits)

### Performance
- P3-PERF-1: Migrate coordinator to Firestore (multi-instance)
- P3-PERF-2: No batch staging cleanup strategy

### Orchestration
- P3-ORCH-1: Add SLA monitoring for predictions
- P3-ORCH-2: Fix batch ID format for Firestore
- P3-ORCH-3: Phase 2â†’3 vestigial trigger (wasted Pub/Sub)

### Monitoring
- P3-MON-1: Add metrics/Prometheus endpoint
- P3-MON-2: Admin audit trail to database
- P3-MON-3: Quality score distribution metrics
- P3-MON-4: Recommendation distribution (OVER/UNDER/PASS)

### Data
- P3-DATA-1: Multi-source scraper fallback
- P3-DATA-2: BigQuery fallback for Firestore

---

## Summary Statistics

| Category | P0 | P1 | P2 | P3 | Total |
|----------|-----|-----|-----|-----|-------|
| Security | ~~1~~ 0 âœ… | 0 | 2 | 0 | 3 |
| Performance | 0 | 3 | 2 | 2 | 7 |
| Orchestration | ~~2~~ 0 âœ… | 2 | 2 | 3 | 9 |
| Data Reliability | 0 | 2 | 2 | 2 | 6 |
| Monitoring | 0 | 2 | 4 | 4 | 10 |
| Processors | 0 | 2 | 3 | 0 | 5 |
| **TOTAL** | ~~**3**~~ **0** | **11** | **15** | **11** | **40** |

**P0 Status:** All 3 P0 items verified COMPLETE as of Jan 12, 2026 (Session 18)

*Note: Full count is 75+ when including all sub-items from agent findings*

---

## Quick Reference: Files with Most Issues

| File | Issues | Priority Range |
|------|--------|----------------|
| `predictions/coordinator/coordinator.py` | 8 | P0-P2 |
| `predictions/worker/worker.py` | 6 | P1-P2 |
| `orchestration/cleanup_processor.py` | 2 | P0 |
| `data_processors/analytics/analytics_base.py` | 5 | P1-P2 |
| `services/admin_dashboard/main.py` | 6 | P1-P3 |
| `predictions/worker/data_loaders.py` | 4 | P1-P2 |
| `batch_staging_writer.py` | 2 | P1 |
| `phase4_to_phase5/main.py` | 2 | P0-P1 |

---

## Related Documentation

| Document | Purpose |
|----------|---------|
| `AGENT-FINDINGS-DEC30.md` | Detailed agent findings |
| `2025-12-30-EVENING-SESSION-HANDOFF.md` | Session handoff |
| `plans/PIPELINE-ROBUSTNESS-PLAN.md` | Original robustness plan |
| `plans/ORCHESTRATION-IMPROVEMENTS.md` | Monitoring plan |
| `docs/07-monitoring/observability-gaps.md` | Observability analysis |

---

*Last Updated: January 12, 2026 (Session 18 - P0 verification)*
