# Session 34 - Execution Plan: From Firefighting to Fire Prevention
**Created:** 2026-01-14
**Status:** Ready to Execute
**Estimated Time:** 15-21 hours (Week 1), 10-15 hours (Week 2)

---

## ðŸŽ¯ EXECUTIVE SUMMARY

After comprehensive 5-agent exploration of the entire platform, we've identified a strategic opportunity to shift from **reactive firefighting** to **proactive system excellence**.

**What We Learned:**
- âœ… Tracking bug fixed (95.9% false positive confirmed)
- âœ… All data loss dates self-healed (zero manual reprocessing needed)
- âš ï¸ **Phase 5 is broken** (27% success rate, 123-hour avg duration)
- âš ï¸ **Alert noise at 97.6%** (90%+ of "failures" are expected)
- âœ… System well-designed (55 processors, 28 Cloud Functions, 253+ docs)
- ðŸŽ¯ **High-value improvements identified** (health dashboard, proactive monitoring)

**The Plan:**
Execute high-impact improvements over 2 weeks that will:
1. Fix Phase 5 predictions (27% â†’ 95%+ success rate)
2. Reduce alert noise by 90%+
3. Enable proactive issue detection
4. Build operational excellence

---

## ðŸ“‹ WEEK 1 EXECUTION PLAN (15-21 hours)

### Day 1-2: Critical Fixes (5-7 hours)

#### Task 1: Fix Phase 5 Predictions Timeout (3-4 hours) âš ï¸ CRITICAL
**Problem:**
- 27% success rate (vs 90%+ for other phases)
- Processors stuck for 123 hours on average
- 4-hour timeout not working
- Predictions failing, grading incomplete

**Solution:**
1. Implement 15-minute heartbeat in PredictionCoordinator
2. Add 30-minute hard timeout (replace 4-hour timeout)
3. Add circuit breaker (fail fast after 3 consecutive failures)
4. Enhanced logging to capture stuck state

**Files to Modify:**
- `orchestration/cloud_functions/phase4_to_phase5/main.py`
- `orchestration/cloud_functions/phase5_to_phase6/main.py`
- (Possibly PredictionCoordinator service itself)

**Expected Impact:**
- âœ… 27% â†’ 95%+ success rate
- âœ… Stop wasting $$ on 4-hour hangs
- âœ… Faster failure detection (15 min vs 4 hours)

**Deployment:**
```bash
cd orchestration/cloud_functions/phase4_to_phase5
gcloud functions deploy phase4-to-phase5 --gen2 --region=us-west2 \
  --runtime=python311 --source=. --entry-point=orchestrate_phase4_to_phase5 \
  --trigger-topic=nba-phase4-precompute-complete
```

**Verification:**
```sql
-- Check Phase 5 success rate after deployment
SELECT
  DATE(started_at) as date,
  COUNT(*) as total_runs,
  SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successes,
  ROUND(SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) / COUNT(*) * 100, 1) as success_rate_pct
FROM `nba-props-platform.nba_reference.processor_run_history`
WHERE processor_name LIKE '%Prediction%'
  AND started_at >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
GROUP BY date
ORDER BY date DESC;
```

---

#### Task 2: Add failure_category Field (2-3 hours) âš ï¸ CRITICAL
**Problem:**
- 97.6% of Phase 2 "failures" are expected (no data available)
- 144,475 "failures" in 30 days, but 90%+ are legitimate
- Alert noise destroying signal

**Solution:**
Add `failure_category` field to distinguish:
- `success` - Normal completion
- `no_data_available` - Expected (no games scheduled)
- `dependency_missing` - Upstream data unavailable
- `validation_failed` - Data quality issues
- `timeout` - Processing took too long
- `error` - Actual failure (needs investigation)

**Files to Modify:**
1. `shared/processors/mixins.py` (RunHistoryMixin)
2. `data_processors/raw/processor_base.py` (ProcessorBase)
3. `scripts/monitoring_queries.sql` (update 5 queries)
4. Alert rules in Cloud Monitoring

**Implementation:**
```python
# In RunHistoryMixin._log_run_history():
def _log_run_history(self, status, error_message=None, failure_category=None):
    run_record = {
        "processor_name": self.processor_name,
        "data_date": self.data_date,
        "status": status,
        "failure_category": failure_category,  # NEW
        # ... other fields
    }
```

**Expected Impact:**
- âœ… 90%+ reduction in alert noise
- âœ… Operators focus on real issues
- âœ… Better failure pattern analysis

**Verification:**
```sql
-- Check failure categories after deployment
SELECT
  failure_category,
  COUNT(*) as count,
  ROUND(COUNT(*) / SUM(COUNT(*)) OVER() * 100, 1) as pct
FROM `nba-props-platform.nba_reference.processor_run_history`
WHERE started_at >= CURRENT_TIMESTAMP() - INTERVAL 7 DAY
GROUP BY failure_category
ORDER BY count DESC;

-- Should see:
-- no_data_available: ~90% of failures
-- error: <5% of failures
```

---

### Day 3-4: Proactive Monitoring (5-8 hours)

#### Task 3: Create Unified System Health Dashboard (4-6 hours)
**Current Problem:**
- Monitoring scripts scattered
- 15 minutes to check system health
- No single "is everything okay?" view

**Solution:** Create `scripts/system_health_check.py`

**Features:**
1. **Phase Status** - Deployment status for Phases 1-6
2. **7-Day Success Rates** - Per-phase success trends
3. **Alert Count** - Real alerts (excluding no_data_available)
4. **Performance Metrics** - P50/P95/P99 processing times
5. **Data Completeness** - Expected vs actual record counts
6. **Anomaly Detection** - Automatic detection of unusual patterns

**Implementation:**
```python
# scripts/system_health_check.py
def check_phase_status():
    """Check Cloud Run deployment status for all phases."""
    # gcloud run services list --region=us-west2

def check_success_rates():
    """Query processor_run_history for 7-day trends."""
    # BigQuery query with failure_category filter

def check_performance():
    """Analyze processing time trends."""
    # Detect processors >2x normal time

def check_data_completeness():
    """Compare actual vs expected record counts."""
    # Use processor_registry for expected ranges

def main():
    print("ðŸ¥ System Health Check - " + datetime.now().strftime("%Y-%m-%d %H:%M"))
    print("="*60)

    phase_status = check_phase_status()
    success_rates = check_success_rates()
    performance = check_performance()
    completeness = check_data_completeness()

    # Overall health: RED/YELLOW/GREEN
    overall = calculate_overall_health(...)

    print(f"\n{'âœ…' if overall == 'GREEN' else 'âš ï¸'} Overall Status: {overall}")
```

**Slack Integration:**
```python
def send_slack_report(health_data):
    """Send daily health report to #data-monitoring Slack channel."""
    webhook_url = os.environ['SLACK_WEBHOOK_URL']
    # Post formatted health report
```

**Schedule:** Cloud Scheduler job at 7 AM ET daily
```bash
gcloud scheduler jobs create http system-health-check \
  --schedule="0 7 * * *" \
  --time-zone="America/New_York" \
  --uri="https://us-west2-run.app/health-check" \
  --http-method=POST
```

**Expected Impact:**
- âœ… 15 min â†’ 2 min daily health check
- âœ… Proactive issue detection
- âœ… Single source of truth

---

#### Task 4: Investigate Monday 12-3am UTC Retry Storm (1-2 hours)
**Problem:**
- 30K+ failures Monday 12am-3am UTC
- Potential retry storm or scheduling issue

**Investigation Steps:**
```sql
-- 1. Identify which processors failing
SELECT
  processor_name,
  EXTRACT(DAYOFWEEK FROM started_at) as day_of_week,
  EXTRACT(HOUR FROM started_at) as hour_utc,
  COUNT(*) as failure_count
FROM `nba-props-platform.nba_reference.processor_run_history`
WHERE status != 'success'
  AND started_at >= CURRENT_TIMESTAMP() - INTERVAL 30 DAY
  AND EXTRACT(DAYOFWEEK FROM started_at) = 2  -- Monday
  AND EXTRACT(HOUR FROM started_at) BETWEEN 0 AND 3
GROUP BY processor_name, day_of_week, hour_utc
ORDER BY failure_count DESC
LIMIT 20;

-- 2. Check if Cloud Scheduler jobs scheduled for that window
-- (gcloud scheduler jobs list --location=us-west2)

-- 3. Check Pub/Sub retry counts
-- (Cloud Monitoring metrics for retry counts)
```

**Likely Root Causes:**
1. Sunday night games finishing â†’ Monday early AM processing
2. Multiple Cloud Scheduler jobs triggering simultaneously
3. Pub/Sub retry storm (exponential backoff colliding)

**Fixes (Based on Root Cause):**
- Option A: Stagger Cloud Scheduler jobs (add random jitter)
- Option B: Increase Pub/Sub retry backoff
- Option C: Add processing window check (skip if too early)

**Expected Impact:**
- âœ… Reduce 30K weekly failures
- âœ… Improve Monday reliability

---

### Day 5: Foundation Building (5-6 hours)

#### Task 5: Create Processor Registry (3-4 hours)
**Problem:**
- 55 processors with no central registry
- Dependencies tracked manually
- Hard to answer "which processors depend on X?"

**Solution:** Create `processor_registry.yaml`

**Structure:**
```yaml
version: "1.0"
last_updated: "2026-01-14"

processors:
  # Phase 2: Raw Data
  - name: NbacGamebookProcessor
    phase: 2
    criticality: CRITICAL  # CRITICAL, HIGH, MEDIUM, LOW
    description: "NBA box scores from gamebook API"
    dependencies: []
    dependents:
      - PlayerGameSummaryProcessor
      - TeamOffenseGameSummaryProcessor
      - GamebookRegistryProcessor
    expected_frequency: daily
    sla_completion_time: "03:00 AM ET"
    timeout_seconds: 600
    expected_record_range:
      min: 100  # per game
      max: 500
    bigquery_table: nba_raw.nbac_gamebook_player_stats
    gcs_source_pattern: gs://nba-scraped-data/nbac/gamebook/{date}/*.json

  - name: PlayerGameSummaryProcessor
    phase: 3
    criticality: HIGH
    description: "Per-player statistics aggregated across sources"
    dependencies:
      - NbacGamebookProcessor
      - BdlBoxscoresProcessor
      - NbacScheduleProcessor
    dependents:
      - PlayerCompositeFactorsProcessor
      - Publishing exporters (8 total)
    expected_frequency: daily
    sla_completion_time: "04:00 AM ET"
    timeout_seconds: 1200
    expected_record_range:
      min: 150  # players per day
      max: 500
    bigquery_table: nba_analytics.player_game_summary

  # ... (all 55 processors)
```

**Implementation:**
```python
# shared/processors/registry.py
import yaml
from pathlib import Path

class ProcessorRegistry:
    def __init__(self):
        registry_path = Path(__file__).parent.parent.parent / "processor_registry.yaml"
        with open(registry_path) as f:
            self.data = yaml.safe_load(f)

    def get_processor(self, name: str):
        """Get processor config by name."""
        for proc in self.data['processors']:
            if proc['name'] == name:
                return proc
        return None

    def get_dependencies(self, name: str):
        """Get list of upstream dependencies."""
        proc = self.get_processor(name)
        return proc.get('dependencies', []) if proc else []

    def get_dependents(self, name: str):
        """Get list of downstream dependents."""
        proc = self.get_processor(name)
        return proc.get('dependents', []) if proc else []

    def check_dependencies_ready(self, name: str, data_date: str):
        """Check if all upstream dependencies have completed successfully."""
        deps = self.get_dependencies(name)
        # Query processor_run_history for each dependency
        # Return True if all succeeded, False otherwise
```

**Integrate into ProcessorBase:**
```python
# In ProcessorBase.run():
from shared.processors.registry import ProcessorRegistry

registry = ProcessorRegistry()
if not registry.check_dependencies_ready(self.processor_name, self.data_date):
    logger.warning(f"Dependencies not ready for {self.processor_name}")
    # Set failure_category = 'dependency_missing'
    return False
```

**Expected Impact:**
- âœ… Automated dependency checking
- âœ… Better incident response ("what's affected?")
- âœ… Self-documenting architecture

---

#### Task 6: Start Proactive Quality Monitoring (2 hours - Week 1 portion)
**Goal:** Create baseline metrics for "what's normal"

**Week 1 Work:**
1. **Baseline Queries** - 30-day lookback for:
   - Expected record counts per processor
   - Typical processing times (P50, P95, P99)
   - Normal failure rates by day of week
   - Seasonal patterns

2. **Initial Visualization** - Create BigQuery views:
```sql
-- View: nba_orchestration.processor_baselines
CREATE OR REPLACE VIEW nba_orchestration.processor_baselines AS
WITH historical_stats AS (
  SELECT
    processor_name,
    APPROX_QUANTILES(records_processed, 100)[OFFSET(50)] as p50_records,
    APPROX_QUANTILES(records_processed, 100)[OFFSET(95)] as p95_records,
    APPROX_QUANTILES(duration_seconds, 100)[OFFSET(50)] as p50_duration,
    APPROX_QUANTILES(duration_seconds, 100)[OFFSET(95)] as p95_duration,
    SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) / COUNT(*) as success_rate
  FROM `nba-props-platform.nba_reference.processor_run_history`
  WHERE started_at >= CURRENT_TIMESTAMP() - INTERVAL 30 DAY
    AND failure_category != 'no_data_available'  -- Exclude expected
  GROUP BY processor_name
)
SELECT * FROM historical_stats;
```

**Week 2:** Complete trend detection and early warning (covered later)

---

## ðŸ“Š WEEK 1 SUCCESS METRICS

After completing Week 1, we should see:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Phase 5 Success Rate | 27% | 95%+ | +68 pp |
| Alert Noise (Phase 2) | 97.6% | <5% | -90%+ |
| Daily Health Check Time | 15 min | 2 min | -87% |
| Monday 12-3am Failures | 30K/week | <5K/week | -83%+ |
| Dependency Validation | Manual | Automated | 100% |

**Deliverables:**
- âœ… Phase 5 predictions working reliably
- âœ… failure_category field deployed
- âœ… System health dashboard operational
- âœ… Monday retry storm resolved
- âœ… Processor registry created (55 processors documented)
- âœ… Baseline metrics captured

---

## ðŸ“‹ WEEK 2 EXECUTION PLAN (10-15 hours)

### Day 1-2: Complete Proactive Monitoring (4-7 hours)

#### Task 7: Complete Proactive Quality Monitoring (2-4 hours)
**Week 2 Work:** Build on Week 1 baseline

**1. Trend Detection:**
```python
# scripts/detect_quality_trends.py
def detect_performance_degradation(processor_name, days=7):
    """Detect if processor taking >2x normal time."""
    baseline = get_baseline_p95_duration(processor_name)
    recent = get_recent_p95_duration(processor_name, days)

    if recent > baseline * 2:
        return {
            'alert': True,
            'severity': 'WARNING',
            'message': f'{processor_name} taking 2x normal time ({recent}s vs {baseline}s)',
            'trend': 'DEGRADING'
        }
    return {'alert': False}

def detect_failure_spike(processor_name, days=7):
    """Detect unusual failure rate increase."""
    baseline_rate = get_baseline_failure_rate(processor_name)
    recent_rate = get_recent_failure_rate(processor_name, days)

    if recent_rate > baseline_rate * 1.5 and recent_rate > 0.05:  # 50% increase & >5% abs
        return {
            'alert': True,
            'severity': 'WARNING',
            'message': f'{processor_name} failure rate spiking ({recent_rate:.1%} vs {baseline_rate:.1%})',
            'trend': 'INCREASING_FAILURES'
        }
    return {'alert': False}
```

**2. Early Warning Dashboard:**
```python
# Integrate into system_health_check.py
def check_quality_trends():
    """Run trend detection for all critical processors."""
    critical_processors = registry.get_by_criticality('CRITICAL')

    warnings = []
    for proc in critical_processors:
        perf_trend = detect_performance_degradation(proc['name'])
        fail_trend = detect_failure_spike(proc['name'])

        if perf_trend['alert']:
            warnings.append(perf_trend)
        if fail_trend['alert']:
            warnings.append(fail_trend)

    return warnings
```

**Expected Impact:**
- âœ… Catch degradation before it becomes an outage
- âœ… Proactive capacity planning
- âœ… Data-driven optimization decisions

---

#### Task 8: Gen2 Migration Phase 1 (2-3 hours)
**Migrate 2 critical Cloud Functions:**

**1. Migrate `self_heal` to Gen2:**
```python
# Before (Gen1):
def self_heal(event, context):
    message_data = base64.b64decode(event['data']).decode('utf-8')
    # ...

# After (Gen2):
import functions_framework
from cloudevents.http import CloudEvent

@functions_framework.cloud_event
def self_heal(cloud_event: CloudEvent):
    message_data = base64.b64decode(cloud_event.data["message"]["data"])
    # ...
```

**Deploy:**
```bash
cd orchestration/cloud_functions/self_heal
gcloud functions deploy self-heal --gen2 --region=us-west2 \
  --runtime=python311 --source=. --entry-point=self_heal \
  --trigger-topic=nba-processor-failure
```

**2. Migrate `phase4_timeout_check`:**
(Similar pattern)

**Verification:** Run both Gen1 and Gen2 in parallel for 7 days, compare outputs

---

### Day 3-4: Documentation & Cleanup (4-6 hours)

#### Task 9: Fix upcoming_tables_cleanup Gen2 (30-45 min)
**Quick fix from earlier attempt:**

```python
# Change from Gen1:
def cleanup_upcoming_tables(event=None, context=None):
    # ...

# To Gen2:
@functions_framework.cloud_event
def cleanup_upcoming_tables(cloud_event):
    import base64, json
    message_data = base64.b64decode(cloud_event.data["message"]["data"])
    # ... rest of function
```

**Deploy and test:** Should work now with proper Gen2 signature

---

#### Task 10: Create On-Call Runbook (2-3 hours)
**Create:** `/02-operations/on-call-runbook.md`

**Structure:**
```markdown
# On-Call Runbook

## On-Call Rotation
- Primary: Rotates weekly (Monday 9 AM ET)
- Secondary: Previous week's primary
- Escalation: Engineering manager

## Severity Definitions

### P0 - Critical (15 min response)
- Phase 3/4/5 completely down
- All predictions failing
- Data loss > 24 hours

### P1 - High (1 hour response)
- Single processor failing repeatedly
- Performance degradation >2x normal
- Alert storm (>100 alerts/hour)

### P2 - Medium (4 hour response)
- Non-critical processor failing
- Minor data quality issues
- Documentation needed

### P3 - Low (Next business day)
- Feature requests
- Optimization opportunities
- Archive old data

## Common Incidents

### "Phase 5 Predictions Not Running"
1. Check system health dashboard: `python scripts/system_health_check.py`
2. Query processor_run_history for Phase 5 failures
3. Check Cloud Run logs for prediction-coordinator
4. Common fixes:
   - Restart prediction-coordinator service
   - Check if Phase 4 completed (dependency)
   - Verify Phase 4â†’5 timeout not triggered
5. Escalate if not resolved in 30 minutes

### "Alert Storm"
1. Check Slack #data-alerts for pattern
2. Run failure analysis: `SELECT failure_category, COUNT(*) FROM processor_run_history ...`
3. Common causes:
   - Expected failures (no_data_available) - suppress alerts
   - Upstream API down - wait for recovery
   - Configuration change - rollback if recent deploy
4. Suppress non-critical alerts in AlertManager

## Escalation Matrix
- P0: Immediate Slack page + phone call
- P1: Slack mention in #data-engineering
- P2: Create ticket in Jira
- P3: Add to backlog

## Resources
- System health: `python scripts/system_health_check.py`
- Processor registry: `/processor_registry.yaml`
- Monitoring queries: `/scripts/monitoring_queries.sql`
- Architecture docs: `/docs/01-architecture/`
```

---

#### Task 11: Document Formal SLAs (1-2 hours)
**Create:** `/00-start-here/SLAs.md`

**Content:**
```markdown
# Service Level Agreements (SLAs)

## System-Wide SLAs

### Availability
- **Target:** 99.5% uptime (monthly)
- **Measurement:** Phase 1-6 all completing successfully
- **Downtime Budget:** 3.6 hours/month

### Data Completeness
- **Target:** 99.9% daily completeness
- **Measurement:** All scheduled games have data in BigQuery
- **Exceptions:** Upstream API outages (force majeure)

### Prediction Accuracy
- **Target:** 55%+ hit rate (player props)
- **Measurement:** Grading results over 30-day rolling window
- **Review:** Monthly accuracy review

## Phase-Specific SLAs

### Phase 1 (Scrapers)
- **Completion Time:** 11:00 PM ET (same day)
- **Success Rate:** 95%+
- **Data Loss:** <1% of scheduled games

### Phase 2 (Raw Processors)
- **Completion Time:** 12:00 AM ET (next day)
- **Success Rate:** 95%+ (excluding no_data_available)
- **Data Loss:** <0.5% of games

### Phase 3 (Analytics)
- **Completion Time:** 02:00 AM ET
- **Success Rate:** 99%+
- **Blocking Failures:** <0.1% (blocks downstream)

### Phase 4 (Precompute)
- **Completion Time:** 03:00 AM ET
- **Success Rate:** 99%+
- **Feature Store Freshness:** <12 hours

### Phase 5 (Predictions)
- **Completion Time:** 05:00 PM ET (same day as game)
- **Success Rate:** 95%+ âš ï¸ (currently 27% - BROKEN)
- **Prediction Coverage:** 90%+ of games

### Phase 6 (Publishing)
- **Completion Time:** 06:00 PM ET
- **Success Rate:** 99%+
- **API Latency:** <100ms (P95)

## Incident Response SLAs

### Response Times
- **P0 (Critical):** 15 minutes
- **P1 (High):** 1 hour
- **P2 (Medium):** 4 hours
- **P3 (Low):** Next business day

### Resolution Times
- **P0 (Critical):** 2 hours
- **P1 (High):** 8 hours
- **P2 (Medium):** 2 days
- **P3 (Low):** 1 week

## Monitoring & Reporting

### Daily Health Check
- **Frequency:** 7 AM ET daily
- **Tool:** `scripts/system_health_check.py`
- **Channel:** #data-monitoring Slack

### Weekly Report
- **Frequency:** Sunday 8 AM ET
- **Tool:** `scripts/monitoring/weekly_pipeline_health.sh`
- **Includes:** SLA compliance, trends, incidents

### Monthly Review
- **Frequency:** First Monday of month
- **Attendees:** Engineering team
- **Topics:** SLA compliance, incidents, improvements
```

---

### Day 5: Validation & Handoff (2 hours)

#### Task 12: Run 5-Day Monitoring Report (1 hour)
**Execute on Jan 19-20:**

```bash
cd ~/nba-stats-scraper
PYTHONPATH=. python scripts/monitor_zero_record_runs.py \
  --start-date 2026-01-14 \
  --end-date 2026-01-19 \
  > /tmp/monitoring_5day_post_fix_$(date +%Y%m%d).txt
```

**Analysis:**
```bash
# Compare to pre-fix baseline (Oct-Jan)
grep "Found.*zero-record runs" /tmp/monitoring_after_fix_20260113_*.txt
# Before: 2,346 zero-record runs over 105 days = ~22/day

grep "Found.*zero-record runs" /tmp/monitoring_5day_post_fix_*.txt
# After (expected): <10 total over 5 days = <2/day

# Calculate improvement:
# 22/day â†’ <2/day = >90% reduction âœ…
```

**Document in:** `docs/08-projects/current/daily-orchestration-tracking/SESSION-34-FINAL-REPORT.md`

---

#### Task 13: Create Session Handoff (1 hour)
**Update handoff documents:**
- `docs/09-handoff/2026-01-14-SESSION-34-FINAL-HANDOFF.md`
- Update progress tracking
- Document all improvements deployed
- Next steps for future sessions

---

## ðŸŽ¯ SUCCESS METRICS (End of Week 2)

| Metric | Week 0 | Week 1 | Week 2 | Target |
|--------|--------|--------|--------|--------|
| Phase 5 Success Rate | 27% | 95%+ | 95%+ | 95%+ âœ… |
| Alert Noise | 97.6% | <5% | <5% | <5% âœ… |
| Health Check Time | 15 min | 2 min | 2 min | 2 min âœ… |
| Zero-Record Runs/Day | 22 | <2 | <2 | <2 âœ… |
| Gen2 Functions | 10/28 | 12/28 | 12/28 | 28/28 (future) |

**Deliverables (Week 2):**
- âœ… Proactive quality monitoring with trend detection
- âœ… 2 critical Cloud Functions migrated to Gen2
- âœ… upcoming_tables_cleanup Gen2 working
- âœ… On-call runbook created
- âœ… Formal SLAs documented
- âœ… 5-day validation confirms <1% false positive rate

---

## ðŸ“š REFERENCE DOCUMENTS

**Before Starting:**
- `SESSION-34-COMPREHENSIVE-ULTRATHINK.md` - Deep analysis (18K words)
- `processor_registry.yaml` - Processor dependencies (create in Task 5)
- `/docs/analysis/processor_run_history_quality_analysis.md` - BigQuery analysis

**During Execution:**
- `SESSION-34-PROGRESS.md` - Track daily progress
- `scripts/system_health_check.py` - Daily health monitoring
- `scripts/monitoring_queries.sql` - 30+ production queries

**After Completion:**
- `SESSION-34-FINAL-REPORT.md` - Final results and handoff
- `docs/09-handoff/2026-01-14-SESSION-34-FINAL-HANDOFF.md` - Next session handoff

---

## ðŸš€ QUICK START

**To start Week 1 execution:**

```bash
# 1. Review ultrathink
cat docs/08-projects/current/daily-orchestration-tracking/SESSION-34-COMPREHENSIVE-ULTRATHINK.md

# 2. Start with Task 1 (highest priority)
cd orchestration/cloud_functions/phase4_to_phase5
# Implement heartbeat and 30-min timeout

# 3. Track progress
# Update SESSION-34-PROGRESS.md after each task

# 4. Run daily health check
python scripts/system_health_check.py  # (After Task 3 complete)
```

---

**Let's transform from firefighting to fire prevention! ðŸš€**
