# Session 97: Grading Duplicate Fix Deployment & System Robustness Analysis

**Date:** 2026-01-18
**Session:** 97
**Status:** âœ… Deployed to Production
**Related Sessions:** 94 (Root Cause), 95 (Implementation)

---

## Executive Summary

Session 97 successfully deployed the Session 94/95 grading duplicate fix to production and conducted a comprehensive analysis of system-wide race condition vulnerabilities and robustness improvements.

**Key Achievements:**
- âœ… Deployed distributed locking fix to prevent 190K duplicate rows in `prediction_accuracy`
- âœ… Identified 3 additional vulnerable processors needing similar fixes
- âœ… Designed comprehensive logging, alerting, and robustness improvements
- âœ… Created defense-in-depth architecture documentation

---

## Part 1: Production Deployment

### 1.1 What Was Deployed

**Commit:** `f47e32b` - feat(grading): Implement distributed locking to prevent duplicate rows in prediction_accuracy table

**Files Changed (7 total):**
1. `bin/deploy/deploy_grading_function.sh` - Added predictions directory to deployment
2. `bin/validation/daily_data_quality_check.sh` - Added Check 8 for duplicate monitoring
3. `data_processors/grading/prediction_accuracy/prediction_accuracy_processor.py` - Core fix with locking + validation
4. `orchestration/cloud_functions/grading/main.py` - Duplicate alerting integration
5. `orchestration/cloud_functions/grading/requirements.txt` - Added Firestore, Secret Manager, requests
6. `predictions/worker/batch_staging_writer.py` - Updated to new lock API
7. `predictions/worker/distributed_lock.py` - Refactored ConsolidationLock â†’ DistributedLock

**Deployment Details:**
- **Cloud Function:** `phase5b-grading` (revision: `phase5b-grading-00012-puw`)
- **Deployed:** 2026-01-18 04:10 UTC
- **Status:** ACTIVE
- **Test Run:** 2026-01-18 04:11 UTC (manual trigger for 2026-01-16)

### 1.2 Three-Layer Defense Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 1: PREVENTION (Distributed Lock)          â”‚
â”‚ - Firestore-based lock prevents concurrent ops  â”‚
â”‚ - 5-minute timeout + auto-cleanup               â”‚
â”‚ - 60 retry attempts over 5 minutes              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ If fails (rare)
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 2: DETECTION (Post-Grading Validation)    â”‚
â”‚ - Check for duplicates after write              â”‚
â”‚ - Log detailed duplicate info                   â”‚
â”‚ - Audit trail for investigation                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ If detected
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LAYER 3: ALERTING (Slack + Monitoring)          â”‚
â”‚ - Real-time Slack notification                  â”‚
â”‚ - Cloud Monitoring dashboard                    â”‚
â”‚ - Lock contention metrics                       â”‚
â”‚ - Daily validation checks (Check 8)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 Verification Steps

**Immediate (Completed):**
- âœ… Function deployed successfully
- âœ… Test run executed without errors
- â³ Detailed lock logs verification (pending)

**Pending:**
- Monitor next scheduled run (2026-01-18 11:00 UTC / 6 AM ET)
- Verify lock acquisition appears in Cloud Logs
- Confirm no new duplicates created
- Run data cleanup to remove existing 190K duplicates

---

## Part 2: System-Wide Vulnerability Analysis

### 2.1 Critical Finding: Multiple Vulnerable DELETE + INSERT Patterns

**Root Pattern Identified:**
```python
# VULNERABLE PATTERN (found in 3 locations)
def write_data(self, data, identifier):
    # STEP 1: DELETE existing records
    delete_job = self.bq_client.query(f"DELETE FROM {table} WHERE key = {identifier}")
    delete_job.result()

    # âš ï¸ RACE CONDITION: Another process can run DELETE here!

    # STEP 2: INSERT new records
    load_job = self.bq_client.load_table_from_json(data, table)
    load_job.result()

    # âš ï¸ RACE CONDITION: If both processes INSERT, we get duplicates!
```

### 2.2 Vulnerable Processors Requiring Immediate Attention

| Processor | Location | Risk Level | Business Impact | Fix Effort |
|-----------|----------|------------|-----------------|-----------|
| **PredictionAccuracyProcessor** | `data_processors/grading/prediction_accuracy/` | âœ… FIXED | 190K duplicates (38% of data) | 0h |
| **SystemDailyPerformanceProcessor** | `data_processors/grading/system_daily_performance/` | ðŸ”´ CRITICAL | Daily aggregations corrupted | 2h |
| **PerformanceSummaryProcessor** | `data_processors/grading/performance_summary/` | ðŸ”´ CRITICAL | Summary metrics corrupted | 2h |
| **AnalyticsBase (DELETE handling)** | `data_processors/analytics/analytics_base.py:1700-2060` | ðŸŸ  HIGH | Analytics tables may have duplicates | 3h |
| **BatchConsolidator** | `predictions/worker/batch_staging_writer.py` | âœ… FIXED | Session 92 fixed this | 0h |

**Files Requiring Urgent Review:**
- `/home/naji/code/nba-stats-scraper/data_processors/grading/system_daily_performance/system_daily_performance_processor.py:285-310`
- `/home/naji/code/nba-stats-scraper/data_processors/grading/performance_summary/performance_summary_processor.py`
- `/home/naji/code/nba-stats-scraper/data_processors/analytics/analytics_base.py:1700-2060`

---

## Part 3: Logging & Visibility Gaps

### 3.1 Current State Assessment

**What IS Being Logged:**
- âœ… Lock acquisition attempts
- âœ… Lock release events
- âœ… Duplicate detection details

**Critical Gaps Identified:**

#### Gap 1: Cloud Function Logs Don't Surface Lock Details

**Problem:** Deployed code logs lock events at INFO level, but Cloud Functions logs don't show them by default.

**Root Cause:**
- Cloud Logging filters by severity level
- INFO logs exist but aren't visible in default dashboard views
- No structured logging format for lock operations

**Recommendation:** Add structured logging
```python
# File: orchestration/cloud_functions/grading/main.py

import json
import logging

class StructuredLogger:
    def log_lock_event(self, event_type, game_date, details):
        """Log lock events in structured format for Cloud Logging"""
        entry = {
            'event_type': event_type,  # 'lock_acquired', 'lock_waited', 'lock_timeout'
            'game_date': game_date,
            'timestamp': datetime.utcnow().isoformat(),
            'severity': 'INFO',
            **details
        }
        logging.info(json.dumps(entry))
```

#### Gap 2: Lock Contention Metrics Not Tracked

**Problem:** No visibility into how long processes wait for locks or if locks are being held too long.

**Recommendation:** Add lock metrics to Cloud Monitoring

```python
# File: predictions/worker/distributed_lock.py

def _record_lock_metric(self, metric_type: str, value: int, labels: Dict):
    """Record lock metrics to Cloud Monitoring"""
    from google.cloud import monitoring_v3

    client = monitoring_v3.MetricServiceClient()
    project_name = f"projects/{self.project_id}"

    series = monitoring_v3.TimeSeries()
    series.metric.type = f'custom.googleapis.com/grading/{metric_type}'
    series.resource.type = 'global'

    for key, val in labels.items():
        series.resource.labels[key] = val

    now = time.time()
    point = monitoring_v3.Point({
        "interval": {"seconds": int(now), "nanos": int((now - int(now)) * 10 ** 9)},
        "value": {"int64_value": value}
    })
    series.points = [point]

    client.create_time_series(name=project_name, time_series=[series])

# Usage in acquire():
self._record_lock_metric('lock_wait_time', elapsed_ms, {
    'lock_type': self.lock_type,
    'game_date': game_date
})
```

#### Gap 3: Duplicate Audit Trail Not Persisted

**Problem:** Duplicate detection logs to stdout, not easily queryable or retained long-term.

**Recommendation:** Create dedicated audit table

```sql
-- Create audit table for duplicate detection
CREATE TABLE `nba-props-platform.nba_predictions.grading_duplicate_audit` (
    detected_at TIMESTAMP NOT NULL,
    game_date DATE NOT NULL,
    duplicate_count INT64,
    processor STRING,
    player_lookup STRING,
    system_id STRING,
    line_value FLOAT64,
    duplicate_occurrence_count INT64,
    investigation_url STRING
)
PARTITION BY DATE(detected_at)
CLUSTER BY game_date, processor;
```

---

## Part 4: Alerting Improvements

### 4.1 Current Alerts (Deployed)

| Alert Type | Implementation | Trigger | Status |
|------------|---------------|---------|--------|
| Duplicate Detection | Slack webhook | duplicates > 0 | âœ… Deployed |
| Daily Validation | Check 8 in validation script | duplicates in last 7 days | âœ… Deployed |

### 4.2 Critical Alert Gaps

#### Gap 1: Lock Contention Not Alerted

**Scenario:** Process A holds lock for 5 minutes, Process B waits 5 minutes â†’ No alert until timeout.

**Recommended Alerts:**

```yaml
# Cloud Monitoring Alert Policy

- Display Name: "Grading Lock Contention"
  Condition: custom.googleapis.com/grading/lock_wait_time > 60 seconds
  Notification Channel: #ops-alerts (Slack)
  Documentation: |
    Grading lock held for >1 minute. Possible causes:
    1. Slow BigQuery operations
    2. Large data volume for game_date
    3. Deadlock or stuck process

    Investigation:
    1. Check Firestore collection: grading_locks
    2. View current lock holder in lock document
    3. Check Cloud Function logs for slow queries
    4. Consider force-releasing lock if >10 minutes

- Display Name: "Grading Lock Acquisition Failed"
  Condition: custom.googleapis.com/grading/lock_timeout >= 1 event in 1 hour
  Severity: CRITICAL
  Documentation: |
    Grading lock acquisition timed out - operation may have run WITHOUT lock!
    HIGH RISK of duplicates being created.

    Immediate Actions:
    1. Check prediction_accuracy for new duplicates
    2. Review Cloud Function logs for errors
    3. Verify Firestore connectivity
    4. Manual duplicate check required
```

#### Gap 2: Silent Lock Fallback Not Alerted

**Problem:** Code proceeds without lock on timeout, but no alert is sent.

**Fix Required:**

```python
# File: orchestration/cloud_functions/grading/main.py

def send_lock_failure_alert(target_date: str, reason: str):
    """Send critical alert when lock acquisition fails"""
    message = {
        "text": f"ðŸ”´ *CRITICAL: Grading Lock Acquisition Failed*\n\n"
                f"*Date:* {target_date}\n"
                f"*Reason:* {reason}\n"
                f"*Action:* Grading proceeding WITHOUT distributed lock (HIGH RISK)\n"
                f"*Risk:* Concurrent operations may create duplicates\n"
                f"*Investigation:*\n"
                f"  1. Check Firestore collection: grading_locks\n"
                f"  2. Check for stuck locks in Firestore console\n"
                f"  3. Check Cloud Function logs for errors\n"
                f"*Next Step:* Manual verification required after grading completes"
    }
    # Send to Slack webhook
```

#### Gap 3: Duplicate Rate Trending Not Monitored

**Recommendation:** Weekly duplicate trend analysis

```python
def check_duplicate_trend(days_back: int = 7):
    """Monitor if duplicate rate is increasing"""
    query = f"""
    SELECT
        game_date,
        COUNT(*) as total_records,
        COUNT(DISTINCT CONCAT(player_lookup, '|', game_id, '|', system_id, '|', CAST(line_value AS STRING))) as unique_keys,
        ROUND(1 - (COUNT(DISTINCT CONCAT(...)) / COUNT(*)), 3) as duplication_rate
    FROM `nba-props-platform.nba_predictions.prediction_accuracy`
    WHERE game_date >= DATE_SUB(CURRENT_DATE(), INTERVAL {days_back} DAY)
    GROUP BY game_date
    ORDER BY game_date DESC
    """

    # Alert if duplication rate increasing >10% week-over-week
```

---

## Part 5: Robustness Improvements

### 5.1 Circuit Breaker Pattern (Not Yet Implemented)

**Purpose:** Prevent cascading failures when grading repeatedly fails.

**Implementation:**

```python
# File: data_processors/grading/prediction_accuracy/prediction_accuracy_processor.py

from enum import Enum
from datetime import timedelta

class CircuitBreakerState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"          # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing recovery

class GradingCircuitBreaker:
    """Circuit breaker for grading operations"""

    def __init__(self, failure_threshold: int = 3, timeout_minutes: int = 5):
        self.failure_threshold = failure_threshold
        self.timeout_minutes = timeout_minutes
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.last_failure_time = None
        self.open_time = None

    def record_failure(self):
        """Track failure and open circuit if threshold exceeded"""
        self.failure_count += 1
        self.last_failure_time = datetime.utcnow()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitBreakerState.OPEN
            self.open_time = datetime.utcnow()
            self.send_circuit_open_alert()

    def can_execute(self) -> bool:
        """Check if grading should execute"""
        if self.state == CircuitBreakerState.CLOSED:
            return True

        if self.state == CircuitBreakerState.OPEN:
            if datetime.utcnow() - self.open_time > timedelta(minutes=self.timeout_minutes):
                self.state = CircuitBreakerState.HALF_OPEN
                return True
            return False

        return True  # HALF_OPEN state
```

### 5.2 Stuck Lock Detection & Auto-Cleanup

**Problem:** Crashed process holds lock until 5-minute timeout expires, blocking all other operations.

**Solution:**

```python
# File: predictions/worker/distributed_lock.py

def check_for_stuck_locks(self) -> List[Dict]:
    """Identify locks held longer than expected"""
    collection = self.db.collection(self.collection_name)
    docs = collection.stream()

    stuck_locks = []
    now = datetime.utcnow()

    for doc in docs:
        data = doc.to_dict()
        acquired_at = data['acquired_at'].replace(tzinfo=None)
        held_minutes = (now - acquired_at).total_seconds() / 60

        # If held > 10 minutes (double timeout), likely stuck
        if held_minutes > 10:
            stuck_locks.append({
                'lock_key': doc.id,
                'operation_id': data['operation_id'],
                'held_for_minutes': held_minutes,
                'acquired_at': acquired_at
            })

            # Alert and auto-cleanup
            logger.error(f"Stuck lock detected: {doc.id} held for {held_minutes:.0f} min")
            send_stuck_lock_alert(doc.id, held_minutes)

            # Force release if > 15 minutes (3x timeout)
            if held_minutes > 15:
                self.force_release(data['lock_key'].split('_')[1])
                logger.warning(f"Auto-released stuck lock: {doc.id}")

    return stuck_locks
```

### 5.3 Automated Duplicate Remediation

**Purpose:** Automatically fix small duplicate issues without manual intervention.

**Implementation:**

```python
def auto_remediate_duplicates(self, game_date: date) -> bool:
    """Automatically deduplicate if count is small"""
    duplicate_count = self._check_for_duplicates(game_date)

    # Only auto-fix if:
    # 1. Duplicates detected
    # 2. Count is small (<1000, not massive corruption)
    # 3. No lock currently held for this date
    if 0 < duplicate_count < 1000:
        logger.warning(f"Auto-remediating {duplicate_count} duplicates for {game_date}")

        # Deduplication query: keep earliest record
        dedup_query = f"""
        CREATE OR REPLACE TABLE `{self.accuracy_table}_temp` AS
        WITH ranked_records AS (
            SELECT
                *,
                ROW_NUMBER() OVER (
                    PARTITION BY player_lookup, game_id, system_id, line_value
                    ORDER BY graded_at ASC
                ) as rn
            FROM `{self.accuracy_table}`
            WHERE game_date = '{game_date}'
        )
        SELECT * EXCEPT(rn)
        FROM ranked_records
        WHERE rn = 1;

        DELETE FROM `{self.accuracy_table}` WHERE game_date = '{game_date}';

        INSERT INTO `{self.accuracy_table}`
        SELECT * FROM `{self.accuracy_table}_temp`;

        DROP TABLE `{self.accuracy_table}_temp`;
        """

        self.bq_client.query(dedup_query).result()

        # Verify remediation
        final_count = self._check_for_duplicates(game_date)
        if final_count == 0:
            send_alert(f"âœ… Auto-remediation: Removed {duplicate_count} duplicates for {game_date}")
            return True
        else:
            send_alert(f"âŒ Auto-remediation failed: {final_count} duplicates remain")
            return False

    return False
```

---

## Part 6: Immediate Action Items

### Phase 1: This Week (Critical - Prevent Future Duplicates)

| Priority | Action | File(s) | Impact | Effort |
|----------|--------|---------|--------|--------|
| ðŸ”´ P0 | Apply distributed lock to SystemDailyPerformanceProcessor | `data_processors/grading/system_daily_performance/system_daily_performance_processor.py` | Prevent daily aggregation duplicates | 2h |
| ðŸ”´ P0 | Apply distributed lock to PerformanceSummaryProcessor | `data_processors/grading/performance_summary/` | Prevent summary metric duplicates | 2h |
| ðŸŸ  P1 | Add structured logging for lock events | `orchestration/cloud_functions/grading/main.py` | Enable lock visibility in Cloud Logging | 1h |
| ðŸŸ  P1 | Add lock failure alerts | `orchestration/cloud_functions/grading/main.py` | Catch silent lock failures | 1h |

### Phase 2: Next Week (High Priority - Visibility)

| Priority | Action | File(s) | Impact | Effort |
|----------|--------|---------|--------|--------|
| ðŸŸ¡ P2 | Create duplicate audit table | BigQuery + `prediction_accuracy_processor.py` | Enable duplicate investigation | 1h |
| ðŸŸ¡ P2 | Implement lock contention metrics | `predictions/worker/distributed_lock.py` | Track lock performance | 2h |
| ðŸŸ¡ P2 | Create Cloud Monitoring dashboard | GCP Console | Visualize lock + duplicate metrics | 3h |
| ðŸŸ¡ P2 | Add duplicate trend monitoring | `bin/validation/` | Detect increasing duplication | 1h |

### Phase 3: Ongoing (Medium Priority - Robustness)

| Priority | Action | File(s) | Impact | Effort |
|----------|--------|---------|--------|--------|
| ðŸŸ¢ P3 | Implement stuck lock detection | `predictions/worker/distributed_lock.py` | Auto-cleanup after 10 min | 2h |
| ðŸŸ¢ P3 | Add circuit breaker pattern | `data_processors/grading/` | Prevent cascading failures | 2h |
| ðŸŸ¢ P3 | Implement auto-remediation | `prediction_accuracy_processor.py` | Auto-fix small duplicate issues | 2h |
| ðŸŸ¢ P3 | Audit all DELETE + INSERT patterns | All data processors | System-wide safety audit | 4h |

---

## Part 7: Monitoring & Verification

### 7.1 Daily Checks (Automated via Check 8)

```bash
# bin/validation/daily_data_quality_check.sh runs daily
# Check 8: Grading accuracy table duplicates (last 7 days)

# Manual verification:
bq query --use_legacy_sql=false '
SELECT
  game_date,
  COUNT(*) as total_rows,
  COUNT(DISTINCT CONCAT(player_lookup, "|", game_id, "|", system_id, "|", CAST(line_value AS STRING))) as unique,
  COUNT(*) - COUNT(DISTINCT ...) as duplicates
FROM `nba-props-platform.nba_predictions.prediction_accuracy`
WHERE game_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
GROUP BY game_date
ORDER BY game_date DESC'
```

### 7.2 Lock Health Checks

```bash
# Check Firestore for active locks
gcloud firestore export gs://nba-props-locks/exports/$(date +%Y%m%d) \
  --collection-ids=grading_locks

# View Cloud Function logs for lock events
gcloud functions logs read phase5b-grading \
  --region us-west2 \
  --limit 100 | grep -i "lock"
```

### 7.3 Weekly Review Checklist

- [ ] Check for any new duplicates in prediction_accuracy (last 7 days)
- [ ] Review Cloud Monitoring dashboard for lock contention
- [ ] Check for stuck locks in Firestore (held > 10 minutes)
- [ ] Review Slack alerts for any duplicate detection
- [ ] Verify daily validation Check 8 passing
- [ ] Review grading function error rate (should be < 0.1%)

---

## Part 8: Lessons Learned

### 8.1 Key Insights

1. **DELETE + INSERT â‰  Atomic**: Two separate BigQuery jobs with no transaction isolation
2. **Idempotency â‰  Concurrency Safety**: Idempotent operations can still have race conditions
3. **Source Data Cleanliness â‰  Output Cleanliness**: Grading pipeline can create duplicates from clean source
4. **Defense in Depth is Critical**: Single lock might fail â†’ need validation + alerting layers
5. **Logging Visibility Gap**: Cloud Functions default logging level hides INFO logs

### 8.2 System-Wide Pattern to Apply

```python
# BEST PRACTICE TEMPLATE for all DELETE + INSERT operations:

def write_data_safely(self, data, identifier):
    """Safe write pattern with 3-layer defense"""

    # LAYER 1: Distributed Lock (Prevent)
    lock = DistributedLock(project_id=PROJECT_ID, lock_type="processor_name")

    try:
        with lock.acquire(game_date=identifier, operation_id=f"write_{identifier}"):
            # LAYER 2: Atomic Write + Validation (Detect)
            rows_written = self._write_with_validation(data, identifier)

            duplicate_count = self._check_for_duplicates(identifier)

            # LAYER 3: Alert (Respond)
            if duplicate_count > 0:
                send_duplicate_alert(identifier, duplicate_count)

            return rows_written

    except LockAcquisitionError as e:
        # Graceful degradation with alert
        send_lock_failure_alert(identifier, str(e))
        logger.warning(f"Proceeding WITHOUT lock (HIGH RISK)")
        return self._write_with_validation(data, identifier)
```

---

## Part 9: Related Documentation

**Session Documentation:**
- Session 94 Root Cause Analysis: `SESSION-94-ROOT-CAUSE-ANALYSIS.md`
- Session 94 Fix Design: `SESSION-94-FIX-DESIGN.md`
- Session 95 Implementation: `/home/naji/code/nba-stats-scraper/SESSION-94-95-COMPLETE.md`

**Operational Guides:**
- Performance Analysis: `PERFORMANCE-ANALYSIS-GUIDE.md`
- Model Performance Tracking: `HOW-TO-ADD-MODEL-PERFORMANCE-TRACKING.md`

**Handoff Documents:**
- Session 96â†’97 Handoff: `/home/naji/code/nba-stats-scraper/docs/09-handoff/SESSION-96-TO-97-HANDOFF.md`

---

## Part 10: Next Session Recommendations

**For Session 98:**

1. **If implementing Phase 1 critical fixes:**
   - Apply distributed lock to SystemDailyPerformanceProcessor
   - Apply distributed lock to PerformanceSummaryProcessor
   - Add structured logging and lock failure alerts
   - Verify no duplicates in system_daily_performance table

2. **If focusing on data cleanup:**
   - Remove 190K existing duplicates from prediction_accuracy
   - Clean 50+ orphaned staging tables from Nov 19
   - Remove 117 historical duplicate predictions (Jan 4, 11)
   - Investigate 175 ungraded predictions

3. **If implementing visibility improvements:**
   - Create duplicate audit table
   - Add lock contention metrics to Cloud Monitoring
   - Build Cloud Monitoring dashboard
   - Set up lock contention alerts

**Decision Criteria:**
- **Choose Phase 1** if you want to prevent future duplicates in other tables NOW
- **Choose data cleanup** if you want to clean existing corruption first
- **Choose visibility** if you want better monitoring before making more changes

---

**Document Status:** Active
**Last Updated:** 2026-01-18
**Maintainer:** AI Session Documentation
**Review Cycle:** After each grading system change
