# Tracking Bug Root Cause Analysis
**Date:** 2026-01-14
**Session:** 32
**Status:** ‚úÖ ROOT CAUSE IDENTIFIED

---

## üéØ Executive Summary

**Problem:** `processor_run_history.records_processed` shows 0 even when data successfully loads to BigQuery

**Root Cause:** Custom `save_data()` implementations don't set `self.stats["rows_inserted"]`

**Impact:**
- 2,344 "zero-record runs" reported (mostly false positives)
- Cannot distinguish real data loss from tracking bugs
- Monitoring scripts unreliable

**Fix:** One-line addition to processors that override `save_data()`: set `self.stats["rows_inserted"]`

---

## üî¨ Investigation Steps

### Step 1: Validated Data Exists (Morning)

Cross-referenced `processor_run_history` with actual BigQuery tables:

| Date | run_history shows | BigQuery has | Result |
|------|-------------------|--------------|--------|
| Jan 11 | 0 records | 348 players, 10 games | üêõ Tracking Bug |
| Jan 10 | 0 records | 211 players, 6 games | üêõ Tracking Bug |
| Jan 9 | 0 records | 347 players, 10 games | üêõ Tracking Bug |
| Jan 8 | 0 records | 106 players, 3 games | üêõ Tracking Bug |

**Conclusion:** Data loading works fine. Tracking is broken.

---

### Step 2: Traced Code Flow

**ProcessorBase.run()** (line 250-255):
```python
# After successful processing
self.record_run_complete(
    status='success',
    records_processed=self.stats.get('rows_inserted', 0),  # ‚Üê Gets from stats
    records_created=self.stats.get('rows_inserted', 0),
    summary=self.stats
)
```

**ProcessorBase.save_data()** (line 1147):
```python
# Default implementation
load_job.result(timeout=60)
self.stats["rows_inserted"] = len(rows)  # ‚Üê Sets the value
logger.info(f"‚úÖ Successfully batch loaded {len(rows)} rows")
```

**Key Finding:** `ProcessorBase` correctly sets `self.stats["rows_inserted"]` when data loads.

---

### Step 3: Found Custom save_data() Override

**BdlBoxscoresProcessor.save_data()** (line 772-776):
```python
def save_data(self) -> None:
    rows = self.transformed_data
    # ... complex MERGE_UPDATE logic ...

    # üêõ BUG: Returns dict but doesn't set self.stats!
    return {
        'rows_processed': len(rows) if not errors else 0,
        'errors': errors,
        'streaming_conflicts': streaming_conflicts
    }
```

**Problem Identified:**
1. ‚ùå Overrides `save_data()` completely
2. ‚ùå Doesn't call `super().save_data()`
3. ‚ùå Doesn't set `self.stats["rows_inserted"]`
4. ‚ùå Returns dict (which is ignored by ProcessorBase)

---

## üìê Architecture Analysis

### Inheritance Chain

```
ProcessorBase (sets self.stats["rows_inserted"])
    ‚Üë
    |
SmartIdempotencyMixin (calls super().save_data())
    ‚Üë
    |
BdlBoxscoresProcessor (overrides save_data(), breaks the chain)
```

### Expected Behavior

**ProcessorBase Documentation** (line 1065):
```python
def save_data(self):
    """
    Override for custom save strategies:
    - MERGE operations (upserts)
    - DELETE operations
    - Query-based transformations

    If overriding, set self.stats["rows_inserted"] for tracking.
    """
```

**Documented requirement:** Processors that override `save_data()` MUST set `self.stats["rows_inserted"]`.

---

## üêõ The Bug Pattern

### Processors Affected

Found 20+ processors with custom `save_data()` methods:

**BallDontLie Processors:**
- bdl_boxscores_processor.py ‚ùå
- bdl_active_players_processor.py ?
- bdl_standings_processor.py ?
- bdl_live_boxscores_processor.py ?
- bdl_injuries_processor.py ?

**Other Processors:**
- basketball_ref/br_roster_processor.py ?
- bettingpros/bettingpros_player_props_processor.py ?
- bigdataball/bigdataball_pbp_processor.py ?
- MLB processors (8+) ?

Each needs validation to check if they set `self.stats["rows_inserted"]`.

---

## ‚úÖ The Fix

### For BdlBoxscoresProcessor

**File:** `data_processors/raw/balldontlie/bdl_boxscores_processor.py`
**Line:** After line 729 (successful load)

**BEFORE (Buggy):**
```python
# Load using batch job
load_job = self.bq_client.load_table_from_json(
    rows,
    table_id,
    job_config=job_config
)

# Wait for completion
load_job.result(timeout=60)
logger.info(f"Successfully loaded {len(rows)} rows for {len(game_ids)} games")

# Log game ID format compliance
sample_game_ids = list(game_ids)[:3]
logger.info(f"Sample game IDs inserted (AWAY_HOME format): {sample_game_ids}")

# Send success notification
...

return {
    'rows_processed': len(rows) if not errors else 0,
    ...
}
```

**AFTER (Fixed):**
```python
# Load using batch job
load_job = self.bq_client.load_table_from_json(
    rows,
    table_id,
    job_config=job_config
)

# Wait for completion
load_job.result(timeout=60)

# ‚úÖ FIX: Set stats for run_history tracking
self.stats["rows_inserted"] = len(rows)

logger.info(f"Successfully loaded {len(rows)} rows for {len(game_ids)} games")

# Log game ID format compliance
sample_game_ids = list(game_ids)[:3]
logger.info(f"Sample game IDs inserted (AWAY_HOME format): {sample_game_ids}")

# Send success notification
...

return {
    'rows_processed': len(rows) if not errors else 0,
    ...
}
```

**Fix Location:** Insert one line after line 728:
```python
self.stats["rows_inserted"] = len(rows)
```

---

## üìä Expected Impact

### After Fix Deployment

**Before Fix:**
```sql
SELECT data_date, records_processed
FROM processor_run_history
WHERE processor_name = 'BdlBoxscoresProcessor'
  AND data_date = '2026-01-11';

-- Result: records_processed = 0
```

**After Fix:**
```sql
-- Same query
-- Result: records_processed = 348 (correct!)
```

### Monitoring Scripts

**Before:** Report 2,344 "zero-record runs" (mostly false positives)
**After:** Report only TRUE zero-record runs (legitimate empty responses)

### Data Loss Detection

**Before:** Cannot distinguish tracking bug from real data loss
**After:** Accurate tracking enables reliable data loss detection

---

## üîß Deployment Plan

### Phase 1: Fix Core Processors (Immediate)

1. **BdlBoxscoresProcessor** ‚úÖ Root cause identified
   - Add `self.stats["rows_inserted"] = len(rows)` after line 728
   - Test with manual run
   - Verify run_history shows correct count

2. **Audit Other BDL Processors**
   - Check: bdl_active_players_processor.py
   - Check: bdl_standings_processor.py
   - Check: bdl_live_boxscores_processor.py
   - Check: bdl_injuries_processor.py
   - Apply same fix if needed

### Phase 2: System-Wide Audit

3. **Audit All Custom save_data() Methods**
   - Found 20+ processors with custom implementations
   - For each: Check if sets `self.stats["rows_inserted"]`
   - Create checklist with pass/fail status

4. **Batch Fix Deployment**
   - Group fixes by service (Phase 2, Phase 3, Phase 4)
   - Deploy via Cloud Shell (WSL2 deployments hanging)
   - Verify each service after deployment

### Phase 3: Validation

5. **Re-run Monitoring**
   - Execute: `python scripts/monitor_zero_record_runs.py`
   - Compare before/after counts
   - Validate false positives eliminated

6. **Create True Data Loss Inventory**
   - Cross-reference remaining "zero-record" dates with BigQuery
   - Separate: Real Loss vs Legitimate Zero vs Still Buggy
   - Prioritize reprocessing for confirmed losses

---

## üéì Lessons Learned

### Design Issue

**Problem:** Implicit contract without enforcement
- Documentation says "set self.stats['rows_inserted']"
- But no validation or warning if not set
- Easy to miss when overriding methods

**Better Design:**
```python
def save_data(self):
    # ... save logic ...

    # Validate tracking was set
    if "rows_inserted" not in self.stats:
        logger.warning(
            f"{self.__class__.__name__}.save_data() didn't set "
            "self.stats['rows_inserted'] - tracking will show 0 records"
        )
        self.stats["rows_inserted"] = 0
```

### Code Review Gaps

**Issue:** No automated checks for contract compliance
- No linter rule
- No unit test validation
- No runtime validation

**Prevention:**
1. Add validation in ProcessorBase.run()
2. Add unit tests that verify stats are set
3. Document requirement more prominently

### Monitoring Gaps

**Issue:** Didn't notice false positives sooner
- Monitoring script trusted run_history blindly
- No cross-validation with actual data
- No automated validation job

**Prevention:** (Already documented in PREVENTION-STRATEGY.md)
1. Daily automated validation: run_history vs BigQuery
2. Alert on mismatches
3. Trend analysis for suspicious patterns

---

## üìã Action Items

### Immediate (Today)
- [ ] Fix BdlBoxscoresProcessor (add one line)
- [ ] Audit other BDL processors
- [ ] Deploy fixes to Phase 2 via Cloud Shell
- [ ] Test and verify fix works

### Short Term (This Week)
- [ ] Audit all 20+ processors with custom save_data()
- [ ] Create comprehensive fix PR
- [ ] Deploy to all services (Phase 2, 3, 4)
- [ ] Re-run monitoring script
- [ ] Validate false positives eliminated

### Long Term (This Month)
- [ ] Add runtime validation in ProcessorBase
- [ ] Add unit tests for stats tracking
- [ ] Implement automated validation job
- [ ] Update documentation with warnings

---

## üìù Related Documents

- **Validation Report:** `2026-01-14-DATA-LOSS-VALIDATION-REPORT.md`
- **Session Progress:** `2026-01-14-SESSION-PROGRESS.md`
- **Prevention Strategy:** `silent-failure-prevention/PREVENTION-STRATEGY.md`
- **Project Status:** `STATUS.md` (updated with tracking bug findings)

---

**Analysis Complete:** 2026-01-14 15:30 UTC
**Root Cause:** Confirmed - Missing `self.stats["rows_inserted"]` in custom save_data()
**Fix Ready:** Yes - One line addition per processor
**Confidence:** HIGH - Validated with code trace and data cross-reference
