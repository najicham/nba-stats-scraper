# Eval Pipeline Fix — Session 166

## Problem

The model experiment pipeline (`quick_retrain.py`) had two evaluation issues:

### 1. Line Source Mismatch
- **Experiments** used DraftKings-only lines from `odds_api_player_points_props`
- **Production** uses a multi-source cascade: DraftKings -> FanDuel -> BetMGM (OddsAPI -> BettingPros fallback at each level)
- **Impact:** Experiment hit rates were artificially lower than production because they compared against a narrower, sometimes stale set of lines

### 2. Tier Bias Hindsight
- `compute_tier_bias()` classified players by `actual_points` (what they scored)
- Should use `points_avg_season` (what the model knows pre-game)
- Session 124 proved this distorts tier analysis (Stars show artificial -10 bias when classified by actuals)

## Solution

### Production Line Eval (`load_eval_data_from_production()`)
- Queries `prediction_accuracy` table which stores the EXACT lines production used
- Default behavior via `--use-production-lines` flag
- Falls back to raw sportsbook lines if no production predictions exist for eval period
- Properly casts `FLOAT64` for numeric fields (BQ returns `decimal.Decimal`)

### Season Average Tier Bias
- `compute_tier_bias(preds, actuals, season_avgs=None)` now accepts season averages
- Extracts `points_avg_season` from feature matrix (V9 contract index 2)
- Backward compatible — falls back to actuals if season_avgs not provided

## Verification

Experiment 1 (Jan 8 model, Jan 9-15 eval) showed 83.16% edge 3+ hit rate with production lines vs ~63% with DraftKings-only. This matches the 71.2% production hit rate from CLAUDE.md (difference due to different eval date range).

## Related Discovery

Analysis revealed Feb 2-7 predictions were generated by the deprecated Feb 2 retrain model (`catboost_v9_feb_02_retrain.cbm`) which has systematic UNDER bias. Backfill triggered to regenerate with correct production model.
