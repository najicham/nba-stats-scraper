# v1.0 Implementation Plan - Event-Driven Pipeline with Change Detection

**Status:** üìã Ready to Implement
**Version:** 1.0 FINAL (Batch + Change Detection + Event-Driven + Hardened Backfill Scripts)
**Timeline:** 3-4 weeks, ~92 hours
**Created:** 2025-11-28 9:06 PM PST
**Last Updated:** 2025-11-28 11:00 PM PST (backfill review integrated)

---

## Executive Summary

**What We're Building:**
- ‚úÖ Unified message format across ALL phases
- ‚úÖ Event-driven orchestration (Phase 1‚Üí2‚Üí3‚Üí4‚Üí5)
- ‚úÖ **Change detection in v1.0** (process only changed entities)
- ‚úÖ Correlation ID tracing (scraper ‚Üí prediction)
- ‚úÖ Backfill mode support (skip downstream triggers)
- ‚úÖ Deduplication everywhere (idempotent operations)
- ‚úÖ Orchestrators for Phase 2‚Üí3, Phase 3‚Üí4, and Phase 4 internal
- ‚úÖ Test dataset support (easy testing without affecting production)
- ‚úÖ Comprehensive backfill scripts
- ‚úÖ Smart alert manager (rate limiting + backfill mode awareness)

**What We're NOT Building (v1.1+):**
- ‚ùå Real-time per-player endpoints (defer to v1.1)
- ‚ùå Prediction versioning/superseding (defer to v1.1)
- ‚ùå Line movement triggers (defer to v1.1)

**Why This Scope:**
- Change detection critical for sports betting (injury updates)
- Event-driven pipeline with efficiency optimizations
- Professional orchestration architecture
- Production-ready from day 1

---

## Key Architecture Decisions (FINAL)

### ‚úÖ Decision 1: YES Orchestrator for Phase 2‚Üí3

**Changed from original plan based on user feedback**

**Rationale:**
- Consistency: All phase transitions have orchestrators
- Cleaner: 1 trigger instead of 21 triggers to Phase 3
- Professional: Proper separation of concerns
- Marginal cost: One more Cloud Function (same pattern)

**Implementation:**
- Cloud Function tracks all 21 Phase 2 processors in Firestore
- Publishes ONE message to Phase 3 when all complete
- Document: `phase2_completion/{game_date}/{processor_name: {completed_at, correlation_id}}`

---

### ‚úÖ Decision 2: Change Detection IN v1.0

**Changed from original plan - moved from v1.1 to v1.0**

**Rationale:**
- Sports betting requires fresh injury data mid-day
- 99% efficiency gain worth the additional complexity
- Users need sub-5 minute updates when injury status changes
- Better to build it right from the start

**How It Works:**
```
2:00 PM - Injury scraper runs, gets ALL 450 players
  ‚Üì Phase 2 processes ALL 450 (MERGE upsert)
  ‚Üì Phase 2 detects ONLY LeBron changed (hash comparison)
  ‚Üì Publishes: {entities_changed: ["lebron-james"], is_full_batch: false}
  ‚Üì Phase 3 receives trigger
  ‚Üì Phase 3 processes ONLY LeBron (1 player instead of 450)
  ‚Üì Phase 4 processes ONLY LeBron
  ‚Üì Phase 5 generates predictions ONLY for LeBron
2:03 PM - LeBron's updated prediction ready!

Result: ‚úÖ 99% efficiency gain, 3-minute update
```

**Implementation Strategy:**
- **Simple hash-based change detection** (not complex queries)
- Compare hash of current row vs previous row in BigQuery
- Track changed entity IDs in message
- Processors process EITHER full batch OR changed entities only

---

### ‚úÖ Decision 3: Test Dataset Support

**Implementation:**
```python
# Environment variable controls dataset
DATASET_ID = os.environ.get('DATASET_ID', 'nba_analytics')

# Production datasets:
nba_raw, nba_analytics, nba_precompute, nba_predictions

# Test datasets:
nba_raw_test, nba_analytics_test, nba_precompute_test, nba_predictions_test

# Usage:
export DATASET_ID=nba_analytics_test
python -m data_processors.analytics.player_game_summary --date=2024-01-15
```

**Benefits:**
- Test without affecting production
- Same infrastructure (Pub/Sub, Cloud Functions)
- Easy to switch back and forth
- Parallel testing during development

---

### ‚úÖ Decision 4: Comprehensive Backfill Scripts

**Backfill Strategy:**

**Phase 1: Historical Seasons (Phases 1-2 only)**
```bash
# Backfill 4 seasons with skip_downstream_trigger
./bin/backfill/backfill_season.sh \
    --season=2023-24 \
    --phases=1,2 \
    --skip-downstream \
    --threads=12 \
    --parallel-scrapers=5
```

**Phase 2: Historical Analytics (Phases 3-4)**
```bash
# Manually trigger Phases 3-4 for all historical dates
./bin/backfill/trigger_phase3_batch.sh \
    --start-date=2020-10-01 \
    --end-date=2024-06-30 \
    --skip-downstream
```

**Phase 3: Current Season**
```bash
# Backfill current season up to yesterday
./bin/backfill/backfill_season.sh \
    --season=2024-25 \
    --start-date=2024-10-22 \
    --end-date=$(date -d "yesterday" +%Y-%m-%d) \
    --phases=1,2,3,4 \
    --skip-downstream
```

**Phase 4: Enable Daily Processing**
```bash
# The night before going live
./bin/backfill/verify_completeness.sh --all-seasons
# Enable Cloud Scheduler for overnight runs
```

---

### ‚úÖ Decision 5: Smart Alert Manager

**Alert Strategy:**

**Normal Processing:**
- Critical: Slack + Email (Phase 5 <90% coverage, pipeline failure)
- Error: Slack only (processor failures)
- Warning: Slack only (performance issues)
- Info: Logs only

**Backfill Mode:**
- Critical: Slack + Email (only truly critical)
- Error: Batched Slack summary every 30 min
- Warning: Batched Slack summary every hour
- Info: Logs only

**Rate Limiting:**
- Email: Max 10/hour, 50/day
- Slack: Max 30/hour, 200/day
- Exceeded: Batch into hourly summary

**Implementation:**
```python
# Set backfill mode
export BACKFILL_MODE=true

# AlertManager automatically batches during backfill
alert_manager.send_alert('error', 'Processor Failed', '...')
```

---

## Updated Unified Message Format

```python
{
    # ========== IDENTITY ==========
    "processor_name": str,
    "phase": str,
    "execution_id": str,
    "correlation_id": str,

    # ========== DATA REFERENCE ==========
    "game_date": str,
    "output_table": str,
    "output_dataset": str,

    # ========== STATUS ==========
    "status": str,  # "success" | "partial" | "no_data" | "failed"
    "record_count": int,
    "records_failed": int,

    # ========== TIMING ==========
    "timestamp": str,
    "duration_seconds": float,

    # ========== TRACING ==========
    "parent_processor": str,
    "trigger_source": str,
    "trigger_message_id": str,

    # ========== BACKFILL SUPPORT ==========
    "skip_downstream_trigger": bool,
    "backfill_mode": bool,
    "backfill_reason": str,

    # ========== CHANGE DETECTION (NEW IN v1.0) ==========
    "is_full_batch": bool,              # true = process all, false = process changed only
    "entities_total": int,              # Total entities in dataset (450 players)
    "entities_changed": List[str],      # IDs of changed entities ["lebron-james", "curry-stephen"]
    "entities_processed": int,          # How many processed (450 or 1)
    "change_detection_method": str,     # "hash_comparison" | "full_batch"

    # ========== ERROR INFO ==========
    "error_message": str | null,
    "error_type": str | null,

    # ========== PHASE-SPECIFIC ==========
    "metadata": dict
}
```

---

## Week-by-Week Implementation Plan (UPDATED)

### **Week 1: Foundation + Phase 1-2 + Orchestrators** (~18 hours)

#### Day 1: Unified Infrastructure (5 hours)

**Tasks:**
1. Create `shared/utils/unified_pubsub_publisher.py`
   - Validates unified message format
   - Handles backfill mode
   - Handles change detection fields

2. Create `shared/utils/change_detector.py`
   - Hash-based change detection
   - Compare current vs previous row hashes
   - Return list of changed entity IDs

3. Create `shared/utils/alert_manager.py`
   - Smart alerting with rate limiting
   - Backfill mode awareness
   - Batching logic

4. Add test dataset support to all base classes
   - Environment variable: DATASET_ID
   - Defaults to production datasets

**Deliverable:** Core infrastructure classes

---

#### Day 2: Update Phase 1 (4 hours)

**Tasks:**
1. Update `scrapers/utils/pubsub_utils.py`
   - Add unified format fields
   - Add correlation_id (= execution_id)
   - Add backfill mode support
   - Keep backward compatibility (dual format)

2. Test scraper publishing
   - Run test scraper
   - Verify message in Pub/Sub console
   - Verify both old and new fields

**Deliverable:** Phase 1 publishing unified messages

---

#### Day 3: Update Phase 2 + Change Detection (6 hours)

**Tasks:**
1. Update `data_processors/raw/processor_base.py`
   - Extract correlation_id from upstream
   - Add hash-based change detection
   - Track which entities changed
   - Publish unified format with entities_changed
   - Add backfill mode support

2. Implement change detection logic
   ```python
   def _detect_changed_entities(self, current_data):
       # Query previous run's data
       # Compare row hashes
       # Return list of changed entity IDs
       pass
   ```

3. Test Phase 1‚Üí2 with change detection
   - Run twice with same data ‚Üí no changes detected
   - Modify one row ‚Üí only that entity detected

**Deliverable:** Phase 2 with change detection working

---

#### Day 4: Build Phase 2‚Üí3 Orchestrator (3 hours)

**Tasks:**
1. Create `cloud_functions/phase2_to_phase3_orchestrator/main.py`
   - Listen to `nba-phase2-raw-complete`
   - Track 21 Phase 2 processors in Firestore
   - When ALL complete ‚Üí publish to `nba-phase3-trigger`

2. Create deployment script

3. Test orchestrator
   - Trigger Phase 2 processors
   - Verify Firestore state updates
   - Verify Phase 3 triggered once

**Deliverable:** Phase 2‚Üí3 orchestrator working

---

### **Week 2: Phase 3-4 + Change Detection** (~20 hours)

#### Day 5-6: Update Phase 3 with Change Detection (8 hours)

**Tasks:**
1. Update `data_processors/analytics/analytics_base.py`
   - Extract correlation_id
   - Extract entities_changed from message
   - Process EITHER full batch OR only changed entities
   - Implement change detection (compare analytics vs raw)
   - Publish unified format with entities_changed

2. Implement selective processing
   ```python
   def run(self, opts):
       entities_changed = opts.get('entities_changed', [])

       if entities_changed and not opts.get('is_full_batch', True):
           # Process only changed entities
           self._process_specific_entities(entities_changed)
       else:
           # Process all entities (normal batch)
           self._process_all_entities()
   ```

3. Test all 5 Phase 3 processors
   - Test full batch mode
   - Test incremental mode (1 player changed)
   - Verify efficiency gain

**Deliverable:** Phase 3 with change detection

---

#### Day 7: Build Phase 3‚Üí4 Orchestrator (6 hours)

**Tasks:**
1. Create `cloud_functions/phase3_to_phase4_orchestrator/main.py`
   - Track 5 Phase 3 processors in Firestore
   - Aggregate entities_changed from all processors
   - Publish to Phase 4 with combined list

2. Test orchestrator with change detection
   - Verify changed entities aggregated correctly
   - Verify Phase 4 receives correct list

**Deliverable:** Phase 3‚Üí4 orchestrator with change tracking

---

#### Day 8: Update Phase 4 with Change Detection (6 hours)

**Tasks:**
1. Update `data_processors/precompute/precompute_base.py`
   - Extract entities_changed
   - Process selectively or full batch
   - Propagate entities_changed

2. Build Phase 4 internal orchestrator
   - Track 5 processors across 3 levels
   - Aggregate entities_changed through cascade

3. Update ml_feature_store_v2
   - Add Phase 5 completion publishing
   - Include entities_changed in message

**Deliverable:** Phase 4 with change detection and orchestration

---

### **Week 3: Phase 5 + Backfill Scripts** (~25 hours)

#### Day 9-10: Phase 5 with Change Detection (10 hours)

**Tasks:**
1. Update `predictions/coordinator/coordinator.py`
   - Add `/trigger` endpoint (PRIMARY)
   - Extract entities_changed from Phase 4 message
   - Process ONLY changed players (not all 450)
   - Update `/start` endpoint (BACKUP with validation)
   - Add `/retry` endpoint (INCREMENTAL)
   - Add 7 helper functions

2. Update worker to handle incremental requests
   - Worker processes any player (no change needed)
   - Coordinator just publishes fewer messages

3. Test change detection end-to-end
   - Change 1 player in Phase 2
   - Verify only that player processed through to Phase 5

**Deliverable:** Phase 5 with selective player processing

---

#### Day 11: Create Backfill Scripts (6 hours)

**Tasks:**
1. Create `bin/backfill/backfill_historical_phases1_2.sh`
   - Orchestrates seasonal backfill
   - Parallel scraper execution (10 dates √ó 21 scrapers)
   - Supports season iteration

2. Create `bin/backfill/backfill_phase3.sh`
   - Batch trigger Phase 3 for date range
   - Parallel processing (20 dates)
   - Progress tracking

3. Create `bin/backfill/backfill_phase4.sh`
   - Batch trigger Phase 4 (10 dates parallel)
   - Heavy processing consideration

4. Create `bin/backfill/backfill_current_season.sh`
   - Sequential full pipeline validation
   - Current season (2024-25) through all phases

5. Create basic verification scripts
   - `verify_phase1_2.sh`
   - `verify_phase3.sh`
   - `verify_phase4.sh`
   - `verify_current_season.sh`

**Deliverable:** Comprehensive backfill tooling (base scripts)

---

#### Day 12: Harden Backfill Scripts (3 hours) - NEW FROM BACKFILL REVIEW

**Tasks:**
1. **Add critical fixes to all scripts** (1.5h)
   - Fix 85% threshold ‚Üí 100% requirement with failed tracking
   - Add resume capability (checkpoint file)
   - Add Ctrl+C signal handling (trap cleanup)
   - Fix empty date list check
   - Fix integer comparison bugs (use ${var:-0})
   - Add curl timeouts and error handling

2. **Create preflight check script** (0.5h)
   - Validate nba_schedule table populated
   - Check BigQuery datasets exist
   - Verify Cloud Run services healthy
   - Check Bash version (4.3+)
   - Validate prerequisites before execution

3. **Create tmux wrapper script** (0.5h)
   - Detached session for persistent execution
   - Survives SSH disconnects
   - Log file persistence
   - Easy attach/detach commands

4. **Add operational features** (0.5h)
   - Failed date/processor tracking log
   - Progress logging to file (tee)
   - Add rollback procedures to docs
   - Test with dry run

**Deliverable:** Production-hardened backfill scripts

---

#### Day 13: End-to-End Testing (6 hours)

**Tasks:**
1. Unit tests (>90% coverage)
   - UnifiedPubSubPublisher
   - ChangeDetector
   - AlertManager
   - Orchestrators

2. Integration tests
   - Full batch processing
   - Incremental processing (1 player changed)
   - Backfill mode
   - All orchestrators

3. Performance testing
   - Measure change detection overhead
   - Verify 99% efficiency gain for single-player changes

4. **Backfill script testing** - NEW
   - Test preflight check
   - Test resume capability (interrupt and resume)
   - Test failed tracking
   - Dry run with 1-2 dates

**Deliverable:** All tests passing, scripts validated

---

### **Week 4: Deploy + Monitor** (~12 hours)

#### Day 13: Deploy to Production (4 hours)

**Tasks:**
1. Deploy all services sequentially
   - Phase 1 updates
   - Phase 2 updates + orchestrator
   - Phase 3 updates + orchestrator
   - Phase 4 updates + orchestrator
   - Phase 5 updates

2. Create infrastructure
   - Pub/Sub topics (add 3 new for orchestrators)
   - Firestore collections (3 orchestrators)
   - Cloud Scheduler jobs

3. Verify deployment
   - All services healthy
   - All topics created
   - All orchestrators deployed

**Deliverable:** v1.0 deployed to production

---

#### Day 14: Execute Backfills (4 hours setup, ~5-7 days execution)

**Tasks:**
1. Backfill historical seasons (Phases 1-2)
   ```bash
   for season in 2020-21 2021-22 2022-23 2023-24; do
       ./bin/backfill/backfill_season.sh \
           --season=$season \
           --phases=1,2 \
           --skip-downstream \
           --threads=12
   done
   ```

2. Backfill historical Phases 3-4
   ```bash
   ./bin/backfill/trigger_phase3_batch.sh \
       --start-date=2020-10-01 \
       --end-date=2024-06-30
   ```

3. Backfill current season
   ```bash
   ./bin/backfill/backfill_season.sh \
       --season=2024-25 \
       --phases=1,2,3,4 \
       --skip-downstream
   ```

**Deliverable:** All historical data loaded

---

#### Day 15: Enable Daily Processing + Monitor (4 hours)

**Tasks:**
1. Verify completeness
   ```bash
   ./bin/backfill/verify_completeness.sh --all-seasons
   ```

2. Test one current date end-to-end
   - Trigger manually for today
   - Verify full pipeline Phase 1‚Üí5
   - Verify predictions generated

3. Enable Cloud Scheduler
   - Overnight scraper runs
   - Phase 5 backup schedulers

4. Monitor first automatic overnight run
   - Watch all phases complete
   - Check latency
   - Verify predictions ready by 7 AM PT

5. Create monitoring dashboards
   - Pipeline health
   - Change detection efficiency
   - Latency metrics

**Deliverable:** Daily processing enabled and monitored

---

## File Modifications Checklist (UPDATED)

### New Files to Create

**Core Infrastructure:**
- [ ] `shared/utils/unified_pubsub_publisher.py` - Unified publisher
- [ ] `shared/utils/change_detector.py` - Change detection logic
- [ ] `shared/utils/alert_manager.py` - Smart alerting

**Orchestrators:**
- [ ] `cloud_functions/phase2_to_phase3_orchestrator/main.py` - NEW
- [ ] `cloud_functions/phase2_to_phase3_orchestrator/deploy.sh` - NEW
- [ ] `cloud_functions/phase3_to_phase4_orchestrator/main.py`
- [ ] `cloud_functions/phase3_to_phase4_orchestrator/deploy.sh`
- [ ] `cloud_functions/phase4_orchestrator/main.py`
- [ ] `cloud_functions/phase4_orchestrator/deploy.sh`

**Backfill Scripts:**
- [ ] `bin/backfill/backfill_season.sh` - Seasonal backfill orchestrator
- [ ] `bin/backfill/trigger_phase3_batch.sh` - Batch Phase 3 trigger
- [ ] `bin/backfill/trigger_phase4_batch.sh` - Batch Phase 4 trigger
- [ ] `bin/backfill/verify_completeness.sh` - Completeness checker

**Tests:**
- [ ] `tests/shared/test_unified_pubsub_publisher.py`
- [ ] `tests/shared/test_change_detector.py`
- [ ] `tests/shared/test_alert_manager.py`
- [ ] `tests/cloud_functions/test_phase2_orchestrator.py` - NEW
- [ ] `tests/cloud_functions/test_phase3_orchestrator.py`
- [ ] `tests/integration/test_change_detection.py` - NEW
- [ ] `tests/integration/test_end_to_end_pipeline.py`

### Files to Modify

**Phase 1:**
- [ ] `scrapers/utils/pubsub_utils.py` - Add unified format + correlation_id

**Phase 2:**
- [ ] `data_processors/raw/processor_base.py` - Add change detection + unified format

**Phase 3:**
- [ ] `data_processors/analytics/analytics_base.py` - Add change detection + selective processing

**Phase 4:**
- [ ] `data_processors/precompute/precompute_base.py` - Add change detection + selective processing
- [ ] `data_processors/precompute/ml_feature_store/ml_feature_store_processor.py` - Add Phase 5 trigger

**Phase 5:**
- [ ] `predictions/coordinator/coordinator.py` - Add /trigger, update /start, add /retry, selective processing

**All Base Classes:**
- [ ] Add test dataset support (DATASET_ID environment variable)

---

## Infrastructure Checklist (UPDATED)

### Pub/Sub Topics to Create

- [ ] `nba-phase2-trigger` - NEW: Phase 2‚Üí3 orchestrator output
- [ ] `nba-phase4-trigger` - Phase 3‚Üí4 orchestrator output
- [ ] `nba-phase4-processor-complete` - Phase 4 internal tracking
- [ ] `nba-phase4-precompute-complete` - Phase 4‚Üí5 trigger

### Pub/Sub Subscriptions to Create

- [ ] `nba-phase2-orchestrator-sub` - NEW: Listen to nba-phase2-raw-complete
- [ ] `nba-phase3-sub` - NEW: Listen to nba-phase2-trigger
- [ ] `nba-phase4-trigger-sub` - Listen to nba-phase4-trigger
- [ ] `nba-phase5-trigger-sub` - Listen to nba-phase4-precompute-complete

### Cloud Functions to Deploy

- [ ] `phase2-to-phase3-orchestrator` - NEW: Tracks 21 Phase 2 processors
- [ ] `phase3-to-phase4-orchestrator` - Tracks 5 Phase 3 processors
- [ ] `phase4-orchestrator` - Tracks 5 Phase 4 processors (3 levels)

### Firestore Collections to Create

- [ ] `phase2_completion/{game_date}` - NEW: Phase 2 orchestrator state
- [ ] `phase3_completion/{analysis_date}` - Phase 3 orchestrator state
- [ ] `phase4_completion/{analysis_date}` - Phase 4 orchestrator state

### Cloud Scheduler Jobs to Create

- [ ] `phase5-daily-backup` - 6:00 AM PT ‚Üí /start
- [ ] `phase5-retry-1` - 6:15 AM PT ‚Üí /retry
- [ ] `phase5-retry-2` - 6:30 AM PT ‚Üí /retry
- [ ] `phase5-status-check` - 7:00 AM PT ‚Üí /status

---

## Change Detection Implementation Details

### Hash-Based Change Detection Strategy

**Why Hash-Based:**
- Simple to implement
- Fast to compute
- Works for any data type
- No custom queries per processor

**How It Works:**

```python
# In ChangeDetector class
def detect_changed_entities(
    self,
    current_data: List[Dict],
    entity_id_field: str,  # e.g., "player_lookup"
    previous_table: str     # e.g., "nba_raw.nbac_player_boxscore"
) -> List[str]:
    """
    Compare current data vs previous run using row hashes.

    Returns: List of entity IDs that changed
    """
    # 1. Compute hash for each current row
    current_hashes = {
        row[entity_id_field]: self._hash_row(row)
        for row in current_data
    }

    # 2. Query previous hashes from BigQuery
    previous_hashes = self._query_previous_hashes(
        previous_table,
        game_date,
        entity_id_field
    )

    # 3. Compare and find changes
    changed_entities = []
    for entity_id, current_hash in current_hashes.items():
        previous_hash = previous_hashes.get(entity_id)

        if previous_hash is None:
            # New entity
            changed_entities.append(entity_id)
        elif previous_hash != current_hash:
            # Modified entity
            changed_entities.append(entity_id)

    return changed_entities

def _hash_row(self, row: Dict) -> str:
    """Create stable hash of row data"""
    import hashlib
    import json

    # Sort keys for stable hash
    sorted_data = json.dumps(row, sort_keys=True)
    return hashlib.sha256(sorted_data.encode()).hexdigest()
```

**Storage:**
- Add `row_hash` column to all raw tables
- Updated on every MERGE
- Indexed for fast lookups

**Query Pattern:**
```sql
-- Get previous hashes
SELECT player_lookup, row_hash
FROM `nba-props-platform.nba_raw.nbac_player_boxscore`
WHERE game_date = @game_date
  AND processing_run_id = (
      -- Get last successful run
      SELECT run_id FROM processor_run_history
      WHERE processor_name = 'NbacPlayerBoxscoreProcessor'
        AND data_date = @game_date
        AND status = 'success'
      ORDER BY processed_at DESC
      LIMIT 1
  )
```

**Efficiency:**
- Initial run: Process all entities (no previous hashes)
- Subsequent runs: Process only changed entities
- Mid-day updates: 1-5 entities typically change (99% reduction)

---

## Testing Strategy (UPDATED)

### Unit Tests (Target: >90% coverage)

**New Components to Test:**
- `ChangeDetector`: Hash computation, change detection logic
- `AlertManager`: Rate limiting, batching, backfill mode
- `UnifiedPubSubPublisher`: Message validation, backfill skip
- All 3 orchestrators: State tracking, triggering logic

### Integration Tests

**Scenarios:**
1. **Full Batch Processing** - Process all 450 players
2. **Incremental Processing** - 1 player changed, verify only that player processed
3. **Mid-Day Update** - Second run same day with 1 change
4. **Backfill Mode** - Verify downstream skipped
5. **Change Detection Efficiency** - Measure overhead (<5%)

### Performance Tests

**Metrics:**
- Change detection query time (target: <1 second)
- Hash computation time (target: <0.1 seconds per 1000 rows)
- End-to-end latency with change detection vs without
- Memory usage with change detection

---

## Updated Timeline

| Week | Focus | Hours | Key Deliverable |
|------|-------|-------|-----------------|
| **Week 1** | Foundation + Phase 1-2 + Phase 2‚Üí3 Orchestrator | 18 | Unified infrastructure + change detection base |
| **Week 2** | Phase 3-4 + Change Detection | 20 | Phase 3-4 with selective processing |
| **Week 3** | Phase 5 + Backfill Scripts + Hardening | 25 | Complete pipeline + hardened backfill tooling |
| **Week 4** | Deploy + Backfill + Monitor | 12 | Production-ready system |
| **TOTAL** | | **75 hours** | Event-driven pipeline with change detection |

**Note:** Original 72h plan + 3h for backfill script hardening from external review = 75h

**With Critical Fixes (from failure analysis review):** 75h + 17h = **92 hours total**

**Calendar Timeline:** 3-4 weeks for implementation + ~3-4 days for backfill execution

---

## Success Criteria (UPDATED)

### Week 1 Success
- [ ] Change detection working in Phase 2
- [ ] Phase 2‚Üí3 orchestrator tracking 21 processors
- [ ] Test dataset support in all base classes
- [ ] Smart alert manager preventing email spam

### Week 2 Success
- [ ] Phase 3 with selective processing working
- [ ] Phase 3‚Üí4 orchestrator aggregating changed entities
- [ ] Phase 4 with change detection through 3-level cascade

### Week 3 Success
- [ ] Phase 5 processing only changed players
- [ ] Backfill scripts created
- [ ] **Backfill scripts hardened (all critical fixes applied)**
- [ ] Preflight check script created
- [ ] Tmux wrapper script created
- [ ] End-to-end change detection test passing
- [ ] All unit tests >90% coverage

### Week 4 Success
- [ ] Production deployment successful
- [ ] Preflight check passes
- [ ] Historical backfills complete (4 seasons)
- [ ] Current season backfilled
- [ ] Daily processing enabled
- [ ] Change detection efficiency >95% (for incremental updates)
- [ ] No systematic data loss (100% completion validated)

### Overall Success (Production Validation)
- [ ] Mid-day injury update ‚Üí prediction updated in <5 minutes
- [ ] Change detection working: 1 player changed = 1 player processed
- [ ] Full batch still works: overnight run processes all 450 players
- [ ] Backfill mode prevents downstream triggers
- [ ] Correlation ID traces predictions to scraper runs
- [ ] Smart alerts prevent email spam during backfills
- [ ] Test datasets allow safe testing

---

## Risk Assessment (UPDATED)

### New Risks with Change Detection

**Risk:** Change detection adds complexity
- **Mitigation:** Start with simple hash-based approach, extensive testing
- **Fallback:** Can disable change detection per processor if issues

**Risk:** Hash computation overhead
- **Mitigation:** Benchmark early, optimize if needed
- **Acceptance:** <5% overhead acceptable for 99% efficiency gain

**Risk:** Changed entity tracking through orchestrators
- **Mitigation:** Test aggregation logic thoroughly
- **Fallback:** Can fall back to full batch processing

### Medium Risks

**Risk:** 3 orchestrators instead of 2
- **Mitigation:** Same pattern repeated, extensive testing
- **Acceptance:** More infrastructure but cleaner architecture

**Risk:** Backfill execution time (~5-7 days)
- **Mitigation:** Parallel execution, progress monitoring
- **Acceptance:** One-time cost, worth doing right

---

**Document Status:** ‚úÖ FINAL Implementation Plan (Updated with Backfill Review)
**Next Action:** Begin Week 1 Day 1 - Create UnifiedPubSubPublisher + ChangeDetector
**Estimated Completion:** 3-4 weeks (92h) implementation + 3-4 days backfill
**Reviews Integrated:** Failure Analysis Review (+17h) + Backfill Review (+3h)
