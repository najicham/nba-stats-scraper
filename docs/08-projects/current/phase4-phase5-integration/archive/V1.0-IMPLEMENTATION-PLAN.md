# v1.0 Implementation Plan - Event-Driven Pipeline (Phases 1-5)

**Status:** ðŸ“‹ Ready to Implement
**Version:** 1.0 (Batch Processing + Event-Driven Architecture)
**Timeline:** 3-4 weeks, ~68 hours
**Scope:** Complete event-driven pipeline with unified patterns

---

## Executive Summary

**What We're Building:**
- âœ… Unified message format across ALL phases
- âœ… Event-driven orchestration (Phase 1â†’2â†’3â†’4â†’5)
- âœ… Correlation ID tracing (scraper â†’ prediction)
- âœ… Backfill mode support (skip downstream triggers)
- âœ… Deduplication everywhere (idempotent operations)
- âœ… Orchestrators for Phase 3â†’4 and Phase 4 internal dependencies
- âœ… Graceful degradation with backup schedulers

**What We're NOT Building (v1.1):**
- âŒ Change detection (defer to v1.1)
- âŒ Real-time incremental updates (defer to v1.1)
- âŒ Per-player processing endpoints (defer to v1.1)

**Why This Scope:**
- Focus on getting event-driven pipeline solid
- Add efficiency optimizations later when we have data
- Ship faster, iterate based on real usage

---

## Architecture Decisions

### Decision 1: No Orchestrator for Phase 2â†’3

**Why:** Each Phase 3 processor depends on DIFFERENT Phase 2 tables
- PlayerGameSummaryProcessor needs: ~4 specific Phase 2 tables
- TeamDefenseProcessor needs: ~3 different Phase 2 tables
- Dependency checks in each processor handle coordination perfectly

**Result:** Phase 3 gets triggered multiple times (once per Phase 2 processor), but deduplication prevents duplicate work

---

### Decision 2: YES Orchestrators for Phase 3â†’4 and Phase 4 Internal

**Phase 3â†’4 Orchestrator:**
- Listens to `nba-phase3-analytics-complete`
- Tracks all 5 Phase 3 processors in Firestore
- When ALL complete â†’ publishes ONE trigger to Phase 4
- Reduces noise, ensures clean handoff

**Phase 4 Internal Orchestrator:**
- Tracks 3-level dependency cascade:
  - Level 1: team_defense_zone, player_shot_zone, player_daily_cache (parallel)
  - Level 2: player_composite_factors (waits for Level 1)
  - Level 3: ml_feature_store_v2 (waits for all)
- Triggers each level when dependencies ready
- Clean separation of concerns

**Implementation:** Cloud Functions + Firestore (atomic state updates)

---

### Decision 3: Correlation ID Across All Phases

**Implementation:**
```python
# Phase 1 (source)
correlation_id = execution_id  # Same value

# Phases 2-5 (propagate)
correlation_id = upstream_message.get('correlation_id') or upstream_message.get('execution_id')

# Use in all messages
message = {
    "execution_id": self.run_id,  # This run's ID
    "correlation_id": correlation_id,  # Original scraper run ID
    # ...
}
```

**Benefit:** Trace any prediction back to original scraper run

---

### Decision 4: Backfill Mode Support

**Unified Message Fields:**
```python
{
    # ... standard fields ...
    "skip_downstream_trigger": bool,  # If true, don't publish downstream event
    "backfill_mode": bool,  # Indicates historical data processing
    "backfill_reason": str,  # "historical_load", "reprocessing", etc.
}
```

**Implementation:**
```python
def _publish_completion_event(self, ...):
    # Check if backfill mode
    if self.opts.get('skip_downstream_trigger'):
        logger.info("Backfill mode - skipping downstream trigger")
        return  # Don't publish to next phase

    # Normal publishing
    publisher.publish(topic, message)
```

**Use Case:** Load 4 seasons of historical data without triggering predictions

---

### Decision 5: Defer Change Detection to v1.1

**v1.0 Scope:**
- Batch processing only (process all players every run)
- Simple, predictable, testable

**v1.1 Scope (Future):**
- Add change detection queries
- Process only changed entities
- 99% efficiency gain for incremental updates

**Why Defer:** Focus on getting event-driven pipeline working first, add optimizations based on real usage patterns

---

## Updated Unified Message Format

```python
{
    # ========== IDENTITY ==========
    "processor_name": str,       # e.g., "bdl_games", "PlayerGameSummaryProcessor"
    "phase": str,                # e.g., "phase_1_scrapers", "phase_3_analytics"
    "execution_id": str,         # UUID for this specific run
    "correlation_id": str,       # UUID from Phase 1 scraper (traces full pipeline)

    # ========== DATA REFERENCE ==========
    "game_date": str,            # ISO date "2025-11-28"
    "output_table": str,         # BigQuery table name
    "output_dataset": str,       # BigQuery dataset

    # ========== STATUS ==========
    "status": str,               # "success" | "partial" | "no_data" | "failed"
    "record_count": int,         # Records processed
    "records_failed": int,       # Records that failed

    # ========== TIMING ==========
    "timestamp": str,            # ISO 8601 UTC timestamp
    "duration_seconds": float,   # Execution duration

    # ========== TRACING ==========
    "parent_processor": str,     # Upstream processor that triggered this
    "trigger_source": str,       # "pubsub" | "scheduler" | "manual"
    "trigger_message_id": str,   # Pub/Sub message ID

    # ========== BACKFILL SUPPORT (NEW) ==========
    "skip_downstream_trigger": bool,  # If true, don't trigger next phase
    "backfill_mode": bool,            # Historical data processing flag
    "backfill_reason": str,           # Why backfilling

    # ========== ERROR INFO ==========
    "error_message": str | null,
    "error_type": str | null,

    # ========== PHASE-SPECIFIC ==========
    "metadata": dict             # Phase-specific additional data
}
```

---

## Week-by-Week Implementation Plan

### **Week 1: Foundation & Unified Infrastructure** (~14 hours)

#### Day 1: Unified Message Format & Publisher (4 hours)

**Tasks:**
1. Create `shared/utils/unified_pubsub_publisher.py`
   - Validates unified message format (all required fields)
   - Handles backfill mode (skip_downstream_trigger)
   - Logs published messages for debugging

2. Write unit tests for UnifiedPubSubPublisher
   - Test message validation
   - Test backfill mode skipping
   - Test error handling

**Deliverable:** Reusable publisher class used by all phases

---

#### Day 2: Update Phase 1 (Scrapers) (4 hours)

**Tasks:**
1. Update `scrapers/utils/pubsub_utils.py` - ScraperPubSubPublisher
   - Add new unified fields (phase, correlation_id, output_table, output_dataset)
   - Keep old fields for backward compatibility (dual format)
   - Set `correlation_id = execution_id` (Phase 1 is the source)
   - Add backfill mode support

2. Test scraper publishing
   - Run one scraper manually
   - Verify message format in Pub/Sub console
   - Verify both old and new fields present

**Deliverable:** Phase 1 publishing unified format (with backward compatibility)

---

#### Day 3: Update Phase 2 (Raw Processors) (6 hours)

**Tasks:**
1. Update `data_processors/raw/processor_base.py`
   - Accept both old and new message formats (migration period)
   - Extract `correlation_id` from upstream message
   - Update `_publish_completion_event()` to use UnifiedPubSubPublisher
   - Propagate correlation_id through message
   - Add backfill mode checking

2. Test Phase 1â†’2 end-to-end
   - Trigger scraper manually
   - Verify Phase 2 receives and processes
   - Verify correlation_id propagated
   - Test backfill mode (skip_downstream_trigger=true)

**Deliverable:** Phase 1â†’2 working with unified messages + correlation tracking

---

### **Week 2: Build Phase 3 & Phase 3â†’4 Orchestrator** (~20 hours)

#### Day 4: Update Phase 3 (Analytics) (6 hours)

**Tasks:**
1. Update `data_processors/analytics/analytics_base.py`
   - Extract correlation_id from upstream message
   - Update `_publish_completion_message()` to unified format
   - Use UnifiedPubSubPublisher
   - Add backfill mode support

2. Update all 5 Phase 3 processors
   - Verify they extend AnalyticsProcessorBase correctly
   - Verify they call super().post_process()
   - Test each processor publishes correctly

**Deliverable:** All 5 Phase 3 processors publishing unified messages

---

#### Day 5: Build Phase 3â†’4 Orchestrator (Cloud Function) (8 hours)

**Tasks:**
1. Create `cloud_functions/phase3_to_phase4_orchestrator/main.py`
   - Listen to `nba-phase3-analytics-complete`
   - Track 5 Phase 3 processors in Firestore
   - Document schema: `{analysis_date: {processor_name: {completed_at, correlation_id}}}`
   - When all 5 complete â†’ publish to `nba-phase4-trigger`

2. Create `cloud_functions/phase3_to_phase4_orchestrator/deploy.sh`
   - Deploy Cloud Function
   - Create Firestore collection
   - Create Pub/Sub subscription

3. Write tests
   - Test partial completion tracking
   - Test full completion trigger
   - Test idempotency (duplicate messages)

**Deliverable:** Phase 3â†’4 orchestrator working in staging

---

#### Day 6: Integration Test Phase 1â†’2â†’3 (6 hours)

**Tasks:**
1. Manual end-to-end test
   - Trigger Phase 1 scraper for test date
   - Verify Phase 2 processes
   - Verify all 5 Phase 3 processors run
   - Verify orchestrator tracks and triggers Phase 4

2. Test backfill mode
   - Trigger Phase 1 with skip_downstream_trigger=true
   - Verify Phase 2 processes but doesn't trigger Phase 3

3. Test correlation tracking
   - Query processor_run_history
   - Verify correlation_id matches from Phase 1â†’2â†’3

**Deliverable:** Phases 1-3 working end-to-end with orchestration

---

### **Week 3: Build Phase 4 & Phase 5** (~22 hours)

#### Day 7: Update Phase 4 (Precompute) (6 hours)

**Tasks:**
1. Update `data_processors/precompute/precompute_base.py`
   - Extract correlation_id
   - Update publishing to unified format
   - Add backfill mode support

2. Update ml_feature_store_v2 processor
   - Add `_publish_phase5_trigger()` method (from IMPLEMENTATION-FULL.md)
   - Publish to `nba-phase4-precompute-complete`
   - Include players_ready, players_total counts

**Deliverable:** Phase 4 publishing completion events

---

#### Day 8: Build Phase 4 Internal Orchestrator (8 hours)

**Tasks:**
1. Create `cloud_functions/phase4_orchestrator/main.py`
   - Track 5 Phase 4 processors in Firestore
   - Model 3-level dependency graph:
     - Level 1: 3 processors (parallel)
     - Level 2: 1 processor (depends on Level 1)
     - Level 3: 1 processor (depends on all)
   - Trigger each level when dependencies ready

2. Update Phase 4 processors to publish to internal topic
   - Each Level 1 processor publishes to `nba-phase4-processor-complete`
   - Orchestrator tracks and triggers Level 2
   - Level 2 publishes, orchestrator triggers Level 3

**Deliverable:** Phase 4 dependency orchestration working

---

#### Day 9-10: Build Phase 5 Coordinator (8 hours)

**Tasks:**
1. Update `predictions/coordinator/coordinator.py`
   - Add `/trigger` endpoint (PRIMARY - from Pub/Sub)
   - Update `/start` endpoint (BACKUP - from scheduler with 30-min wait)
   - Add `/retry` endpoint (INCREMENTAL - catch stragglers)
   - Add all 7 helper functions from IMPLEMENTATION-FULL.md:
     - `_validate_phase4_ready()`
     - `_wait_for_phase4()`
     - `_get_batch_status()`
     - `_get_players_needing_retry()`
     - `_process_retry_batch()`
     - `_record_batch_run()`
     - `_send_alert()`

2. Create Pub/Sub infrastructure
   - Topic: `nba-phase4-precompute-complete`
   - Subscription: `nba-phase5-trigger-sub` â†’ /trigger endpoint
   - Scheduler: phase5-daily-backup (6:00 AM PT)
   - Scheduler: phase5-retry-1 (6:15 AM PT)
   - Scheduler: phase5-retry-2 (6:30 AM PT)
   - Scheduler: phase5-status-check (7:00 AM PT)

**Deliverable:** Phase 5 complete with Pub/Sub trigger + backup schedulers

---

### **Week 4: Testing, Deployment & Monitoring** (~12 hours)

#### Day 11: Comprehensive Testing (6 hours)

**Tasks:**
1. Unit tests (all new code >90% coverage)
   - Test UnifiedPubSubPublisher
   - Test orchestrator logic
   - Test Phase 5 helper functions

2. Integration tests
   - Full pipeline Phase 1â†’2â†’3â†’4â†’5 with test data
   - Test failure scenarios (processor fails, Pub/Sub fails)
   - Test retry logic
   - Test backfill mode (skip downstream)

3. Performance testing
   - Measure latency Phase 4â†’5
   - Verify predictions complete within SLA

**Deliverable:** All tests passing, confidence in system

---

#### Day 12: Deploy to Production (3 hours)

**Tasks:**
1. Deploy all services sequentially
   - Deploy Phase 1 updates
   - Deploy Phase 2 updates
   - Deploy Phase 3 updates + orchestrator
   - Deploy Phase 4 updates + orchestrator
   - Deploy Phase 5 updates

2. Create all Pub/Sub topics/subscriptions

3. Create Cloud Functions (orchestrators)

4. Create Cloud Scheduler jobs (Phase 5 backup)

5. Verify infrastructure
   - All topics exist
   - All subscriptions connected
   - All schedulers configured

**Deliverable:** All code deployed to production

---

#### Day 13-14: Monitor & Document (3 hours)

**Tasks:**
1. Monitor first overnight run
   - Watch Phase 1 scrapers
   - Verify Phase 2-3-4-5 cascade
   - Check latency at each phase
   - Verify predictions generated

2. Create monitoring dashboards
   - Pipeline health (all phases)
   - Latency metrics (Phase 1â†’5 end-to-end)
   - Error rates by phase

3. Set up alerts
   - Critical: Phase 5 predictions <90% coverage
   - Warning: Any phase partial completion
   - Info: Daily summary

4. Document operational procedures
   - How to manually trigger phases
   - How to check orchestrator state
   - How to debug failures
   - Runbook for common issues

**Deliverable:** Production-ready system with monitoring

---

## File Modifications Checklist

### New Files to Create

- [ ] `shared/utils/unified_pubsub_publisher.py` - Unified publisher class
- [ ] `cloud_functions/phase3_to_phase4_orchestrator/main.py` - Phase 3â†’4 orchestrator
- [ ] `cloud_functions/phase3_to_phase4_orchestrator/deploy.sh` - Deployment script
- [ ] `cloud_functions/phase4_orchestrator/main.py` - Phase 4 internal orchestrator
- [ ] `cloud_functions/phase4_orchestrator/deploy.sh` - Deployment script
- [ ] `bin/phase5/deploy_pubsub_infrastructure.sh` - Phase 5 infrastructure deployment
- [ ] `tests/shared/test_unified_pubsub_publisher.py` - Publisher tests
- [ ] `tests/cloud_functions/test_phase3_orchestrator.py` - Orchestrator tests
- [ ] `tests/integration/test_end_to_end_pipeline.py` - Integration tests

### Files to Modify

#### Phase 1
- [ ] `scrapers/utils/pubsub_utils.py` - Add unified format fields + correlation_id

#### Phase 2
- [ ] `data_processors/raw/processor_base.py` - Extract correlation_id, use unified publisher

#### Phase 3
- [ ] `data_processors/analytics/analytics_base.py` - Unified format + correlation_id

#### Phase 4
- [ ] `data_processors/precompute/precompute_base.py` - Unified format + correlation_id
- [ ] `data_processors/precompute/ml_feature_store/ml_feature_store_processor.py` - Add Phase 5 trigger

#### Phase 5
- [ ] `predictions/coordinator/coordinator.py` - Add /trigger, update /start, add /retry + helpers

---

## Infrastructure Checklist

### Pub/Sub Topics to Create

- [ ] `nba-phase4-precompute-complete` - Phase 4â†’5 trigger
- [ ] `nba-phase4-trigger` - Phase 3â†’4 orchestrator trigger
- [ ] `nba-phase4-processor-complete` - Phase 4 internal tracking

### Pub/Sub Subscriptions to Create

- [ ] `nba-phase4-trigger-sub` - Phase 3â†’4 orchestrator â†’ Phase 4
- [ ] `nba-phase5-trigger-sub` - Phase 4 â†’ Phase 5 coordinator /trigger
- [ ] `nba-phase4-processor-complete-sub` - Phase 4 orchestrator listener

### Cloud Functions to Deploy

- [ ] `phase3-to-phase4-orchestrator` - Tracks 5 Phase 3 processors
- [ ] `phase4-orchestrator` - Tracks 5 Phase 4 processors (3 levels)

### Firestore Collections to Create

- [ ] `phase3_completion/{analysis_date}` - Phase 3 orchestrator state
- [ ] `phase4_completion/{analysis_date}` - Phase 4 orchestrator state

### Cloud Scheduler Jobs to Create

- [ ] `phase5-daily-backup` - 6:00 AM PT â†’ /start (backup trigger)
- [ ] `phase5-retry-1` - 6:15 AM PT â†’ /retry (stragglers)
- [ ] `phase5-retry-2` - 6:30 AM PT â†’ /retry (stragglers)
- [ ] `phase5-status-check` - 7:00 AM PT â†’ /status (SLA check)

---

## Testing Strategy

### Unit Tests (Target: >90% coverage)

**What to Test:**
- UnifiedPubSubPublisher message validation
- Backfill mode skip logic
- Correlation ID propagation
- Orchestrator state tracking
- Phase 5 helper functions

**Tools:** pytest, unittest.mock

---

### Integration Tests

**Scenarios:**
1. **Happy Path:** Phase 1â†’2â†’3â†’4â†’5 with test data
2. **Backfill Mode:** Skip downstream triggers
3. **Partial Failure:** Some processors fail, others succeed
4. **Pub/Sub Retry:** Duplicate messages handled via deduplication
5. **Orchestrator Recovery:** Orchestrator crashes mid-tracking

**Tools:** pytest with live GCP resources (test project)

---

### Performance Tests

**Metrics to Measure:**
- Phase 1â†’2 latency (target: <5 min)
- Phase 2â†’3 latency (target: <10 min)
- Phase 3â†’4 latency (target: <2 min)
- Phase 4â†’5 latency (target: <5 min)
- End-to-end latency (target: <60 min from scraper start to predictions ready)

**Load Test:** Process 4 seasons of backfill data, measure performance

---

## Rollback Procedures

### If Phase 1-2 Updates Fail

**Symptoms:** Phase 2 not receiving messages or failing to process

**Rollback:**
```bash
# Revert Phase 1 scraper code
git revert [commit-hash]
./bin/scrapers/deploy/deploy_scrapers_simple.sh

# Revert Phase 2 processor code
git revert [commit-hash]
./bin/raw/deploy/deploy_processors_simple.sh
```

**Impact:** System reverts to pre-upgrade state (working)

---

### If Orchestrators Fail

**Symptoms:** Phase 3 completes but Phase 4 never triggers

**Rollback:**
```bash
# Delete Cloud Function
gcloud functions delete phase3-to-phase4-orchestrator \
    --project=nba-props-platform \
    --region=us-west2

# Fall back to time-based scheduler (temporary)
gcloud scheduler jobs create http phase4-manual-trigger \
    --schedule="0 1 * * *" \
    --uri="https://[phase4-url]/process-date" \
    --http-method=POST
```

**Impact:** Loses event-driven triggering, falls back to time-based (slower but works)

---

### If Phase 5 Updates Fail

**Symptoms:** Predictions not generating, coordinator errors

**Rollback:**
```bash
# Rollback coordinator code
gcloud run revisions list --service=prediction-coordinator \
    --project=nba-props-platform \
    --region=us-west2

gcloud run services update-traffic prediction-coordinator \
    --to-revisions=[PREVIOUS-REVISION]=100 \
    --project=nba-props-platform \
    --region=us-west2
```

**Impact:** Phase 5 reverts to previous version (manual triggering only)

---

## Success Criteria

### Week 1 Success
- [ ] Phase 1 publishing unified messages
- [ ] Phase 2 receiving and processing with correlation_id
- [ ] End-to-end test Phase 1â†’2 passing
- [ ] Backfill mode working (skip downstream)

### Week 2 Success
- [ ] All 5 Phase 3 processors publishing unified messages
- [ ] Phase 3â†’4 orchestrator tracking completion
- [ ] Phase 4 triggered when all 5 Phase 3 complete
- [ ] End-to-end test Phase 1â†’2â†’3â†’4 passing

### Week 3 Success
- [ ] Phase 4 internal orchestrator managing 3-level cascade
- [ ] Phase 5 /trigger endpoint receiving events from Phase 4
- [ ] Phase 5 backup scheduler working
- [ ] End-to-end test Phase 1â†’2â†’3â†’4â†’5 passing

### Week 4 Success
- [ ] All unit tests passing (>90% coverage)
- [ ] All integration tests passing
- [ ] Production deployment successful
- [ ] First overnight run completed successfully
- [ ] Monitoring dashboards created
- [ ] Alerts configured

### Overall Success
- [ ] Complete pipeline Phase 1â†’5 working
- [ ] Predictions generated for current season
- [ ] End-to-end latency <60 minutes
- [ ] >95% prediction completion rate
- [ ] Correlation ID tracing working
- [ ] Backfill mode working
- [ ] Clean, maintainable codebase
- [ ] Comprehensive documentation

---

## Risk Mitigation

### Risk 1: Orchestrator Complexity

**Risk:** Cloud Functions + Firestore adds moving parts

**Mitigation:**
- Comprehensive testing before deployment
- Keep orchestrator logic simple (just state tracking)
- Add extensive logging
- Create manual override (bypass orchestrator if needed)

---

### Risk 2: Migration Breakage

**Risk:** Unified message format breaks existing Phase 2

**Mitigation:**
- Dual format publishing from Phase 1 (old + new fields)
- Phase 2 accepts both formats during migration
- Gradual rollout (Phase 1 first, then Phase 2, etc.)
- Test each phase independently

---

### Risk 3: Timing Issues

**Risk:** Race conditions in orchestrators

**Mitigation:**
- Use Firestore atomic updates (transactions)
- Add deduplication in orchestrators
- Test with concurrent messages
- Add retry logic for transient failures

---

### Risk 4: Production Data Issues

**Risk:** Real production data has edge cases not in test data

**Mitigation:**
- Start with small subset (one game day)
- Monitor closely for first week
- Keep old code deployable (rollback ready)
- Add comprehensive error logging

---

## Estimated Timeline

| Week | Focus | Hours | Key Deliverable |
|------|-------|-------|-----------------|
| **Week 1** | Foundation | 14 | Phases 1-2 unified + correlation tracking |
| **Week 2** | Phase 3 + Orchestrator | 20 | Phase 3â†’4 event-driven |
| **Week 3** | Phase 4-5 | 22 | Complete pipeline working |
| **Week 4** | Testing + Deploy | 12 | Production-ready system |
| **TOTAL** | | **68 hours** | Event-driven pipeline Phases 1-5 |

**Calendar Timeline:** 3-4 weeks depending on testing thoroughness

---

## Post-Deployment Monitoring (First Week)

### Daily Checks
- [ ] All phases completed successfully?
- [ ] Correlation IDs traced correctly?
- [ ] Orchestrators tracking state properly?
- [ ] Predictions generated on time?
- [ ] Any alerts triggered?

### Weekly Review
- [ ] Latency metrics within targets?
- [ ] Error rates acceptable (<5%)?
- [ ] Backfill mode working as expected?
- [ ] Any unexpected behavior?

### Metrics to Track
- **Pipeline Latency:** Scraper start â†’ Predictions ready
- **Phase Latency:** Each phase completion time
- **Completion Rate:** % of expected predictions generated
- **Error Rate:** Failures per phase
- **Orchestrator Performance:** State tracking accuracy

---

## Next Steps After v1.0

Once v1.0 is stable (1-2 months of production use):

1. **Evaluate v1.1 Need**
   - How many incremental updates per day?
   - Is 2-5 minute latency sufficient?
   - User demand for real-time updates?

2. **Consider Adding:**
   - Change detection (process only changed entities)
   - Real-time incremental path (per-player updates)
   - Prediction versioning (supersede old predictions)

3. **Infrastructure Improvements:**
   - Monitoring dashboards refinement
   - Alert tuning based on real patterns
   - Cost optimization

---

**Document Status:** âœ… Complete Implementation Plan
**Next Action:** Begin Week 1 Day 1 - Create UnifiedPubSubPublisher
**Questions?** Review with team before starting
