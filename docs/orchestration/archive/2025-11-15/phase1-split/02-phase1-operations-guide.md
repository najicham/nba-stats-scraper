# Phase 1 Operations Guide

**File:** `docs/orchestration/02-phase1-operations-guide.md`
**Created:** 2025-11-13 10:06 PST
**Last Updated:** 2025-11-15 10:52 PST (reorganization)
**Purpose:** Technical specifications and operational guide for Phase 1 orchestration and scheduling
**Status:** Production Deployed
**Audience:** Engineers operating and extending Phase 1 orchestration infrastructure

ðŸ“‹ Table of Contents

Executive Summary

Current Architecture

Deployed Components

Cloud Scheduler Jobs

BigQuery Tables

Orchestration Endpoints

Monitoring & Health Checks

Manual Operations

What's NOT Yet Deployed

Next Steps (Week 2-3)

Troubleshooting

Executive Summary

What's Deployed (November 12, 2025)

Orchestration System Status: âœ… OPERATIONAL

We have successfully deployed a 3-layer orchestration system that makes intelligent decisions about when NBA data scrapers should run, but does not yet trigger the scrapers automatically.

Key Components:

âœ… Schedule Locker - Generates daily workflow plan (5 AM ET)

âœ… Master Controller - Makes RUN/SKIP/ABORT decisions (hourly 6-11 PM ET)

âœ… Workflow Executor - Executes scrapers based on decisions (5 min after controller)

âœ… Cleanup Processor - Self-healing for missed Pub/Sub events (every 15 min)

âœ… 5 BigQuery Tables - Complete audit trail of decisions and operations

âœ… 4 Cloud Scheduler Jobs - Fully automated orchestration

What It Does

Current State (November 12, 2025):

5:00 AM ET â†’ Schedule Locker generates daily plan
6:00 AM ET â†’ Master Controller evaluates â†’ RUN decisions logged
6:05 AM ET â†’ Workflow Executor reads decisions â†’ Scrapers execute
7:00 AM ET â†’ Master Controller evaluates â†’ RUN decisions logged  
7:05 AM ET â†’ Workflow Executor reads decisions â†’ Scrapers execute
... continues hourly through 11 PM ET ...


The system knows when to scrape AND automatically executes scrapers. âœ…

Key Achievement: 5-minute delay between decision and execution allows for:

Decision auditing before execution

Manual intervention if needed (pause scheduler)

Clear separation of concerns

Mission Statement

Phase 1 Mission:
Intelligently orchestrate 33 NBA data collection scrapers across 7 external sources based on game schedule, timing windows, and data availability patterns.

Current State:
Phase 1 orchestration is complete and operational. The system makes intelligent decisions AND executes scrapers automatically. No manual intervention required.

Architecture Philosophy

Three-Layer Design:

Configuration Layer (config/workflows.yaml)

Business logic: "Betting lines run 6h before games"

Workflow definitions: Which scrapers, when, why

Orchestration Layer (Cloud Run: nba-scrapers)

Schedule Locker: Plans daily execution

Master Controller: Evaluates current conditions

Decision Engine: RUN/SKIP/ABORT logic

Monitoring Layer (BigQuery)

Daily plans: What SHOULD run

Hourly decisions: What we DECIDED

Audit trail: Complete history

Future Layer (Week 2-3):

Execution Layer (Workflow Executor)

Reads RUN decisions

Resolves scraper parameters

Triggers scraper endpoints

Tracks execution status

Current Architecture

System Diagram (Deployed Nov 12, 2025)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Scheduler (3 Jobs) - GCP Managed                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. daily-schedule-locker    (5:00 AM ET)                  â”‚ â”‚
â”‚  â”‚ 2. master-controller-hourly (6-11 PM ET, every hour)      â”‚ â”‚
â”‚  â”‚ 3. cleanup-processor        (Every 15 minutes)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP POST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Run Service: nba-scrapers (us-west2)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Orchestration Endpoints (Flask)                           â”‚ â”‚
â”‚  â”‚ â€¢ POST /generate-daily-schedule â†’ Schedule Locker         â”‚ â”‚
â”‚  â”‚ â€¢ POST /evaluate                â†’ Master Controller       â”‚ â”‚
â”‚  â”‚ â€¢ POST /cleanup                 â†’ Cleanup Processor       â”‚ â”‚
â”‚  â”‚                                                            â”‚ â”‚
â”‚  â”‚ Scraper Endpoints (NOT auto-triggered yet)                â”‚ â”‚
â”‚  â”‚ â€¢ POST /scraper/{name}          â†’ 33 scrapers            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ BigQuery Insert
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BigQuery Dataset: nba_orchestration                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. daily_expected_schedule (What SHOULD run)              â”‚ â”‚
â”‚  â”‚ 2. workflow_decisions (What we DECIDED - RUN/SKIP/ABORT)  â”‚ â”‚
â”‚  â”‚ 3. cleanup_operations (Self-healing activity)             â”‚ â”‚
â”‚  â”‚ 4. scraper_execution_log (Scraper runs - when they run)  â”‚ â”‚
â”‚  â”‚ 5. workflow_executions (Execution tracking - NEW)         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ Wait 5 minutes
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Scheduler (:05 hourly 6 AM-11 PM ET)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP POST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Workflow Executor (Cloud Run: nba-scrapers)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ POST /execute-workflows                                    â”‚ â”‚
â”‚  â”‚ â€¢ Reads RUN decisions from workflow_decisions table        â”‚ â”‚
â”‚  â”‚ â€¢ Resolves parameters (season, date, game_ids)             â”‚ â”‚
â”‚  â”‚ â€¢ Calls scrapers via POST /scrape                          â”‚ â”‚
â”‚  â”‚ â€¢ Tracks execution in workflow_executions table            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“ HTTP POST /scrape
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 5: Scrapers (Cloud Run: nba-scrapers)                   â”‚
â”‚  â€¢ 33 scrapers operational                                      â”‚
â”‚  â€¢ Triggered automatically by Workflow Executor                 â”‚
â”‚  â€¢ Write JSON to GCS â†’ Publish Pub/Sub events                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Daily Timeline (Actual as of Nov 12)

5:00 AM ET - Schedule Locker
    â†“ Reads game schedule
    â†“ Reads workflow config
    â†“ Generates daily plan
    â†“ Writes to: daily_expected_schedule
    Result: 5-20 workflows scheduled for today

6:00 AM ET - Master Controller (1st evaluation)
    â†“ Reads daily_expected_schedule
    â†“ Checks current time vs expected times
    â†“ Evaluates: Should "morning_operations" run now?
    â†“ Decision: RUN (ideal window 6-10 AM)
    â†“ Writes to: workflow_decisions
    Result: Decision logged, but scraper NOT triggered yet

6:05 AM ET - Workflow Executor (1st execution)
    â†“ Reads workflow_decisions for RUN decisions
    â†“ Finds "morning_operations" RUN decision from 6:00 AM
    â†“ Resolves parameters for each scraper
    â†“ Calls: POST /scrape for each scraper
    â†“ Writes to: workflow_executions
    Result: Scrapers execute automatically

7:00 AM ET - Master Controller (2nd evaluation)
    â†“ Re-evaluates all workflows
    â†“ Decision: morning_operations already decided
    â†“ Decision: betting_lines - SKIP (games not for 12 hours)
    â†“ Writes to: workflow_decisions
    Result: More decisions logged

7:05 AM ET - Workflow Executor (2nd execution)
    â†“ Reads workflow_decisions for new RUN decisions
    â†“ Finds no new RUN decisions (only SKIP)
    â†“ No scrapers to execute
    Result: Quick check, no action needed

... Every hour 6 AM - 11 PM ET ...

Every 15 minutes - Cleanup Processor
    â†“ Checks for orphaned GCS files (files without BigQuery records)
    â†“ Checks for failed Pub/Sub deliveries
    â†“ Re-publishes events if needed
    â†“ Writes to: cleanup_operations
    Result: Self-healing activity tracked


Data Flow

Configuration â†’ Planning â†’ Decision â†’ (Future: Execution)

config/workflows.yaml
    â†“ Read by Schedule Locker
daily_expected_schedule (BigQuery)
    â†“ Read by Master Controller
workflow_decisions (BigQuery)
    â†“ (Future) Read by Workflow Executor
scraper_execution_log (BigQuery)
    â†“ Read by Master Controller (check if already ran)


Deployed Components

1. Schedule Locker

Purpose: Generate daily workflow execution plan at 5 AM ET

Trigger: Cloud Scheduler job daily-schedule-locker (5 AM ET daily)

Endpoint: POST /generate-daily-schedule

What It Does:

Reads config/workflows.yaml for workflow definitions

Queries game schedule for today

Calculates expected run times for each workflow

Writes plan to daily_expected_schedule table

Example Output:

{
  "status": "success",
  "schedule": {
    "date": "2025-11-12",
    "games_scheduled": 12,
    "workflows_evaluated": 7,
    "expected_runs": 19,
    "locked_at": "2025-11-12T10:00:15.123456"
  }
}


BigQuery Records Created:

19 rows in daily_expected_schedule (one per workflow instance)

Example Schedule Entry:

date: '2025-11-12'
workflow_name: 'betting_lines'
expected_run_time: '2025-11-12T13:30:00-05:00'  -- 6h before first game
reason: 'Pre-game betting lines (12 games today)'
scrapers: ['oddsa_events', 'oddsa_player_props', 'oddsa_game_lines']


Success Criteria:

âœ… Runs within 2 minutes (typical: 10-30 seconds)

âœ… Generates 5-20 workflow entries (depends on game schedule)

âœ… No errors in logs

Monitoring Query:

SELECT 
  date,
  locked_at,
  COUNT(*) as workflows_scheduled
FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')
GROUP BY date, locked_at;


2. Master Controller

Purpose: Evaluate workflow execution decisions hourly

Trigger: Cloud Scheduler job master-controller-hourly (Every hour, 6 AM - 11 PM ET)

Endpoint: POST /evaluate

What It Does:

Reads daily_expected_schedule for today

For each workflow:

Check if within time window

Check if already ran today

Check game schedule/conditions

Evaluate dependencies

Makes decision: RUN, SKIP, or ABORT

Writes decisions to workflow_decisions table

Example Output:

{
  "status": "success",
  "evaluation_time": "2025-11-12T18:30:00-05:00",
  "workflows_evaluated": 8,
  "decisions": [
    {
      "workflow": "betting_lines",
      "action": "RUN",
      "priority": "CRITICAL",
      "reason": "Ready: 12 games today, 0.3h until first game",
      "scrapers": ["oddsa_events", "oddsa_player_props", "oddsa_game_lines"],
      "alert_level": "NONE"
    },
    {
      "workflow": "morning_operations",
      "action": "SKIP",
      "reason": "Already completed today at 06:15 ET",
      "scrapers": [],
      "alert_level": "NONE"
    }
  ]
}


BigQuery Records Created:

8 rows in workflow_decisions (one per workflow evaluated)

Decision Logic Examples:

RUN Decision:

Workflow: betting_lines
Time: 1:30 PM ET
First game: 7:00 PM ET (5.5h away)
Expected: Run 6h before games (1:00 PM Â± 30min window)
Decision: RUN (within window, games today, not yet run)


SKIP Decision:

Workflow: post_game_window_1
Time: 3:00 PM ET
Expected: 8:00 PM ET (Â±30min window)
Decision: SKIP (not in time window)
Next check: 8:00 PM ET


ABORT Decision:

Workflow: betting_lines
Time: 6:55 PM ET
First game: 7:00 PM ET (5 min away)
Expected: Run 6h before games
Decision: ABORT (missed window, too late to collect betting lines)
Alert: WARNING (workflow missed critical window)


Success Criteria:

âœ… Runs every hour 6 AM - 11 PM ET (18 runs daily)

âœ… Evaluates 5-8 workflows per run

âœ… Completes within 2 minutes (typical: 10-20 seconds)

Monitoring Query:

SELECT 
  EXTRACT(HOUR FROM decision_time AT TIME ZONE 'America/New_York') as hour_et,
  COUNT(*) as evaluations,
  COUNTIF(action = 'RUN') as run_decisions,
  COUNTIF(action = 'SKIP') as skip_decisions,
  COUNTIF(action = 'ABORT') as abort_decisions
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY hour_et
ORDER BY hour_et;


3. Cleanup Processor

Purpose: Self-healing - detect and recover from failed Pub/Sub deliveries

Trigger: Cloud Scheduler job cleanup-processor (Every 15 minutes)

Endpoint: POST /cleanup

What It Does:

Queries scraper_execution_log for recent GCS files

Checks if corresponding BigQuery records exist (Phase 2 processing)

If missing: Re-publishes Pub/Sub event to trigger Phase 2 processor

Tracks cleanup activity in cleanup_operations table

Example Output:

{
  "status": "success",
  "cleanup_result": {
    "cleanup_id": "uuid-here",
    "duration_seconds": 0.85,
    "files_checked": 0,
    "missing_files_found": 0,
    "republished_count": 0
  }
}


Example with Recovery:

{
  "cleanup_result": {
    "cleanup_id": "uuid-here",
    "duration_seconds": 2.34,
    "files_checked": 15,
    "missing_files_found": 2,
    "republished_count": 2,
    "recovered_files": [
      "gs://bucket/oddsa-events/2025-11-12/140530.json",
      "gs://bucket/bdl-games/2025-11-12/141022.json"
    ]
  }
}


Recovery Scenario:

1. Scraper runs successfully at 2:00 PM
2. Writes to GCS: gs://bucket/oddsa-events/2025-11-12/140000.json
3. Publishes Pub/Sub event
4. Pub/Sub delivery fails (network issue)
5. Phase 2 processor never triggered
6. Cleanup Processor runs at 2:30 PM (30 min later)
7. Detects: GCS file exists, no BigQuery record
8. Re-publishes Pub/Sub event
9. Phase 2 processor triggered successfully
10. Data appears in BigQuery within 5 minutes


Success Criteria:

âœ… Runs every 15 minutes (96 runs daily)

âœ… Completes within 5 seconds (typical: 1-2 seconds)

âœ… Missing files detected within 30-45 minutes

âœ… Recovery rate >99%

Monitoring Query:

SELECT 
  DATE(cleanup_time, 'America/New_York') as date,
  COUNT(*) as cleanup_runs,
  SUM(files_checked) as total_files_checked,
  SUM(missing_files_found) as total_missing,
  SUM(republished_count) as total_republished,
  AVG(duration_seconds) as avg_duration_sec
FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY date;


Alert Thresholds:

âš ï¸ WARNING: >5 missing files in single run

ðŸ”´ CRITICAL: >10 missing files in single run

ðŸ”´ CRITICAL: No cleanup runs for >30 minutes

Cloud Scheduler Jobs

Overview

Total Jobs: 4 (all active as of Nov 12, 2025)

Project: nba-props-platform
Region: us-west2
Timezone: America/New_York (ET)

List Jobs:

gcloud scheduler jobs list --location=us-west2


Job 1: daily-schedule-locker

Purpose: Generate daily workflow execution plan

Schedule:

Cron: 0 10 * * * (10 AM UTC = 5 AM ET)

Timezone: America/New_York

Frequency: Once daily at 5:00 AM ET

Target:

Service: nba-scrapers (Cloud Run)

Endpoint: POST /generate-daily-schedule

Timeout: 180 seconds (3 minutes)

Configuration:

Job Name: daily-schedule-locker
Location: us-west2
Schedule: 0 10 * * * (America/New_York)
URI: https://nba-scrapers-f7p3g7f6ya-wl.a.run.app/generate-daily-schedule
Method: POST
Body: {}
Auth: OIDC (scheduler-orchestration@nba-props-platform.iam.gserviceaccount.com)
Attempt Deadline: 180s
Retry Config:
  Max Attempts: 3
  Max Backoff: 3600s
  Min Backoff: 5s


Expected Behavior:

Runs automatically at 5:00 AM ET every day

Completes within 10-30 seconds

Generates 5-20 workflow entries in BigQuery

No manual intervention required

Manual Trigger:

gcloud scheduler jobs run daily-schedule-locker --location=us-west2


Verify Execution:

SELECT * FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')
ORDER BY expected_run_time;


Job 2: master-controller-hourly

Purpose: Evaluate workflow execution decisions

Schedule:

Cron: 0 6-23 * * * (Every hour from 6 AM to 11 PM ET)

Timezone: America/New_York

Frequency: 18 times daily (6 AM, 7 AM, ..., 11 PM)

Target:

Service: nba-scrapers (Cloud Run)

Endpoint: POST /evaluate

Timeout: 180 seconds (3 minutes)

Configuration:

Job Name: master-controller-hourly
Location: us-west2
Schedule: 0 6-23 * * * (America/New_York)
URI: https://nba-scrapers-f7p3g7f6ya-wl.a.run.app/evaluate
Method: POST
Body: {}
Auth: OIDC
Attempt Deadline: 180s


Expected Behavior:

Runs automatically every hour 6 AM - 11 PM ET

Completes within 10-20 seconds

Evaluates 5-8 workflows per run

Creates 5-8 BigQuery records per run (90-144 daily)

Manual Trigger:

gcloud scheduler jobs run master-controller-hourly --location=us-west2


Verify Execution:

SELECT 
  decision_time,
  workflow_name,
  action,
  reason
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY decision_time DESC
LIMIT 20;


Job 3: cleanup-processor

Purpose: Self-healing for failed Pub/Sub deliveries

Schedule:

Cron: */15 * * * * (Every 15 minutes)

Timezone: America/New_York

Frequency: 96 times daily (every 15 min Ã— 24 hours)

Target:

Service: nba-scrapers (Cloud Run)

Endpoint: POST /cleanup

Timeout: 180 seconds (3 minutes)

Configuration:

Job Name: cleanup-processor
Location: us-west2
Schedule: */15 * * * * (America/New_York)
URI: https://nba-scrapers-f7p3g7f6ya-wl.a.run.app/cleanup
Method: POST
Body: {}
Auth: OIDC
Attempt Deadline: 180s


Expected Behavior:

Runs automatically every 15 minutes

Completes within 1-3 seconds (typical)

Creates 1 BigQuery record per run (96 daily)

Usually finds 0 missing files (healthy system)

Manual Trigger:

gcloud scheduler jobs run cleanup-processor --location=us-west2


Verify Execution:

SELECT 
  cleanup_time,
  files_checked,
  missing_files_found,
  republished_count
FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY cleanup_time DESC
LIMIT 10;


Job 4: execute-workflows

Purpose: Execute pending workflows (read RUN decisions and trigger scrapers)

Schedule:

Cron: 5 6-23 * * * (5 minutes past every hour, 6 AM-11 PM ET)

Timezone: America/New_York

Frequency: 18 times daily (at :05 each hour)

Target:

Service: nba-scrapers (Cloud Run)

Endpoint: POST /execute-workflows

Timeout: 180 seconds (3 minutes)

Configuration:

Job Name: execute-workflows
Location: us-west2
Schedule: 5 6-23 * * * (America/New_York)
URI: https://nba-scrapers-f7p3g7f6ya-wl.a.run.app/execute-workflows
Method: POST
Body: {}
Auth: OIDC (scheduler-orchestration@nba-props-platform.iam.gserviceaccount.com)
Attempt Deadline: 180s
Retry Config:
  Max Attempts: 3
  Max Backoff: 3600s
  Min Backoff: 5s


Expected Behavior:

Runs automatically 5 minutes after master-controller

Reads RUN decisions from workflow_decisions table

Resolves parameters for each scraper

Calls scrapers via HTTP: POST /scrape

Tracks execution in workflow_executions table

Completes within 30-60 seconds typically

Manual Trigger:

gcloud scheduler jobs run execute-workflows --location=us-west2


Verify Execution:

SELECT 
  workflow_name,
  status,
  scrapers_triggered,
  scrapers_succeeded,
  scrapers_failed,
  duration_seconds
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY execution_time DESC;


Link to Decision:

-- Trace decision â†’ execution â†’ scraper runs
SELECT 
  d.workflow_name,
  d.action,
  d.decision_time,
  e.execution_time,
  e.status,
  e.scrapers_succeeded,
  e.scrapers_failed
FROM `nba-props-platform.nba_orchestration.workflow_decisions` d
LEFT JOIN `nba-props-platform.nba_orchestration.workflow_executions` e
  ON d.decision_id = e.decision_id
WHERE DATE(d.decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
  AND d.action = 'RUN'
ORDER BY d.decision_time DESC;


Managing Scheduler Jobs

Pause All Jobs (Maintenance):

for job in daily-schedule-locker master-controller-hourly cleanup-processor; do
  gcloud scheduler jobs pause $job --location=us-west2
done


Resume All Jobs:

for job in daily-schedule-locker master-controller-hourly cleanup-processor; do
  gcloud scheduler jobs resume $job --location=us-west2
done


Check Job Status:

gcloud scheduler jobs describe daily-schedule-locker --location=us-west2


View Recent Executions:

gcloud logging read \
  "resource.type=cloud_scheduler_job AND resource.labels.job_id=daily-schedule-locker" \
  --limit=10 \
  --format="table(timestamp,httpRequest.status,textPayload)"


BigQuery Tables

Dataset: nba_orchestration

Location: nba-props-platform.nba_orchestration

Tables: 5 operational tables

Purpose: Complete audit trail of orchestration decisions and operations

Table 1: daily_expected_schedule

Purpose: Daily plan of what workflows SHOULD run

Populated By: Schedule Locker (5 AM ET daily)

Schema:

CREATE TABLE `nba-props-platform.nba_orchestration.daily_expected_schedule` (
  date DATE,                          -- Date this schedule is for
  locked_at TIMESTAMP,                -- When schedule was generated
  workflow_name STRING,               -- Name of workflow
  expected_run_time TIMESTAMP,        -- When workflow should execute
  reason STRING,                      -- Why scheduled (human-readable)
  scrapers ARRAY<STRING>              -- List of scrapers to run
)
PARTITION BY date
CLUSTER BY workflow_name;


Example Data:

date: '2025-11-12'
locked_at: '2025-11-12T10:00:15Z'
workflow_name: 'betting_lines'
expected_run_time: '2025-11-12T13:30:00-05:00'
reason: 'Pre-game betting lines (12 games today)'
scrapers: ['oddsa_events', 'oddsa_player_props', 'oddsa_game_lines']


Typical Row Count:

5-20 rows per day (one per workflow instance)

Empty on days with no games

Monitoring Queries:

Daily Summary:

SELECT 
  date,
  COUNT(*) as workflows_scheduled,
  COUNT(DISTINCT workflow_name) as unique_workflows,
  MIN(expected_run_time) as first_run,
  MAX(expected_run_time) as last_run
FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')
GROUP BY date;


Workflow Breakdown:

SELECT 
  workflow_name,
  COUNT(*) as scheduled_count,
  MIN(expected_run_time) as earliest,
  MAX(expected_run_time) as latest
FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')
GROUP BY workflow_name
ORDER BY earliest;


Table 2: workflow_decisions

Purpose: Log every workflow evaluation decision (RUN/SKIP/ABORT)

Populated By: Master Controller (hourly 6-11 PM ET)

Schema:

CREATE TABLE `nba-props-platform.nba_orchestration.workflow_decisions` (
  decision_time TIMESTAMP,            -- When decision was made
  workflow_name STRING,               -- Which workflow
  action STRING,                      -- RUN, SKIP, or ABORT
  reason STRING,                      -- Why this decision
  priority STRING,                    -- CRITICAL, HIGH, MEDIUM, LOW
  alert_level STRING,                 -- NONE, INFO, WARNING, ERROR
  next_check TIMESTAMP,               -- When to check again (for SKIP)
  scrapers ARRAY<STRING>              -- Scrapers for RUN actions
)
PARTITION BY DATE(decision_time)
CLUSTER BY workflow_name, action;


Example Data:

RUN Decision:

decision_time: '2025-11-12T13:30:15-05:00'
workflow_name: 'betting_lines'
action: 'RUN'
reason: 'Ready: 12 games today, 5.5h until first game'
priority: 'CRITICAL'
alert_level: 'NONE'
next_check: NULL
scrapers: ['oddsa_events', 'oddsa_player_props', 'oddsa_game_lines']


SKIP Decision:

decision_time: '2025-11-12T15:00:10-05:00'
workflow_name: 'post_game_window_1'
action: 'SKIP'
reason: 'Not in time window (20:00 Â±30min)'
priority: 'HIGH'
alert_level: 'NONE'
next_check: '2025-11-12T20:00:00-05:00'
scrapers: []


ABORT Decision:

decision_time: '2025-11-12T18:55:00-05:00'
workflow_name: 'betting_lines'
action: 'ABORT'
reason: 'Missed window (first game in 5 min, needed 6h lead time)'
priority: 'CRITICAL'
alert_level: 'WARNING'
next_check: NULL
scrapers: []


Typical Row Count:

90-144 rows per day (5-8 workflows Ã— 18 hourly evaluations)

Peak during game days

Monitoring Queries:

Action Summary:

SELECT 
  action,
  COUNT(*) as count,
  ROUND(COUNT(*) / SUM(COUNT(*)) OVER() * 100, 1) as pct
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY action;


Alert Summary:

SELECT 
  alert_level,
  workflow_name,
  COUNT(*) as count
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
  AND alert_level IN ('WARNING', 'ERROR')
GROUP BY alert_level, workflow_name
ORDER BY alert_level DESC, count DESC;


Hourly Pattern:

SELECT 
  EXTRACT(HOUR FROM decision_time AT TIME ZONE 'America/New_York') as hour_et,
  COUNT(*) as evaluations,
  COUNTIF(action = 'RUN') as run_count,
  COUNTIF(action = 'SKIP') as skip_count,
  COUNTIF(action = 'ABORT') as abort_count
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY hour_et
ORDER BY hour_et;


Table 3: cleanup_operations

Purpose: Track self-healing activity (orphaned file detection and recovery)

Populated By: Cleanup Processor (every 15 minutes)

Schema:

CREATE TABLE `nba-props-platform.nba_orchestration.cleanup_operations` (
  cleanup_time TIMESTAMP,             -- When cleanup ran
  cleanup_id STRING,                  -- Unique UUID for this operation
  files_checked INT64,                -- Files examined
  missing_files_found INT64,          -- Files without BigQuery records
  republished_count INT64,            -- Pub/Sub events republished
  duration_seconds FLOAT64            -- Processing time
)
PARTITION BY DATE(cleanup_time)
CLUSTER BY DATE(cleanup_time);


Example Data (Normal):

cleanup_time: '2025-11-12T15:15:00-05:00'
cleanup_id: 'uuid-here'
files_checked: 0
missing_files_found: 0
republished_count: 0
duration_seconds: 0.85


Example Data (Recovery):

cleanup_time: '2025-11-12T14:30:00-05:00'
cleanup_id: 'uuid-here'
files_checked: 12
missing_files_found: 2
republished_count: 2
duration_seconds: 2.15


Typical Row Count:

96 rows per day (every 15 min Ã— 24 hours)

Most rows have 0 missing files (healthy)

Monitoring Queries:

Daily Summary:

SELECT 
  DATE(cleanup_time, 'America/New_York') as date,
  COUNT(*) as cleanup_runs,
  SUM(files_checked) as total_files_checked,
  SUM(missing_files_found) as total_missing,
  SUM(republished_count) as total_republished,
  AVG(duration_seconds) as avg_duration_sec,
  MAX(duration_seconds) as max_duration_sec
FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY date;


Alert: High Missing File Count:

SELECT 
  cleanup_time,
  files_checked,
  missing_files_found,
  republished_count
FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
  AND missing_files_found > 5  -- Alert threshold
ORDER BY cleanup_time DESC;


Table 4: scraper_execution_log

Purpose: Log every scraper execution (when scrapers actually run)

Populated By: Scrapers (via scraper_base.py)

Schema:

CREATE TABLE `nba-props-platform.nba_orchestration.scraper_execution_log` (
  triggered_at TIMESTAMP,             -- When scraper started
  execution_id STRING,                -- Unique UUID
  scraper_name STRING,                -- Which scraper
  status STRING,                      -- SUCCESS or FAILED
  duration_seconds FLOAT64,           -- Execution time
  records_processed INT64,            -- Data collected
  gcs_path STRING,                    -- Output file location
  error_message STRING                -- If FAILED, what went wrong
)
PARTITION BY DATE(triggered_at)
CLUSTER BY scraper_name, status;


Current State: This table is empty because scrapers aren't being triggered automatically yet (Week 2-3).

Future State: When Workflow Executor is deployed, this table will track all scraper executions.

Example Future Data:

triggered_at: '2025-11-12T13:30:45-05:00'
execution_id: 'uuid-here'
scraper_name: 'oddsa_events'
status: 'SUCCESS'
duration_seconds: 28.5
records_processed: 12
gcs_path: 'gs://nba-scraped-data/oddsa-events/2025-11-12/133045.json'
error_message: NULL


Future Monitoring Queries:

Scraper Success Rate:

SELECT 
  scraper_name,
  COUNT(*) as total_runs,
  COUNTIF(status = 'SUCCESS') as successful,
  COUNTIF(status = 'FAILED') as failed,
  ROUND(COUNTIF(status = 'SUCCESS') / COUNT(*) * 100, 1) as success_rate_pct
FROM `nba-props-platform.nba_orchestration.scraper_execution_log`
WHERE DATE(triggered_at, 'America/New_York') >= CURRENT_DATE('America/New_York') - 7
GROUP BY scraper_name
ORDER BY success_rate_pct ASC, total_runs DESC;


Table 5: workflow_executions

Purpose: Track workflow execution attempts and link decision â†’ execution â†’ scraper runs

Populated By: Workflow Executor (5 min after master controller hourly)

Schema:

CREATE TABLE `nba-props-platform.nba_orchestration.workflow_executions` (
  execution_id STRING NOT NULL,        -- Unique UUID
  execution_time TIMESTAMP NOT NULL,   -- When execution started (partition key)
  workflow_name STRING NOT NULL,       -- Which workflow
  decision_id STRING,                  -- Links to workflow_decisions.decision_id
  
  -- Scraper tracking
  scrapers_requested ARRAY<STRING> NOT NULL,  -- Scrapers to execute
  scrapers_triggered INT64 NOT NULL,          -- Number attempted
  scrapers_succeeded INT64 NOT NULL,          -- Number succeeded
  scrapers_failed INT64 NOT NULL,             -- Number failed
  scraper_execution_ids ARRAY<STRING>,        -- Links to scraper_execution_log
  
  -- Status
  status STRING NOT NULL,              -- completed, failed
  duration_seconds FLOAT64,            -- Execution time
  error_message STRING                 -- If failed, why
)
PARTITION BY DATE(execution_time)
CLUSTER BY workflow_name, status
OPTIONS(
  partition_expiration_days = 90
);


Example Data:

execution_id: 'uuid-abc-123'
execution_time: '2025-11-12T18:05:30-05:00'
workflow_name: 'betting_lines'
decision_id: 'uuid-def-456'
scrapers_requested: ['oddsa_events', 'oddsa_player_props', 'oddsa_game_lines']
scrapers_triggered: 3
scrapers_succeeded: 3
scrapers_failed: 0
scraper_execution_ids: ['exec-001', 'exec-002', 'exec-003']
status: 'completed'
duration_seconds: 12.5
error_message: NULL


Typical Row Count:

0-20 rows per day (one per workflow execution)

Only workflows with RUN decisions create rows

Monitoring Queries:

Daily Summary:

SELECT 
  workflow_name,
  status,
  COUNT(*) as executions,
  AVG(duration_seconds) as avg_duration,
  SUM(scrapers_triggered) as total_scrapers,
  SUM(scrapers_succeeded) as total_succeeded,
  SUM(scrapers_failed) as total_failed
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY workflow_name, status;


Execution Success Rate:

SELECT 
  workflow_name,
  COUNT(*) as total_executions,
  COUNTIF(status = 'completed') as successful,
  COUNTIF(status = 'failed') as failed,
  ROUND(COUNTIF(status = 'completed') / COUNT(*) * 100, 1) as success_rate_pct
FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time, 'America/New_York') >= CURRENT_DATE('America/New_York') - 7
GROUP BY workflow_name
ORDER BY success_rate_pct ASC;


End-to-End Flow (Decision â†’ Execution â†’ Scrapers):

WITH flow AS (
  SELECT 
    d.workflow_name,
    d.decision_time,
    d.action,
    e.execution_time,
    e.status as execution_status,
    e.scrapers_triggered,
    e.scrapers_succeeded,
    e.scraper_execution_ids
  FROM `nba-props-platform.nba_orchestration.workflow_decisions` d
  LEFT JOIN `nba-props-platform.nba_orchestration.workflow_executions` e
    ON d.decision_id = e.decision_id
  WHERE DATE(d.decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
    AND d.action = 'RUN'
)
SELECT 
  f.workflow_name,
  f.decision_time,
  f.execution_time,
  f.execution_status,
  f.scrapers_triggered,
  f.scrapers_succeeded,
  COUNT(s.execution_id) as scraper_runs_logged
FROM flow f
LEFT JOIN `nba-props-platform.nba_orchestration.scraper_execution_log` s
  ON s.execution_id IN UNNEST(f.scraper_execution_ids)
GROUP BY 1,2,3,4,5,6
ORDER BY f.decision_time DESC;


Orchestration Endpoints

Cloud Run Service Details

Service Name: nba-scrapers
Region: us-west2
URL: https://nba-scrapers-f7p3g7f6ya-wl.a.run.app
Auth: Requires OIDC token (Cloud Scheduler has permission)

Get Service URL:

SERVICE_URL=$(gcloud run services describe nba-scrapers \
  --region=us-west2 \
  --format="value(status.url)")
echo $SERVICE_URL


Endpoint 1: Health Check

Path: GET /health

Purpose: Verify service is running and orchestration components loaded

Auth: Public (no auth required)

Request:

curl -s "${SERVICE_URL}/health" | jq '.'


Response:

{
  "status": "healthy",
  "service": "nba-scrapers",
  "version": "2.2.3",
  "deployment": "orchestration-enabled",
  "timestamp": "2025-11-12T23:16:55.258911+00:00",
  "components": {
    "scrapers": {
      "available": 33,
      "status": "operational"
    },
    "orchestration": {
      "master_controller": "available",
      "schedule_locker": "available",
      "workflow_executor": "available",
      "cleanup_processor": "available",
      "enabled_workflows": 7,
      "workflows": [
        "morning_operations",
        "betting_lines",
        "post_game_window_1",
        "post_game_window_2",
        "post_game_window_3",
        "injury_discovery",
        "referee_discovery"
      ]
    }
  }
}


Endpoint 2: Generate Daily Schedule

Path: POST /generate-daily-schedule

Purpose: Generate daily workflow plan (called by Schedule Locker)

Auth: OIDC (Cloud Scheduler service account)

Trigger: Automatically at 5 AM ET, or manually

Request:

# Manual trigger (requires auth)
TOKEN=$(gcloud auth print-identity-token)
curl -s -X POST "${SERVICE_URL}/generate-daily-schedule" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


Response:

{
  "status": "success",
  "schedule": {
    "date": "2025-11-12",
    "games_scheduled": 12,
    "workflows_evaluated": 7,
    "expected_runs": 19,
    "locked_at": "2025-11-12T10:00:15.123456"
  }
}


What Happens:

Reads game schedule for today

Reads config/workflows.yaml

Calculates expected run times

Writes 5-20 rows to daily_expected_schedule

Verify:

SELECT * FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')
ORDER BY expected_run_time;


Endpoint 3: Evaluate Workflows

Path: POST /evaluate

Purpose: Make RUN/SKIP/ABORT decisions (called by Master Controller)

Auth: OIDC (Cloud Scheduler service account)

Trigger: Automatically every hour 6-11 PM ET, or manually

Request:

# Manual trigger
TOKEN=$(gcloud auth print-identity-token)
curl -s -X POST "${SERVICE_URL}/evaluate" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


Response:

{
  "status": "success",
  "evaluation_time": "2025-11-12T18:30:15-05:00",
  "workflows_evaluated": 8,
  "decisions": [
    {
      "workflow": "betting_lines",
      "action": "RUN",
      "priority": "CRITICAL",
      "reason": "Ready: 12 games today, 0.3h until first game",
      "scrapers": ["oddsa_events", "oddsa_player_props", "oddsa_game_lines"],
      "alert_level": "NONE",
      "next_check": null
    },
    {
      "workflow": "morning_operations",
      "action": "SKIP",
      "reason": "Already completed today at 06:15 ET",
      "scrapers": [],
      "alert_level": "NONE",
      "next_check": null
    }
  ]
}


What Happens:

Reads daily_expected_schedule for today

For each workflow:

Check if within time window

Check if already ran

Check dependencies

Make decision

Writes 5-8 rows to workflow_decisions

Verify:

SELECT * FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY decision_time DESC
LIMIT 10;


Endpoint 4: Run Cleanup

Path: POST /cleanup

Purpose: Self-healing - detect orphaned files (called by Cleanup Processor)

Auth: OIDC (Cloud Scheduler service account)

Trigger: Automatically every 15 minutes, or manually

Request:

# Manual trigger
TOKEN=$(gcloud auth print-identity-token)
curl -s -X POST "${SERVICE_URL}/cleanup" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


Response:

{
  "status": "success",
  "cleanup_result": {
    "cleanup_id": "uuid-here",
    "duration_seconds": 0.85,
    "files_checked": 0,
    "missing_files_found": 0,
    "republished_count": 0
  }
}


What Happens:

Queries scraper_execution_log for recent files

Checks if BigQuery records exist (Phase 2)

If missing: Re-publishes Pub/Sub event

Writes 1 row to cleanup_operations

Verify:

SELECT * FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY cleanup_time DESC
LIMIT 5;


Endpoint 5: Execute Workflows

Path: POST /execute-workflows

Purpose: Execute pending workflows (called by Workflow Executor)

Auth: OIDC (Cloud Scheduler service account)

Trigger: Automatically at :05 every hour (5 min after /evaluate), or manually

Request:

# Manual trigger (requires auth)
TOKEN=$(gcloud auth print-identity-token)
curl -s -X POST "${SERVICE_URL}/execute-workflows" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


Response:

{
  "status": "success",
  "execution_result": {
    "workflows_executed": 2,
    "succeeded": 2,
    "failed": 0,
    "duration_seconds": 15.2,
    "executions": [
      {
        "execution_id": "uuid-abc",
        "workflow_name": "betting_lines",
        "status": "completed",
        "scrapers_triggered": 3,
        "scrapers_succeeded": 3,
        "scrapers_failed": 0,
        "duration_seconds": 12.5
      },
      {
        "execution_id": "uuid-def",
        "workflow_name": "injury_discovery",
        "status": "completed",
        "scrapers_triggered": 1,
        "scrapers_succeeded": 1,
        "scrapers_failed": 0,
        "duration_seconds": 2.7
      }
    ]
  }
}


What Happens:

Reads workflow_decisions table for recent RUN decisions

For each RUN decision:

Resolves parameters (season, date, game_ids)

Calls scrapers via HTTP: POST /scrape

Tracks execution status

Writes execution records to workflow_executions table

Verify:

SELECT * FROM `nba-props-platform.nba_orchestration.workflow_executions`
WHERE DATE(execution_time, 'America/New_York') = CURRENT_DATE('America/New_York')
ORDER BY execution_time DESC;


Monitoring & Health Checks

Daily Health Check (Run at 9 AM ET)

Quick Health Check Script:

#!/bin/bash
# check_orchestration_health.sh

echo "=== Orchestration Health Check ==="
echo "Date: $(date)"
echo ""

# 1. Check if schedule was generated today
echo "1. Schedule Generation (5 AM ET):"
bq query --use_legacy_sql=false --format=prettyjson "
SELECT 
  date,
  locked_at,
  COUNT(*) as workflows_scheduled
FROM \`nba-props-platform.nba_orchestration.daily_expected_schedule\`
WHERE date = CURRENT_DATE('America/New_York')
GROUP BY date, locked_at
" | jq -r '.[] | "âœ… \(.workflows_scheduled) workflows scheduled at \(.locked_at)"'

# 2. Check workflow evaluations
echo ""
echo "2. Workflow Evaluations (6 AM-11 PM ET hourly):"
bq query --use_legacy_sql=false --format=prettyjson "
SELECT 
  COUNT(DISTINCT EXTRACT(HOUR FROM decision_time AT TIME ZONE 'America/New_York')) as hours_evaluated,
  COUNT) as total_decisions,
  COUNTIF(action = 'RUN') as run_decisions,
  COUNTIF(action = 'SKIP') as skip_decisions,
  COUNTIF(action = 'ABORT') as abort_decisions
FROM \`nba-props-platform.nba_orchestration.workflow_decisions\`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
" | jq -r '.[] | "âœ… \(.hours_evaluated) hours evaluated, \(.total_decisions) decisions (\(.run_decisions) RUN, \(.skip_decisions) SKIP, \(.abort_decisions) ABORT)"'

# 3. Check cleanup operations
echo ""
eo "3. Cleanup Operations (Every 15 min):"
bq query --use_legacy_sql=false --format=prettyjson "
SELECT 
  COUNT(*) as cleanup_runs,
  SUM(missing_files_found) as total_missing,
  SUM(republished_count) as total_republished
FROM \`nba-props-platform.nba_orchestration.cleanup_operations\`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
" | jq -r '.[] | "âœ… \(.cleanup_runs) cleanup runs, \(.total_missing) missing files found, \(.total_republished) events republished"'

# 4. Che for alerts
echo ""
echo "4. Alerts (WARNING or ERROR):"
bq query --use_legacy_sql=false --format=prettyjson "
SELECT 
  alert_level,
  workflow_name,
  COUNT(*) as count
FROM \`nba-props-platform.nba_orchestration.workflow_decisions\`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
  AND alert_level IN ('WARNING', 'ERROR')
GROUP BY alert_level, workflow_name
ORDER BY alert_level DESC
" | jq -r '.[] | "âš ï¸  \(.alert_level): \(.workflow_name) (\(.count)x)"'

echo ""
echo "Health Check Complete ==="


Run:

chmod +x check_orchestration_health.sh
./check_orchestration_health.sh


Monitoring Dashboards (Week 2 - Grafana)

Dashboard 1: Orchestration Overview

Panels:

Schedule Generation Status (single stat)

Hourly Evaluations (time series)

Cleanup Operations (time series)

Workflow Actions (pie chart)

Alert Summary (table)

Query Examples:

Schedule Generated Today:

SELECT COUNT(*) as schedules_today
FROM `nba-props-platform.nba_orchestration.daily_expected_schedule`
WHERE date = CURRENT_DATE('America/New_York')


Hourly Evaluation Count:

SELECT 
  TIMESTAMP_TRUNC(decision_time, HOUR) as hour,
  COUNT(*) as evaluations
FROM `nba-props-platform.nba_orchestration.workflow_decisions`
WHERE DATE(decision_time, 'America/New_York') = CURRENT_DATE('America/New_York')
GROUP BY hour
ORDER BY hour


Alert Thresholds

Critical Alerts (Immediate Action):

Schedule Not Generated

Condition: No records in daily_expected_schedule after 5:30 AM ET

Action: Check schedule locker job

Master Controller Not Running

Condition: No decisions for >2 hours during 6 AM-11 PM ET

Action: Check master-controller-hourly job

Cleanup Processor Stuck

Condition: No cleanup operations for >30 minutes

Action: Check cleanup-processor job

Warning Alerts (Monitor Closely):

High Missing File Count

Condition: >5 missing files in single cleanup run

Action: Investigate scraper or Pub/Sub issues

Workflow Abort Actions

Condition: Any ABORT action with alert_level='WARNING'

Action: Review workflow timing windows

Manual Operations

Test Orchestration System

1. Generate Schedule Manually:

SERVICE_URL=$(gcloud run services describe nba-scrapers \
  --region=us-west2 --format="value(status.url)")

TOKEN=$(gcloud auth print-identity-token)

curl -s -X POST "${SERVICE_URL}/generate-daily-schedule" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


2. Evaluate Workflows Manually:

curl -s -X POST "${SERVICE_URL}/evaluate" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


3. Run Cleanup Manually:

curl -s -X POST "${SERVICE_URL}/cleanup" \
  -H "Authorization: Bearer $TOKEN" | jq '.'


View Orchestration Logs

Schedule Locker Logs:

gcloud logging read \
  "resource.type=cloud_run_revision 
   AND resource.labels.service_name=nba-scrapers
   AND textPayload=~'Schedule'" \
  --limit=20 \
  --format="table(timestamp,severity,textPayload)"


Master Controller Logs:

gcloud logging read \
  "resource.type=cloud_run_revision 
   AND resource.labels.service_name=nba-scrapers
   AND textPayload=~'Decision'" \
  --limit=50 \
  --format="table(timestamp,severity,textPayload)"


Cleanup Processor Logs:

gcloud logging read \
  "resource.type=cloud_run_revision 
   AND resource.labels.service_name=nba-scrapers
   AND textPayload=~'Cleanup'" \
  --limit=20 \
  --format="table(timestamp,severity,textPayload)"


Pause/Resume Orchestration

Pause All Orchestration (Emergency):

for job in daily-schedule-locker master-controller-hourly cleanup-processor; do
  gcloud scheduler jobs pause $job --location=us-west2
  echo "Paused: $job"
done


Resume All Orchestration:

for job in daily-schedule-locker master-controller-hourly cleanup-processor; do
  gcloud scheduler jobs resume $job --location=us-west2
  echo "Resumed: $job"
done


Manual Scraper Trigger (Still Works)

Scrapers can still be triggered manually (not automatically yet):

# Trigger single scraper
curl -s -X POST "${SERVICE_URL}/scraper/oddsa-events" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"date": "2025-11-12"}' | jq '.'

# Trigger another scraper
curl -s -X POST "${SERVICE_URL}/scraper/nbac-player-list" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"season": "2024-25"}' | jq '.'


Phase 1 Status: âœ… COMPLETE

What's Deployed and Working (November 12, 2025)

All Phase 1 components are operational:

âœ… Schedule Locker - Generates daily plan (5 AM ET)

âœ… Master Controller - Makes RUN/SKIP/ABORT decisions (hourly 6-11 PM ET)

âœ… Workflow Executor - Executes scrapers based on decisions (5 min aftoller)

âœ… Cleanup Processor - Self-healing for missed events (every 15 min)

âœ… 5 BigQuery Tables - Complete audit trail

âœ… 4 Cloud Scheduler Jobs - Fully automated orchestration

The system is now FULLY AUTOMATED:

Decisions made hourly (6 AM - 11 PM ET)

Scrapers triggered automatically 5 minutes later

Complete end-to-end tracking

No manual intervention required

Architecture Achievement:

Decision â†’ Execution â†’ Scraper Runs â†’ Data Collection
     âœ…         âœ…            âœ…              Ã¢hat's Next (Future Enhancements)

Phase 2: Per-Game Iteration (Week 3-4)

Current Limitation:

Game-specific scrapers (play-by-play) only run for first game

Need: Iterate over all games, parallel execution

Enhancement:

Update parameter resolver to return list of games

Execute scraper once per game

Track per-game execution status

Timeline: 1-2 days implementation

Phase 3: Async Execution (Future)

Current Implementation:

Synchronous HTTP calls (one scraper at a time)

Simple but slower for multiple scrapers

Enhancement:

Async HTTP calls for parallel execution

Better performance for workflows with many scrapers

Timeline: Not yet prioritized

Phase 4: Pub/Sub Pipeline Verification (Week 3-4)

Current Status:

Scrapers may or may not publish Pub/Sub events

Phase 2 processors may or may not be deployed

End-to-end flow NOT verified

Tasks:

Audit: Verify all scrapers publish Pub/Sub events

Testing: End-to-end Phase 1 â†’ Phase 2 flow

Recovery: Ensure cleanup processor works correctly

Timeline: 4-hours audit + potential fixes

See: HANDOFF_PUBSUB_PHASE2.md for complete checklist

Next Steps (Week 3-4)

Priority 1: Verify Production Operation (This Week)

Goal: Confirm Phase 1 works end-to-end in production

Tasks:

âœ… Monitor first 48 hours of automated operation

âœ… Verify workflow_executions table populating correctly

âœ… Check scraper execution success rates

âœ… Validate cleanup processor recovery

Time: 2-4 hours monitoring + adjustments

Value: Confidence in production automation

Monitorinds:

# Check today's executions
bq query "SELECT workflow_name, status, COUNT(*) FROM 
nba_orchestration.workflow_executions WHERE 
DATE(execution_time, 'America/New_York') = CURRENT_DATE('America/New_York') 
GROUP BY 1,2"

# Check success rate
bq query "SELECT ROUND(COUNTIF(status='completed')/COUNT(*)*100,1) 
as success_rate FROM nba_orchestration.workflow_executions 
WHERE DATE(execution_time, 'America/New_York') = CURRENT_DATE('America/New_York')"


Priority 2: Grafana Monitoring (Week 3)

Goal: Visibility into orchestration system

Tasks:

Connect Grafana to BigQuery

Create "Orchestration Health" dashboard

Set up alerts for failures

Time: 2-3 hours

Value: Proactive issue detection

See: HANDOFF_GRAFANA_MONITORING.md for setup guide

Priority 3: Per-Game Iteration (Week 3-4)

Goal: Handle game-specific scrapers properly

Current: Game-specific scrapers only run for first game
Target: Execute once per game with proper parameters

Tasks:

Update parameter resolver for game lists

Implement per-game iteration logic

Test with play-by-play scrapers

Time: 1-2 days

Value: Complete game coverage

Example Change:

# Current (only first game)
params = {"game_id": games[0].game_id}

# Target (all games)
for game in games:
    params = {"game_id": game.game_id}
    call_scraper(scraper_name, params)


Priority 4: Pub/Sub Phase 2 Audit (Week 4)

Goal: Verify end-to-end data pipeline

Tasks:

Audit: Do scrapers publish Pub/Sub events?

Verify: Are Phase 2 processors deployed?

Test: Complete flow from scraper to BigQuery

Time: 4-6 hours

Value: Complete pipeline confidence

See: HANDOFF_PUBSUB_PHASE2.md for audit checklist

Troubleshooting

Issue: Schedule Not Generated at 5 AM

Symptoms:

No records in daily_expected_schedule for today

Master Controller has no plan to evaluate

Diagnosis:

# Check if job ran
gcloud logging read \
  "resource.type=cloud_scheduler_job 
   AND resource.labels.job_id=daily-schedule-locker" \
  --limit=5 \
  --format="table(timestamp,httpRequest.status)"

# Check Cloud Run logs
gcloud logging read \
  "resource.type=cloud_run_revision 
   AND resource.labels.service_name=nba-scrapers
   AND timestamp>=DATE_SUB(CURRENT_TIMESTAMP(), INTERVAL 2 HOUR)" \
  --limit=50


Fix:

# Manually trigger schedule generation
TOKEN=$(gcloud auth print-identity-token)
SERVICE_URL=$(gcloud run services describe nba-scrapers \
  --region=us-west2 --format="value(status.url)")

curl -X POST "${SERVICE_URL}/generate-daily-schedule" \
  -H "Authorization: Bearer $TOKEN"

# Verify it worked
bq query "SELECT COUNT(*) FROM nba_orchestration.daily_expected_schedule 
WHERE date = CURRENT_DATE('America/New_York')"


Issue: Master Controller Not Running Hourly

Symptoms:

No new decisions in last 2+ hours

Gap in workflow_decisions table

Diagnosis:

# Check last decision time
bq query "SELECT MAX(decision_time) as last_decision 
FROM nba_orchestration.workflow_decisions"

# Check if job exists and is enabled
gcloud scheduler jobs describe master-controller-hourly --location=us-west2


Fix:

# If paused, resume
gcloud scheduler jobs resume master-controller-hourly --location=us-west2

# If not paused, manually trigger
gcloud scheduler jobs run master-controller-hourly --location=us-west2


Issue: High Missing File Count in Cleanup

Symptoms:

missing_files_found > 5 in cleanup operations

Indicates scraper failures or Pub/Sub issues

Diagnosis:

-- Find which files are missing
SELECT 
  cleanup_time,
  missing_files_found,
  republished_count
FROM `nba-props-platform.nba_orchestration.cleanup_operations`
WHERE DATE(cleanup_time, 'America/New_York') = CURRENT_DATE('America/New_York')
  AND missing_files_found > 0
ORDER BY cleanup_time DESC;


Fix:

Investigate why scrapers are failing

Check Pub/Sub subscription health

Verify Phase 2 processors are running

Cleanup processor will auto-recover within 45 minutes

Issue: Cloud Run Service Unresponsive

Symptoms:

Scheduler jobs timing out

504 Gateway Timeout errors

Diagnosis:

# Check service health
SERVICE_URL=$(gcloud run services describe nba-scrapers \
  --region=us-west2 --format="value(status.url)")

curl -s "${SERVICE_URL}/health"

# Check recent deployments
gcloud run revisions list --service=nba-scrapers --region=us-west2 --limit=5


Fix:

# If recent bad deployment, rollback
gcloud run services update-traffic nba-scrapers \
  --to-revisions=nba-scrapers-00062-424=100 \
  --region=us-west2

# If service down, check logs and redeploy
./bin/scrapers/deploy/deploy_scrapers_simple.sh


Document Status

Version: 3.0
Status: Current and Accurate as of November 12, 2025
Last Verified: November 12, 2025 - Phase 1 Complete
Next Review: After per-game iteration implementation (Week 3-4)

Related Documentation

Current State (This Document):

âœ… Phase 1 Orchestration Current State (this doc)

Future Architecture:

ðŸ“„ HANDOFF_WORKFLOW_EXECUTOR.md - How to build execution layer

ðŸ“„ HANDOFF_PUBSUB_PHASE2.md - Phase 2 integration verification

ðŸ“„ HANDOFF_GRAFANA_MONITORING.md - Monitoring setup guide

Original Planning:

ðŸ“„ phase1_orchestration_scheduling_guide.md (Nov 8) -chitecture

Summary

Phase 1 Status: âœ… COMPLETE (November 12, 2025)

What Works:

âœ… Intelligent workflow decision-making (hourly 6 AM - 11 PM ET)

âœ… Automatic scraper triggering (5 minutes after decisions)

âœ… Complete audit trail in BigQuery (5 tables)

âœ… Self-healing for failures (cleanup every 15 min)

âœ… Fully automated via Cloud Scheduler (4 jobs)

âœ… End-to-end tracking: decision â†’ execution â†’ results

What's Different from Nov 8 Plan:

âœ… 5-minute delay between evaluation and executioetter than immediate)

âœ… HTTP-based scraper calling (better isolation than direct Python)

âœ… Separate workflow_executions table (clearer audit trail)

âœ… 4 scheduler jobs instead of 3 (separate execution job)

Production Readiness: The orchestration system is production-ready and operational. The system makes intelligent decisions about when to scrape AND automatically executes those decisions. No manual intervention required for daily operations.

Key Achievement:

Decision â†’ Execution â†’ Scraper R Collection
     âœ…         âœ…            âœ…              âœ…


Next Phase: Focus shifts to monitoring (Grafana dashboards), optimization (per-game iteration, async execution), and Phase 2 integration verification (Pub/Sub pipeline audit).

ðŸŽ‰ Phase 1 Orchestration: MISSION ACCOMPLISHED

The NBA Props Platform orchestration layer is complete and running autonomously. The system now handles:

Daily workflow planning (5 AM ET)

Hourly decision-making (6 AM - 11 PM ET)

Automatic scraper execution

Self-hovery

Complete operational tracking

No manual intervention required for daily data collection operations.

End of Document
