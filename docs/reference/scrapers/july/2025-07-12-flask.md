# Flask Integration for NBA Scrapers

## Overview

This document describes the Flask integration solution that enables NBA scrapers to run as both CLI tools and Cloud Run web services with minimal code changes.

## Problem Statement

The NBA scraper project needed to:
- Deploy scrapers to Cloud Run for scalable, event-driven processing
- Maintain existing CLI development workflow
- Provide consistent REST API across all scrapers
- Reduce code duplication (each scraper had manual Flask + argparse setup)

## Solution: ScraperFlaskMixin

A reusable mixin that adds Flask web service capabilities to any scraper through simple class configuration.

### Key Benefits

- **90% Code Reduction**: ~125 lines of Flask + argparse â†’ ~5 lines of configuration
- **Dual Mode**: Same scraper works as CLI tool and web service
- **Consistent API**: All scrapers get identical endpoints
- **Cloud Run Ready**: Built-in health checks and proper logging
- **Backward Compatible**: CLI mode works exactly as before

## Implementation

### 1. ScraperFlaskMixin Code

Place this file at: `scrapers/scraper_flask_mixin.py`

```python
"""
scraper_flask_mixin.py

Reusable Flask mixin based on the existing odds scraper Flask implementation.
Extracts the Flask integration pattern into a reusable component.
"""

import os
import logging
import sys
from datetime import datetime, timezone
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from typing import Dict, Any, Optional


class ScraperFlaskMixin:
    """
    Flask mixin for scrapers based on the existing odds scraper pattern.
    
    Child classes should define:
    - scraper_name: str
    - required_params: list
    - optional_params: dict
    """
    
    # Child classes should override these
    scraper_name: str = "unknown_scraper"
    required_params: list = []
    optional_params: dict = {}
    
    def create_app(self):
        """Create Flask app for this scraper (based on existing pattern)."""
        from flask import Flask, request, jsonify
        from dotenv import load_dotenv
        import logging
        import sys
        
        app = Flask(__name__)
        load_dotenv()
        
        # Configure logging for Cloud Run
        if not app.debug:
            logging.basicConfig(level=logging.INFO)
        
        @app.route('/', methods=['GET'])
        @app.route('/health', methods=['GET'])
        def health_check():
            return jsonify({
                "status": "healthy", 
                "service": "scrapers",
                "scraper": self.scraper_name,
                "version": "1.0.0",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }), 200
        
        @app.route('/scrape', methods=['POST'])
        def scrape_endpoint():
            """Main scraping endpoint (based on existing pattern)."""
            try:
                # Get parameters from JSON body or query params
                if request.is_json:
                    params = request.get_json()
                else:
                    params = request.args.to_dict()
                
                # Build scraper opts using the existing pattern
                opts = self._build_scraper_opts(params)
                
                # Validate required params
                validation_error = self._validate_scraper_params(opts)
                if validation_error:
                    return jsonify({"error": validation_error}), 400
                
                # Set debug logging
                if opts.get("debug"):
                    logging.getLogger().setLevel(logging.DEBUG)
                
                # Run the scraper (existing method)
                result = self.run(opts)
                
                if result:
                    return jsonify({
                        "status": "success",
                        "message": f"{self.scraper_name} completed successfully",
                        "scraper": self.scraper_name,
                        "run_id": self.run_id,
                        "data_summary": self.get_scraper_stats()
                    }), 200
                else:
                    return jsonify({
                        "status": "error",
                        "message": f"{self.scraper_name} failed",
                        "scraper": self.scraper_name,
                        "run_id": self.run_id
                    }), 500
                    
            except Exception as e:
                app.logger.error(f"{self.scraper_name} error: {str(e)}", exc_info=True)
                return jsonify({
                    "status": "error",
                    "scraper": self.scraper_name,
                    "message": str(e)
                }), 500
        
        return app
    
    def _build_scraper_opts(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Build opts dict from request parameters (matches existing pattern)."""
        opts = {}
        
        # Add required parameters
        for param in self.required_params:
            if param in params:
                opts[param] = params[param]
        
        # Add optional parameters with defaults
        for param, default_value in self.optional_params.items():
            opts[param] = params.get(param, default_value)
        
        # Add common parameters (matches existing pattern)
        common_params = {
            "group": params.get("group", "prod"),
            "runId": params.get("runId"),
            "debug": bool(params.get("debug", False))
        }
        opts.update(common_params)
        
        return opts
    
    def _validate_scraper_params(self, opts: Dict[str, Any]) -> Optional[str]:
        """Validate required parameters (matches existing pattern)."""
        missing_params = []
        for param in self.required_params:
            if param not in opts or opts[param] is None:
                missing_params.append(param)
        
        if missing_params:
            return f"Missing required parameter{'s' if len(missing_params) > 1 else ''}: {', '.join(missing_params)}"
        
        return None
    
    @classmethod
    def create_cli_and_flask_main(cls):
        """
        Create main function that supports both CLI and Flask modes.
        Based on the existing main entry point pattern.
        """
        import argparse
        from dotenv import load_dotenv
        import logging
        import sys
        
        def main():
            load_dotenv()

            # Check if we're running as a web service or CLI
            if len(sys.argv) > 1 and sys.argv[1] == "--serve":
                # Web service mode for Cloud Run
                app = cls().create_app()
                port = int(os.getenv("PORT", 8080))
                debug = "--debug" in sys.argv
                app.run(host="0.0.0.0", port=port, debug=debug)
            else:
                # CLI mode (existing argparse code)
                parser = argparse.ArgumentParser(description=f"Run {cls.scraper_name}")
                parser.add_argument("--serve", action="store_true", help="Start web server")
                parser.add_argument("--debug", action="store_true", help="Verbose logging")
                parser.add_argument("--group", default="dev", help="exporter group")
                parser.add_argument("--runId", help="Optional correlation ID")
                
                # Add scraper-specific required arguments
                for param in cls.required_params:
                    parser.add_argument(f"--{param}", help=f"Required parameter: {param}")
                
                # Add scraper-specific optional arguments
                for param, default in cls.optional_params.items():
                    help_text = f"Optional parameter: {param}"
                    if default is not None:
                        help_text += f" (default: {default})"
                    parser.add_argument(f"--{param}", default=default, help=help_text)
                
                args = parser.parse_args()
                
                if args.serve:
                    # Start web server
                    app = cls().create_app()
                    port = int(os.getenv("PORT", 8080))
                    app.run(host="0.0.0.0", port=port, debug=args.debug)
                else:
                    # CLI scraping mode
                    
                    # Validate required params for CLI mode
                    for param in cls.required_params:
                        if not getattr(args, param):
                            parser.error(f"--{param} is required for CLI scraping")
                    
                    if args.debug:
                        logging.getLogger().setLevel(logging.DEBUG)
                        
                    # Run the scraper
                    scraper = cls()
                    scraper.run(vars(args))
        
        return main
```

### 2. Usage Pattern

To convert any existing scraper to use Flask integration:

**Before (Original Scraper):**
```python
from ..scraper_base import ScraperBase

class YourScraperClass(ScraperBase):
    required_opts = ["gamedate", "team"]
    
    def set_url(self):
        # your existing code
        pass
    
    # ... all your existing methods

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--gamedate", required=True)
    parser.add_argument("--team", required=True)
    args = parser.parse_args()
    
    scraper = YourScraperClass()
    scraper.run(vars(args))
```

**After (Flask-Enabled Scraper):**
```python
from ..scraper_base import ScraperBase
from ..scraper_flask_mixin import ScraperFlaskMixin  # ADD THIS IMPORT

class YourScraperClass(ScraperBase, ScraperFlaskMixin):  # ADD ScraperFlaskMixin
    # ADD THESE 3 LINES:
    scraper_name = "your_scraper_name"
    required_params = ["gamedate", "team"]
    optional_params = {
        "season": "2024-25",
        "timeout": 30
    }
    
    # KEEP ALL EXISTING SCRAPER CODE UNCHANGED:
    required_opts = ["gamedate", "team"]  # Keep this for compatibility
    
    def set_url(self):
        # your existing code - NO CHANGES NEEDED
        pass
    
    # ... all your existing methods - NO CHANGES NEEDED

# ADD THESE LINES (Flask entry point):
def create_app():
    """Create Flask app for this scraper."""
    return YourScraperClass().create_app()

# REPLACE MAIN ENTRY POINT WITH THIS SINGLE LINE:
if __name__ == "__main__":
    main = YourScraperClass.create_cli_and_flask_main()
    main()
```

### 3. Generated API Endpoints

Every scraper automatically gets these endpoints:

#### Health Check
```bash
GET /health
```

Response:
```json
{
  "status": "healthy",
  "service": "scrapers", 
  "scraper": "your_scraper_name",
  "version": "1.0.0",
  "timestamp": "2025-07-12T15:30:00Z"
}
```

#### Run Scraper
```bash
POST /scrape
Content-Type: application/json

{
  "gamedate": "20250712",
  "team": "lakers",
  "debug": true
}
```

Success Response:
```json
{
  "status": "success",
  "message": "your_scraper_name completed successfully",
  "scraper": "your_scraper_name", 
  "run_id": "abc12345",
  "data_summary": {
    "records_processed": 150,
    "execution_time": 2.3
  }
}
```

Error Response:
```json
{
  "status": "error",
  "scraper": "your_scraper_name",
  "message": "Missing required parameter: gamedate"
}
```

## Testing

### Local Testing

#### CLI Mode (unchanged)
```bash
python -m scrapers.your_scraper --gamedate 20250712 --team lakers --debug
```

#### Flask Mode
```bash
# Start Flask server
python -m scrapers.your_scraper --serve --debug

# Test health endpoint
curl http://localhost:8080/health

# Test scrape endpoint
curl -X POST http://localhost:8080/scrape \
  -H "Content-Type: application/json" \
  -d '{
    "gamedate": "20250712", 
    "team": "lakers",
    "debug": true
  }'
```

### Docker Testing

```dockerfile
# Dockerfile
FROM python:3.11-slim

ENV PORT=8080
WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# Default command for this scraper
CMD ["python", "-m", "scrapers.your_scraper", "--serve"]
```

```bash
# Build and test
docker build -t your-scraper .
docker run -p 8080:8080 -e API_KEY=your-key your-scraper

# Test endpoints
curl http://localhost:8080/health
curl -X POST http://localhost:8080/scrape \
  -H "Content-Type: application/json" \
  -d '{"gamedate": "20250712", "team": "lakers"}'
```

## Cloud Run Deployment

### Deploy Single Scraper

```bash
# Build and push image
gcloud builds submit --tag gcr.io/PROJECT_ID/your-scraper

# Deploy to Cloud Run
gcloud run deploy your-scraper \
  --image gcr.io/PROJECT_ID/your-scraper \
  --platform managed \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 1Gi \
  --timeout 900 \
  --set-env-vars API_KEY=your-key

# Test deployed service
SERVICE_URL=$(gcloud run services describe your-scraper \
  --region=us-central1 --format="value(status.url)")

curl $SERVICE_URL/health
curl -X POST $SERVICE_URL/scrape \
  -H "Content-Type: application/json" \
  -d '{"gamedate": "20250712", "team": "lakers", "group": "prod"}'
```

### Batch Deployment Script

```bash
#!/bin/bash
# deploy_all_scrapers.sh

SCRAPERS=(
  "odds_api_historical_events:scrapers.oddsapi.oddsa_events_his"
  "espn_scoreboard:scrapers.espn.espn_scoreboard_api"
  "ball_dont_lie_games:scrapers.balldontlie.bdl_games"
  # Add more scrapers here
)

for scraper_config in "${SCRAPERS[@]}"; do
  IFS=':' read -r scraper_name scraper_module <<< "$scraper_config"
  
  echo "Deploying $scraper_name..."
  
  gcloud builds submit \
    --tag "gcr.io/$PROJECT_ID/$scraper_name" \
    --substitutions="_SCRAPER_MODULE=$scraper_module"
  
  gcloud run deploy "$scraper_name" \
    --image "gcr.io/$PROJECT_ID/$scraper_name" \
    --platform managed \
    --region us-central1 \
    --allow-unauthenticated
done
```

## Integration with Existing Architecture

### Pub/Sub Integration

```python
# Cloud Function to trigger scrapers
import requests
import json

def trigger_scraper(event, context):
    """Triggered by Pub/Sub message."""
    
    # Parse message
    message = json.loads(base64.b64decode(event['data']).decode('utf-8'))
    scraper_name = message['scraper']
    params = message['params']
    
    # Call scraper Cloud Run service
    scraper_url = f"https://{scraper_name}-hash-uc.a.run.app"
    
    response = requests.post(
        f"{scraper_url}/scrape",
        json=params,
        timeout=900
    )
    
    if response.status_code == 200:
        print(f"Scraper {scraper_name} completed successfully")
    else:
        print(f"Scraper {scraper_name} failed: {response.text}")
```

### Workflow Integration

```yaml
# Cloud Workflows
main:
  params: [event]
  steps:
    - parallel_scraping:
        parallel:
          branches:
            - odds_scraper:
                call: http.post
                args:
                  url: "https://odds-scraper-hash-uc.a.run.app/scrape"
                  body: ${event.odds_params}
            - espn_scraper:
                call: http.post  
                args:
                  url: "https://espn-scraper-hash-uc.a.run.app/scrape"
                  body: ${event.espn_params}
```

## Migration Status

### Converted Scrapers
- [ ] `oddsapi/oddsa_events_his.py` (Ready for testing)

### Pending Conversion (~30 scrapers)
- [ ] `balldontlie/` (15+ scrapers)
- [ ] `espn/` (4 scrapers)  
- [ ] `nbacom/` (8 scrapers)
- [ ] `pbpstats/` (5 scrapers)

### Conversion Checklist

For each scraper:
- [ ] Add `ScraperFlaskMixin` to inheritance
- [ ] Define `scraper_name`, `required_params`, `optional_params`
- [ ] Add `create_app()` function
- [ ] Replace main entry with `create_cli_and_flask_main()`
- [ ] Test CLI mode unchanged
- [ ] Test Flask mode works
- [ ] Deploy to Cloud Run
- [ ] Update any orchestration to use HTTP endpoints

## Next Steps

### Immediate (Testing Phase)
1. Deploy `ScraperFlaskMixin` to `scrapers/scraper_flask_mixin.py`
2. Test converted odds scraper in CLI and Flask modes
3. Validate identical functionality to original
4. Deploy test scraper to Cloud Run
5. Fix any issues found

### Short Term (Mass Conversion)
1. Apply pattern to high-priority scrapers
2. Batch convert remaining scrapers using template
3. Update orchestration workflows to use HTTP endpoints
4. Monitor production performance

### Long Term (Enhancements)
1. Add data validation framework
2. Implement rate limiting and caching
3. Enhanced monitoring and alerting  
4. Auto-scaling optimization
5. CI/CD pipeline for scraper deployment

## Configuration Reference

### Required Mixin Properties

```python
class YourScraper(ScraperBase, ScraperFlaskMixin):
    # Required: Unique identifier for this scraper
    scraper_name = "unique_scraper_name"
    
    # Required: Parameters that must be provided
    required_params = ["param1", "param2"]
    
    # Optional: Parameters with default values
    optional_params = {
        "param3": "default_value",
        "param4": None,  # Falls back to env var or None
        "param5": 30     # Numeric default
    }
```

### Environment Variables

The mixin automatically handles:
- `PORT` - Server port (default: 8080)
- `ENV` - Environment name (development/production)
- Any API keys your scraper needs

### Parameter Types

- **required_params**: Must be provided (CLI: required args, Flask: 400 error if missing)
- **optional_params**: Optional with defaults (CLI: optional args, Flask: uses defaults)

## Troubleshooting

### Common Issues

**ImportError: No module named 'scraper_flask_mixin'**
- Ensure `scraper_flask_mixin.py` is in the `scrapers/` directory
- Check import path: `from ..scraper_flask_mixin import ScraperFlaskMixin`

**Flask server won't start**
- Check if port 8080 is available
- Verify all required dependencies in requirements.txt
- Check for syntax errors in scraper class

**400 error on /scrape endpoint**
- Verify all required_params are included in JSON body
- Check parameter names match exactly
- Ensure Content-Type header is application/json

**Cloud Run deployment fails**
- Verify Dockerfile CMD uses `--serve` flag
- Check environment variables are set
- Ensure image includes all dependencies

### Debug Mode

Enable debug logging for troubleshooting:

```bash
# CLI mode
python -m scrapers.your_scraper --debug --param value

# Flask mode  
python -m scrapers.your_scraper --serve --debug

# API call with debug
curl -X POST http://localhost:8080/scrape \
  -H "Content-Type: application/json" \
  -d '{"param": "value", "debug": true}'
```

## Architecture Benefits

- **Scalability**: Cloud Run auto-scales based on demand
- **Cost Efficiency**: Pay per request, scale to zero
- **Reliability**: Built-in health checks and error handling
- **Consistency**: Same API interface across all scrapers
- **Maintainability**: Single mixin to update vs 30+ individual files
- **Development Speed**: Faster to add new scrapers with template

This Flask integration provides a robust foundation for deploying NBA scrapers at scale while maintaining the existing development workflow.